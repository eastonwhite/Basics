<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Machine learning | Quant Bio Modules</title>
  <meta name="description" content="A set of modules on quantitaitve topics for biology graduates" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Machine learning | Quant Bio Modules" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A set of modules on quantitaitve topics for biology graduates" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Machine learning | Quant Bio Modules" />
  
  <meta name="twitter:description" content="A set of modules on quantitaitve topics for biology graduates" />
  

<meta name="author" content="Kim Cuddington, Andrew Edwards, Brian Ingalls" />


<meta name="date" content="2021-08-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction-to-multivariate-analysis.html"/>
<link rel="next" href="optimization.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.4.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.57.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.57.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Prerequisites</a></li>
<li class="chapter" data-level="2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html"><i class="fa fa-check"></i><b>2</b> Introduction to Git and GitHub</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#getting-set-up-for-the-first-time"><i class="fa fa-check"></i><b>2.2</b> Getting set up for the first time</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#what-you-will-end-up-having-installed"><i class="fa fa-check"></i><b>2.2.1</b> What you will end up having installed</a></li>
<li class="chapter" data-level="2.2.2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#get-a-github-account"><i class="fa fa-check"></i><b>2.2.2</b> Get a GitHub account</a></li>
<li class="chapter" data-level="2.2.3" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#text-editor"><i class="fa fa-check"></i><b>2.2.3</b> Text Editor</a></li>
<li class="chapter" data-level="2.2.4" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#install-the-git-application-on-your-machine"><i class="fa fa-check"></i><b>2.2.4</b> Install the Git application on your machine</a></li>
<li class="chapter" data-level="2.2.5" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#git-shell"><i class="fa fa-check"></i><b>2.2.5</b> Git shell</a></li>
<li class="chapter" data-level="2.2.6" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#git-shell-rstudio"><i class="fa fa-check"></i><b>2.2.6</b> Git shell, RStudio</a></li>
<li class="chapter" data-level="2.2.7" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#powershell-and-posh-git"><i class="fa fa-check"></i><b>2.2.7</b> Powershell and posh-git</a></li>
<li class="chapter" data-level="2.2.8" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#one-time-authentication"><i class="fa fa-check"></i><b>2.2.8</b> One-time authentication</a></li>
<li class="chapter" data-level="2.2.9" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#configure-the-git-application"><i class="fa fa-check"></i><b>2.2.9</b> Configure the Git application</a></li>
<li class="chapter" data-level="2.2.10" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#install-the-difftool"><i class="fa fa-check"></i><b>2.2.10</b> Install the difftool</a></li>
<li class="chapter" data-level="2.2.11" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#cloning-the-git-course-repository"><i class="fa fa-check"></i><b>2.2.11</b> “Cloning” the git-course repository</a></li>
<li class="chapter" data-level="2.2.12" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#copy-the-.gitignore-file"><i class="fa fa-check"></i><b>2.2.12</b> Copy the <em>.gitignore</em> file</a></li>
<li class="chapter" data-level="2.2.13" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#edit-the-.gitconfig-file"><i class="fa fa-check"></i><b>2.2.13</b> Edit the <em>.gitconfig</em> file</a></li>
<li class="chapter" data-level="2.2.14" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#mac-only-make-your-output-pretty"><i class="fa fa-check"></i><b>2.2.14</b> MAC only: make your output pretty</a></li>
<li class="chapter" data-level="2.2.15" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#markdown-pad"><i class="fa fa-check"></i><b>2.2.15</b> Markdown Pad</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#using-git-and-github"><i class="fa fa-check"></i><b>2.3</b> Using Git and GitHub</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#definitions"><i class="fa fa-check"></i><b>2.3.1</b> Definitions</a></li>
<li class="chapter" data-level="2.3.2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#creating-a-new-repository"><i class="fa fa-check"></i><b>2.3.2</b> Creating a new repository</a></li>
<li class="chapter" data-level="2.3.3" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#cloning-your-new-repository"><i class="fa fa-check"></i><b>2.3.3</b> Cloning your new repository</a></li>
<li class="chapter" data-level="2.3.4" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#committing"><i class="fa fa-check"></i><b>2.3.4</b> Committing</a></li>
<li class="chapter" data-level="2.3.5" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#exercise-1-create-edit-and-commit-simpletext.txt"><i class="fa fa-check"></i><b>2.3.5</b> Exercise 1: create, edit and commit <em>simpleText.txt</em></a></li>
<li class="chapter" data-level="2.3.6" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#exercise-2-multiple-files"><i class="fa fa-check"></i><b>2.3.6</b> Exercise 2: multiple files</a></li>
<li class="chapter" data-level="2.3.7" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#collaborating"><i class="fa fa-check"></i><b>2.3.7</b> Collaborating</a></li>
<li class="chapter" data-level="2.3.8" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#exercise-3-collaborating-on-a-single-repository"><i class="fa fa-check"></i><b>2.3.8</b> Exercise 3: collaborating on a single repository</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#beyond-the-basics"><i class="fa fa-check"></i><b>2.4</b> Beyond the basics</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#workflow-tips"><i class="fa fa-check"></i><b>2.4.1</b> Workflow tips</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#so-ive-made-some-changes-but-dont-really-want-to-keep-them-git-stash"><i class="fa fa-check"></i><b>2.4.2</b> So I’ve made some changes but don’t really want to keep them – git stash</a></li>
<li class="chapter" data-level="2.4.3" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#pull-requests"><i class="fa fa-check"></i><b>2.4.3</b> Pull requests</a></li>
<li class="chapter" data-level="2.4.4" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#the-power-to-go-back"><i class="fa fa-check"></i><b>2.4.4</b> The power to go back</a></li>
<li class="chapter" data-level="2.4.5" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#so-how-does-git-do-all-this"><i class="fa fa-check"></i><b>2.4.5</b> So how does Git do all this?</a></li>
<li class="chapter" data-level="2.4.6" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#git-terminology"><i class="fa fa-check"></i><b>2.4.6</b> Git terminology</a></li>
<li class="chapter" data-level="2.4.7" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#branching"><i class="fa fa-check"></i><b>2.4.7</b> Branching</a></li>
<li class="chapter" data-level="2.4.8" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#undoing-stuff"><i class="fa fa-check"></i><b>2.4.8</b> Undoing stuff</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html"><i class="fa fa-check"></i><b>3</b> Introduction to R Markdown</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#motivation-1"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#basic-idea"><i class="fa fa-check"></i><b>3.2</b> Basic idea</a></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#simple-example"><i class="fa fa-check"></i><b>3.3</b> Simple example</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#exercise"><i class="fa fa-check"></i><b>3.3.1</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#output-format"><i class="fa fa-check"></i><b>3.4</b> Output format</a></li>
<li class="chapter" data-level="3.5" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#further-reading"><i class="fa fa-check"></i><b>3.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html"><i class="fa fa-check"></i><b>4</b> Introduction to multivariate analysis</a>
<ul>
<li class="chapter" data-level="4.1" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#multivariate-resemblance"><i class="fa fa-check"></i><b>4.1</b> Multivariate resemblance</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#binary-similarity-metrics"><i class="fa fa-check"></i><b>4.1.1</b> Binary Similarity metrics</a></li>
<li class="chapter" data-level="4.1.2" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#quantitative-similarity-dissimilarity-metrics"><i class="fa fa-check"></i><b>4.1.2</b> Quantitative similarity &amp; dissimilarity metrics</a></li>
<li class="chapter" data-level="4.1.3" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#comparing-more-than-two-communitiessamplessitesgenesspecies"><i class="fa fa-check"></i><b>4.1.3</b> Comparing more than two communities/samples/sites/genes/species</a></li>
<li class="chapter" data-level="4.1.4" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#r-functions"><i class="fa fa-check"></i><b>4.1.4</b> R functions</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#cluster-analysis"><i class="fa fa-check"></i><b>4.2</b> Cluster Analysis</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#hierarchical-clustering-groups-are-nested-within-other-groups."><i class="fa fa-check"></i><b>4.2.1</b> Hierarchical clustering: groups are nested within other groups.</a></li>
<li class="chapter" data-level="4.2.2" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#how-many-clusters"><i class="fa fa-check"></i><b>4.2.2</b> How many clusters?</a></li>
<li class="chapter" data-level="4.2.3" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#partitional-clustering-and-fuzzy-clustering"><i class="fa fa-check"></i><b>4.2.3</b> Partitional clustering and Fuzzy clustering</a></li>
<li class="chapter" data-level="4.2.4" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#r-functions-for-clustering"><i class="fa fa-check"></i><b>4.2.4</b> R functions for clustering</a></li>
<li class="chapter" data-level="4.2.5" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#exercise-cluster-analysis-of-isotope-data"><i class="fa fa-check"></i><b>4.2.5</b> Exercise: Cluster analysis of isotope data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#Ord"><i class="fa fa-check"></i><b>4.3</b> Ordination</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>4.3.1</b> Principal Components Analysis (PCA)</a></li>
<li class="chapter" data-level="4.3.2" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#exercise-pca-on-the-iris-data"><i class="fa fa-check"></i><b>4.3.2</b> Exercise: PCA on the iris data</a></li>
<li class="chapter" data-level="4.3.3" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#principle-coordinates-analysis-pcoa"><i class="fa fa-check"></i><b>4.3.3</b> Principle Coordinates Analysis (PCoA)</a></li>
<li class="chapter" data-level="4.3.4" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#nonmetric-multidimensional-scaling-nmds"><i class="fa fa-check"></i><b>4.3.4</b> Nonmetric Multidimensional Scaling (nMDS)</a></li>
<li class="chapter" data-level="4.3.5" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#exercise-nmds-and-pcoa"><i class="fa fa-check"></i><b>4.3.5</b> Exercise: nMDS and PCoA</a></li>
<li class="chapter" data-level="4.3.6" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#constrained-ordination"><i class="fa fa-check"></i><b>4.3.6</b> Constrained Ordination</a></li>
<li class="chapter" data-level="4.3.7" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#redundancy-analysis"><i class="fa fa-check"></i><b>4.3.7</b> Redundancy Analysis</a></li>
<li class="chapter" data-level="4.3.8" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#exercise-constrained-ordination"><i class="fa fa-check"></i><b>4.3.8</b> Exercise: Constrained ordination</a></li>
<li class="chapter" data-level="4.3.9" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#signficance-tests-for-constrained-ordination"><i class="fa fa-check"></i><b>4.3.9</b> Signficance tests for constrained ordination</a></li>
<li class="chapter" data-level="4.3.10" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#forward-selection-of-explanatory-variables"><i class="fa fa-check"></i><b>4.3.10</b> Forward Selection of explanatory variables</a></li>
<li class="chapter" data-level="4.3.11" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html#tri"><i class="fa fa-check"></i><b>4.3.11</b> Triplots: Graphing a constrained ordination</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="machine-learning.html"><a href="machine-learning.html"><i class="fa fa-check"></i><b>5</b> Machine learning</a>
<ul>
<li class="chapter" data-level="5.1" data-path="machine-learning.html"><a href="machine-learning.html#classification"><i class="fa fa-check"></i><b>5.1</b> Classification</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="machine-learning.html"><a href="machine-learning.html#logistic-regression-as-a-binary-classifier"><i class="fa fa-check"></i><b>5.1.1</b> Logistic regression as a binary classifier</a></li>
<li class="chapter" data-level="5.1.2" data-path="machine-learning.html"><a href="machine-learning.html#interpreting-the-logistic-regression"><i class="fa fa-check"></i><b>5.1.2</b> Interpreting the logistic regression</a></li>
<li class="chapter" data-level="5.1.3" data-path="machine-learning.html"><a href="machine-learning.html#measuring-the-performance-of-a-binary-classifier"><i class="fa fa-check"></i><b>5.1.3</b> Measuring the performance of a binary classifier</a></li>
<li class="chapter" data-level="5.1.4" data-path="machine-learning.html"><a href="machine-learning.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>5.1.4</b> Multiple logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="machine-learning.html"><a href="machine-learning.html#cross-validation"><i class="fa fa-check"></i><b>5.2</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="machine-learning.html"><a href="machine-learning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>5.2.1</b> k-Fold Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="machine-learning.html"><a href="machine-learning.html#tree-based-methods-for-classification"><i class="fa fa-check"></i><b>5.3</b> Tree-based methods for classification</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="machine-learning.html"><a href="machine-learning.html#classification-and-regression-trees-carts"><i class="fa fa-check"></i><b>5.3.1</b> Classification and regression trees (CARTs)</a></li>
<li class="chapter" data-level="5.3.2" data-path="machine-learning.html"><a href="machine-learning.html#tree-pruning"><i class="fa fa-check"></i><b>5.3.2</b> Tree pruning</a></li>
<li class="chapter" data-level="5.3.3" data-path="machine-learning.html"><a href="machine-learning.html#random-forests"><i class="fa fa-check"></i><b>5.3.3</b> Random Forests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="optimization.html"><a href="optimization.html"><i class="fa fa-check"></i><b>6</b> Optimization</a>
<ul>
<li class="chapter" data-level="6.1" data-path="optimization.html"><a href="optimization.html#introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="optimization.html"><a href="optimization.html#fundamentals-of-optimization"><i class="fa fa-check"></i><b>6.2</b> Fundamentals of Optimization</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="optimization.html"><a href="optimization.html#fermats-theorem"><i class="fa fa-check"></i><b>6.2.1</b> Fermat’s Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="optimization.html"><a href="optimization.html#regression"><i class="fa fa-check"></i><b>6.3</b> Regression</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="optimization.html"><a href="optimization.html#linear-regression"><i class="fa fa-check"></i><b>6.3.1</b> Linear Regression</a></li>
<li class="chapter" data-level="6.3.2" data-path="optimization.html"><a href="optimization.html#nonlinear-regression"><i class="fa fa-check"></i><b>6.3.2</b> Nonlinear Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="optimization.html"><a href="optimization.html#iterative-optimization-algorithms"><i class="fa fa-check"></i><b>6.4</b> Iterative Optimization Algorithms</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="optimization.html"><a href="optimization.html#gradient-descent"><i class="fa fa-check"></i><b>6.4.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="6.4.2" data-path="optimization.html"><a href="optimization.html#global-optimization"><i class="fa fa-check"></i><b>6.4.2</b> Global Optimization</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="optimization.html"><a href="optimization.html#calibration-of-dynamic-models"><i class="fa fa-check"></i><b>6.5</b> Calibration of Dynamic Models</a></li>
<li class="chapter" data-level="6.6" data-path="optimization.html"><a href="optimization.html#uncertainty-analysis-and-bayesian-calibration"><i class="fa fa-check"></i><b>6.6</b> Uncertainty Analysis and Bayesian Calibration</a></li>
<li class="chapter" data-level="6.7" data-path="optimization.html"><a href="optimization.html#references"><i class="fa fa-check"></i><b>6.7</b> References</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quant Bio Modules</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machine-learning" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Machine learning</h1>
<p>In this module we will continue our exploration of techniques for multivariate data (see Module 4), but will pay more attention to machine learning approaches. Machine learning is an application of artificial intelligence, which allows algorithms to become more accurate at predicting outcomes without being explicitly programmed to do so. When we fit a regression in the ordinary way, we specify the model ahead of time and determine if the model has good or bad fit. We might then modify our model to reduce prediction error. In machine learning, the algorithm itself completes the tasks of model specification, evaluation and improvement. Spooky!</p>
<div id="classification" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Classification</h2>
<p>Classification is the task of assigning data objects, such as sites, species or images to predetermined classes. Determining what class of data object you have is a question that usually turns on multiple predictors. For example, to classify leaf images to different species, predictors such as size, shape and colour may be used. If you have satellite data, you may need to classify the different pixels of the image as agricultural, forest, or urban. So for classification tasks our response variable, y, is qualitative or categorical (e.g., gender, species, land classification).</p>
<p>There are many methods that can be employed for classification tasks ranging from logistic regression to random forest techniques. While some of these methods are classic multivariate methods, others, like random forest classifiers, are machine learning tasks. The computer algorithm finds a solution to the classification problem without being explicitly programmed to do so.</p>
<div id="logistic-regression-as-a-binary-classifier" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Logistic regression as a binary classifier</h3>
<p>One of the simplest classification methods, and one that does not involve machine learning, is logistic regression. Let’s take a common example. A non-native species has been introduced to a region, and we would like to know what percentage of the region would be suitable habitat, in order to get an idea of risks of impact on the native ecosystem. We think that average temperature controls habitat suitability, and we have presence/absence data for the species accross a range of different sites. Could we use simple regression to answer the question of whether a given area is suitable habitat? If we indicate absence as 0, and presence as 1, we can regress species occurrence again average annual temperatures at each location.</p>
<div class="figure"><span id="fig:log"></span>
<img src="_main_files/figure-html/log-1.png" alt="This figure shows a plot with y-axis labelled 'Species occurance' and x-axis labelled 'Mean annual temperature (°C)'. The plot shows a set of points where 'Species presence/absence' is at 0 and 1. There is a red fitted line, based on the plotted points, that trends upwards." width="672" />
<p class="caption">
Figure 5.1: Species presence/absence and mean annual temperature with linear regression
</p>
</div>
<p>As we can see in Fig. <a href="machine-learning.html#fig:log">5.1</a>, the linear regression does not make a lot of sense for a response variable that is restricted to the values of 0 and 1. The regression line <span class="math inline">\(\beta_0+\beta_1x\)</span> can take on any value between negative and positive infinity, but we don’t know how to interpret values greater than 1 or less than zero. The regression line almost always predicts wrong value for y in classification problems.</p>
<p>Instead of trying to predict y, we can try to predict p(y = 1), i.e., the probability that the species will be found in the area. For invasive species, this probability is often interpreted as habitat suitability for the species. We need a function that gives outputs between 0 and 1: logistic regression is one solution. In this model, probability of y for a given value of x, p(x) is given as: <span class="math display">\[\frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}.\]</span> Rearranging, we have: <span class="math display">\[\frac{p(x)}{1-p(x)}=e^{\beta_0+\beta_1x}.\]</span> Taking the natural logarithm, we can see that the logistic regression is linear in x: <span class="math display">\[\log{\frac{p(x)}{1-p(x)}}=\beta_0+\beta_1x,\]</span> where the lefthand side is called the log-odds or logit. The logistic function will always produce an S-shaped curve, so regardless of the value of x, we will obtain a sensible prediction.</p>
<p>Let’s apply this model to our non-native species data using the <strong>g</strong>eneralized <strong>l</strong>inear <strong>m</strong>odel or <em>glm()</em> function. We use the glm() function to perform logistic regression by passing in the family=“binomial” argument. But if we use glm() to fit a model without passing in the family argument, then it performs linear regression, just like the <strong>l</strong>inear <strong>m</strong>odel or <em>lm()</em> function. We will fit the model and then get the predictions.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="machine-learning.html#cb73-1" aria-hidden="true" tabindex="-1"></a>glm.fit.sp <span class="ot">=</span> <span class="fu">glm</span>(y <span class="sc">~</span> x, <span class="at">family =</span> binomial)</span>
<span id="cb73-2"><a href="machine-learning.html#cb73-2" aria-hidden="true" tabindex="-1"></a>glm.probs <span class="ot">&lt;-</span> <span class="fu">predict</span>(glm.fit.sp, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:logfit"></span>
<img src="_main_files/figure-html/logfit-1.png" alt="This figure shows a plot with y-axis labelled 'Species occurance' and x-axis labelled 'Mean annual temperature (°C). There are a set of points where 'Species occurance' is at 0 and 1. There is a red logistic regression line based on the plotted points." width="672" />
<p class="caption">
Figure 5.2: Species presence/absence and mean annual temperature with logistic regression
</p>
</div>
</div>
<div id="interpreting-the-logistic-regression" class="section level3" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Interpreting the logistic regression</h3>
<p>Let’s take a closer look at the output for our logistic regression (Fig. <a href="machine-learning.html#fig:logfit">5.2</a>). First off, we need to know if the intercept, <span class="math inline">\(\beta_0\)</span>, and slope, <span class="math inline">\(\beta_1\)</span>, are significantly different from zero. A z distribution is used for this test, and we find that both the intercept and slope are significantly different from zero (Table <a href="machine-learning.html#tab:logcoef">5.1</a>). Our p-values are very small, so our model is doing better than random chance.</p>
<table>
<caption>
<span id="tab:logcoef">Table 5.1: </span>Logistic regression coefficient estimates and hypothesis tests from species occurance data
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Estimate
</th>
<th style="text-align:right;">
Std. Error
</th>
<th style="text-align:right;">
z value
</th>
<th style="text-align:right;">
Pr(&gt;|z|)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
-10.72
</td>
<td style="text-align:right;">
1.57
</td>
<td style="text-align:right;">
-6.84
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
x
</td>
<td style="text-align:right;">
0.75
</td>
<td style="text-align:right;">
0.12
</td>
<td style="text-align:right;">
6.52
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
<p>The estimated intercept is typically not of interest. Its main purpose is to adjust the average fitted probability to the proportion of ones in the data. You may be confused by the slope value. Interpreting what <span class="math inline">\(\beta_1\)</span> means is not very easy with logistic regression, simply because we are predicting p(y) and not y. If <span class="math inline">\(\beta_1\)</span> = 0, this means there is no relationship between p(y) and x. If <span class="math inline">\(\beta_1\)</span> &gt; 0, this means that when y gets larger so does the probability that y = 1. If <span class="math inline">\(\beta_1\)</span> &lt; 0, this means that when x gets larger, the probability that y = 1 gets smaller. Our <span class="math inline">\(\beta_1\)</span> is positive, so we are sure that as temperature increases, the probability of habitat suitability will increase as well. For example, suppose a region has an average annual temperature of 12°C. We know that the probability that the habitat is suitable p(y) for our invasive species is: <span class="math display">\[p(y)=\frac{e^{paste \beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}.\]</span>
Substituting in our fitted values from the logistic regression, we have:
<span class="math display">\[p(y)=\frac{e^{-10.72 + 0.75 (12)}}{1+e^{-10.72 + 0.75(12)}}=0.15.\]</span>
The value changes with temperature, so for an annual average temperature of 16°C that probability is 0.78, and so on.</p>
<div id="exercise-1-can-you-calculate-this-probability" class="section level4" number="5.1.2.1">
<h4><span class="header-section-number">5.1.2.1</span> Exercise 1: Can you calculate this probability?</h4>
</div>
</div>
<div id="measuring-the-performance-of-a-binary-classifier" class="section level3" number="5.1.3">
<h3><span class="header-section-number">5.1.3</span> Measuring the performance of a binary classifier</h3>
<p>Of course, the data used to fit this relationship is zeros (absence) and ones (present). How well does our model do at predicting species occurrence? In order to answer this question we need to decide on a <strong>threshold value</strong> for the probability prediction. For example, an easy one is 50%. If the model predicts a probability of greater than 0.5, then we will score that as presence, while if the predicted value is equal to or less than 0.5, we will score as an absence. Now we have model predictions in terms of predicted absences and presences (i.e., zeros and ones), and can compare directly to our data.</p>
<p>When binary classifications are made, by converting the probabilities using a threshold, there can be four cases for a certain observation:</p>
<ol style="list-style-type: decimal">
<li><p>The response actually negative, the model predicts it to be negative. This is known as true negative (TN). In our case, the invasive species is not present at the location, and the model predicts that it should not be present.</p></li>
<li><p>The response actually negative, but the model predicts it to be positive (i.e., false positive, FP).</p></li>
<li><p>The response actually positive, and the model predicts it to be positive (i.e., true positive TP).</p></li>
<li><p>The response actually positive, but the model predicts it to be negative (i.e., false negative FN).</p></li>
</ol>
<p>We can summarize this information in a <strong>confusion matrix</strong> which records the number of times the model correctly predicted the data, and the number of times the model makes incorrect predictions.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="machine-learning.html#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># determine model occurrence predictions based on threshold</span></span>
<span id="cb74-2"><a href="machine-learning.html#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="co"># value</span></span>
<span id="cb74-3"><a href="machine-learning.html#cb74-3" aria-hidden="true" tabindex="-1"></a>logocc <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(glm.probs <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb74-4"><a href="machine-learning.html#cb74-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-5"><a href="machine-learning.html#cb74-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate a confusion matrix</span></span>
<span id="cb74-6"><a href="machine-learning.html#cb74-6" aria-hidden="true" tabindex="-1"></a>ctab <span class="ot">=</span> <span class="fu">table</span>(logocc, y)</span>
<span id="cb74-7"><a href="machine-learning.html#cb74-7" aria-hidden="true" tabindex="-1"></a><span class="fu">dimnames</span>(ctab) <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">Actual =</span> <span class="fu">c</span>(<span class="st">&quot;absence(0)&quot;</span>, <span class="st">&quot;presence(1)&quot;</span>),</span>
<span id="cb74-8"><a href="machine-learning.html#cb74-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">Predicted =</span> <span class="fu">c</span>(<span class="st">&quot;absence(0)&quot;</span>, <span class="st">&quot;presence(1)&quot;</span>))</span>
<span id="cb74-9"><a href="machine-learning.html#cb74-9" aria-hidden="true" tabindex="-1"></a>ctab</span></code></pre></div>
<pre><code>             Predicted
Actual        absence(0) presence(1)
  absence(0)         125          23
  presence(1)         20          32</code></pre>
<p>Elements on the diagonal from left to right are correct classifications, while off-diagonal elements are missclassifications (i.e., predictions where the predict code of 0 or 1 does not match the actual code of 0 or 1). So in this case, we have 125 true negatives (TN), 23 false positives (FP), 20 false negatives (FN), and 32 true positives (TP).</p>
<p>We can quantify these errors in a number of ways. The misclassification rate, or <strong>error rate</strong> is the most common metric used to quantify the performance of a binary classifier. This is the probability that the classifier makes a wrong prediction (given as <span class="math inline">\(\frac{FN+FP}{TN+FN+TP+FP}\)</span>). Overall 43 observations have been misclassified, so we have a total error rate of 22%. However, 38% of the presence data has been misclassified, compared to 16% of the absence data. So with respect to the predicting the potential presence of an invasive species, we’re not doing much better than random chance.</p>
<p>The terms sensitivity and specificity characterize the performance of classifier for these specific types of errors. In this case, the
<strong>sensitivity</strong> is the percentage of occupancy locations that are correctly identified (true positives), which is 62% in this case or one minus the misclassification rate of positives (or 1-0.38), also calculated as TP/(TP+FN). <strong>Specificity</strong> is the percentage of non-occupancy sites that are correctly identified (true negatives). We can calculate this as one minus the misclassification of true negatives from the values above (1 − 0.16) = 0.84, or from the formula TN/(TN+FP).</p>
<p>We can of course, change the decision threshold to see if we can get a better outcome. Let’s try 0.65 instead of 0.5.</p>
<pre><code>      Predicted
Actual   0   1
     0 135  40
     1  10  15</code></pre>
<p>This higher threshold gives us an error rate of 25%, sensitivity of 60% and specificity of 77%, so not much improvement.</p>
<div id="exercise-2-can-you-calculate-these-rates" class="section level4" number="5.1.3.1">
<h4><span class="header-section-number">5.1.3.1</span> Exercise 2: Can you calculate these rates?</h4>
</div>
<div id="roc-and-auc" class="section level4" number="5.1.3.2">
<h4><span class="header-section-number">5.1.3.2</span> ROC and AUC</h4>
<p>We can also examine the performance of the model across a range of thresholds. We often see this approach in species distribution modelling, where specificity (% of true positives) is plotted against 1-sensitivity (% of false positives) for threshold values from 0 to 1. This graph is called the <strong>R</strong>eceiver <strong>O</strong>perator <strong>C</strong>urve (<strong>ROC</strong>), and the <strong>A</strong>rea <strong>U</strong>nder that <strong>C</strong>urve (<strong>AUC</strong>) is calculated. This is a fairly standard evaluation for binary classifiers, and there are a number of R packages that will complete this analysis for you. If the model is not performing better than random chance, the expected ROC curve is simply the y=x line. Where the model can perfectly separate the two classes, the ROC curve consists of a vertical line (x=0) and a horizontal line (y=1). For real and simulated data, usually the ROC stays in between these two extreme scenarios. Let’s try with our simulated data.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="machine-learning.html#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ROCit)</span>
<span id="cb77-2"><a href="machine-learning.html#cb77-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-3"><a href="machine-learning.html#cb77-3" aria-hidden="true" tabindex="-1"></a>ROCit_obj <span class="ot">&lt;-</span> <span class="fu">rocit</span>(<span class="at">score =</span> glm.fit.sp<span class="sc">$</span>fitted.values, <span class="at">class =</span> y)</span>
<span id="cb77-4"><a href="machine-learning.html#cb77-4" aria-hidden="true" tabindex="-1"></a>pauc <span class="ot">=</span> <span class="fu">plot</span>(ROCit_obj)</span></code></pre></div>
<div class="figure"><span id="fig:roc2"></span>
<img src="_main_files/figure-html/roc2-1.png" alt="Sensitivity (TPR) ploted versus 1-Specificity (FPR) and the y=x line, where the optimal threshold is also indicated" width="672" />
<p class="caption">
Figure 5.3: Receiver Operator Curve (ROC) for the logistic regression binary classifier of species occurence data
</p>
</div>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="machine-learning.html#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ROCit_obj)</span></code></pre></div>
<pre><code>                           
 Method used: empirical    
 Number of positive(s): 55 
 Number of negative(s): 145
 Area under curve: 0.8639  </code></pre>
<p>Overall, we have an area under the ROC curve (Fig.<a href="machine-learning.html#fig:roc2">5.3</a>) of 0.86, which is not bad, given the maximum value is one. The optimal threshold value is given by the Youden index as 0.68. The Youden index maximizes the difference between sensitivity and 1-specificity and is defined as sensitivity+specificity-1. Let’s try this threshold directly:</p>
<pre><code>      Predicted
Actual   0   1
     0 136  42
     1   9  13</code></pre>
<p>We can see that this error threshold, as summarized above, gives an error rate of 26%, sensitivity of 59% and specificity of 76%. Overall, an okay, but not fantastic model, at the best performing threshold.</p>
</div>
</div>
<div id="multiple-logistic-regression" class="section level3" number="5.1.4">
<h3><span class="header-section-number">5.1.4</span> Multiple logistic regression</h3>
<p>If we have more than one predictor, we can fit multiple logistic just like regular regression, as:
<span class="math display">\[p(y)=\frac{e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}\]</span>
and the <span class="math inline">\(x_n\)</span> predictors can be both qualitative or quantitative. For example, we could add a land classification to our invasive species habitat suitability model so that we have both temperature (<span class="math inline">\(x_1\)</span>) and urban and rural (<span class="math inline">\(x_2\)</span>) land types.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="machine-learning.html#cb81-1" aria-hidden="true" tabindex="-1"></a>glm.fit.sp2 <span class="ot">=</span> <span class="fu">glm</span>(y <span class="sc">~</span> x <span class="sc">+</span> <span class="fu">as.factor</span>(x2), <span class="at">family =</span> binomial)</span></code></pre></div>
<p>In this case, our model now has two responses, one for land categorized as urban, and one for land categorized as rural (Fig. <a href="machine-learning.html#fig:multilog">5.4</a>).</p>
<table>
<caption>
<span id="tab:logcoefs2">Table 5.2: </span>Logistic regression coefficient estimates and hypothesis tests from species occurance data with temperature and land category as predictors
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Estimate
</th>
<th style="text-align:right;">
Std. Error
</th>
<th style="text-align:right;">
z value
</th>
<th style="text-align:right;">
Pr(&gt;|z|)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
-11.21
</td>
<td style="text-align:right;">
1.70
</td>
<td style="text-align:right;">
-6.59
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
x
</td>
<td style="text-align:right;">
0.75
</td>
<td style="text-align:right;">
0.12
</td>
<td style="text-align:right;">
6.06
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
as.factor(x2)urban
</td>
<td style="text-align:right;">
1.83
</td>
<td style="text-align:right;">
0.45
</td>
<td style="text-align:right;">
4.04
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
Both predictors are significantly different from zero (Table <a href="machine-learning.html#tab:logcoefs2">5.2</a>), and the values tell is that if the land is categorized as urban, we must increase our probability estimate upwards from that of a rural area as:<br />
<span class="math display">\[p(y)=\frac{e^{-11.21 + 0.75 x_1+1.83 (1)}}{1+e^{-11.21 + 0.75x_1+1.83 (1)}}\]</span>
<div class="figure"><span id="fig:multilog"></span>
<img src="_main_files/figure-html/multilog-1.png" alt="This figure shows a plot with y-axis labelled 'Species occurrence' and x-axis labelled 'Mean annual temperature (°C)'. The top left of the plot contains a legend with label 'urban' and 'rural' which are denoted by a red circle and a black circle respectively. The plot shows a set of 'urban' red points and 'rural' black points where 'Species occurance' is at 0 and 1. There are two logistic regression lines that represent the land categories. The 'urban' red logistic regression line is above the 'rural' black logistic regression line." width="672" />
<p class="caption">
Figure 5.4: Species occurrence vs temperature and land classification as urban or rural
</p>
</div>
<p>Logistic regression can be extended to multiple classification problems in different ways, but in practice these methods tend not to be used all that often. Instead other techniques such as discriminant analysis and random forest tend to be used for multiple-class classification problems.</p>
</div>
</div>
<div id="cross-validation" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Cross-validation</h2>
<p>So far, we’ve evaluated model performance with the data that we used to train the model, but the point of classification tools is to be able to use them on data where we don’t already know the answer. In that sense, we are uninterested in in model performance in <strong>training data</strong>, what we really want to do is to test the model on data that was not used in model fitting. For example, we don’t really care how well our method predicts habitat suitability where the invasive species is already located! What we need to know is how well it predicts the habitat suitability of locations where the species has not yet invaded. The model performance on this <strong>testing data</strong> will give us a better idea of the errors we might expect when we apply our classifier to novel data. While we might naively expect that model performance on the training data will be the same on the testing data, in practice the errors are usually larger, sometimes much larger. In more complex models, this error rate is often the result of overfitting the training data, so that pattern which is just noise is included in the model fit. Consequently, the model is not well fit to data with different sources of noise.</p>
<p>Of course, in biological data is almost always limited! While you might have an extra independent dataset kicking around waiting to be used for model testing, if you don’t, you can divide your single dataset into training and testing sets. One easy way to do this is just using random selection. Let’s try on our data. We’ll divide a dataframe with our data into two parts using the <em>sample()</em> function, fit our logistic model on the training data, and evaluate its performance on the testing data.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="machine-learning.html#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="co"># simulate some temperature and occupancy data using a</span></span>
<span id="cb82-2"><a href="machine-learning.html#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="co"># random number generator</span></span>
<span id="cb82-3"><a href="machine-learning.html#cb82-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">200</span>, <span class="dv">12</span>, <span class="fl">2.5</span>)</span>
<span id="cb82-4"><a href="machine-learning.html#cb82-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">ifelse</span>(x <span class="sc">&gt;</span> <span class="dv">12</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb82-5"><a href="machine-learning.html#cb82-5" aria-hidden="true" tabindex="-1"></a>e <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">200</span>, <span class="dv">0</span>, <span class="fl">0.1</span>)</span>
<span id="cb82-6"><a href="machine-learning.html#cb82-6" aria-hidden="true" tabindex="-1"></a><span class="co"># adding some randomness to simulated occupancy data</span></span>
<span id="cb82-7"><a href="machine-learning.html#cb82-7" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">ifelse</span>(y <span class="sc">+</span> e <span class="sc">&gt;=</span> <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb82-8"><a href="machine-learning.html#cb82-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-9"><a href="machine-learning.html#cb82-9" aria-hidden="true" tabindex="-1"></a><span class="co"># create a dataframe with our temperature and occupancy</span></span>
<span id="cb82-10"><a href="machine-learning.html#cb82-10" aria-hidden="true" tabindex="-1"></a><span class="co"># data</span></span>
<span id="cb82-11"><a href="machine-learning.html#cb82-11" aria-hidden="true" tabindex="-1"></a>ivsp <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">temp =</span> x, <span class="at">occ =</span> y)</span>
<span id="cb82-12"><a href="machine-learning.html#cb82-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-13"><a href="machine-learning.html#cb82-13" aria-hidden="true" tabindex="-1"></a><span class="co"># randomly sample 75% of the data (by generating random</span></span>
<span id="cb82-14"><a href="machine-learning.html#cb82-14" aria-hidden="true" tabindex="-1"></a><span class="co"># numbers based on the number of rows)</span></span>
<span id="cb82-15"><a href="machine-learning.html#cb82-15" aria-hidden="true" tabindex="-1"></a>samp <span class="ot">=</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(ivsp), <span class="fu">nrow</span>(ivsp) <span class="sc">*</span> <span class="fl">0.75</span>, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb82-16"><a href="machine-learning.html#cb82-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-17"><a href="machine-learning.html#cb82-17" aria-hidden="true" tabindex="-1"></a><span class="co"># divide into training and testing sets</span></span>
<span id="cb82-18"><a href="machine-learning.html#cb82-18" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> ivsp[samp, ]</span>
<span id="cb82-19"><a href="machine-learning.html#cb82-19" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> ivsp[<span class="sc">-</span>samp, ]</span>
<span id="cb82-20"><a href="machine-learning.html#cb82-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-21"><a href="machine-learning.html#cb82-21" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the logistic model on the training data</span></span>
<span id="cb82-22"><a href="machine-learning.html#cb82-22" aria-hidden="true" tabindex="-1"></a>log.fit.inv <span class="ot">=</span> <span class="fu">glm</span>(occ <span class="sc">~</span> temp, <span class="at">family =</span> binomial, <span class="at">data =</span> train)</span>
<span id="cb82-23"><a href="machine-learning.html#cb82-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-24"><a href="machine-learning.html#cb82-24" aria-hidden="true" tabindex="-1"></a><span class="co"># test the logistic model on the testing data</span></span>
<span id="cb82-25"><a href="machine-learning.html#cb82-25" aria-hidden="true" tabindex="-1"></a>log.predict <span class="ot">&lt;-</span> <span class="fu">predict</span>(log.fit.inv, <span class="at">newdata =</span> test, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb82-26"><a href="machine-learning.html#cb82-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-27"><a href="machine-learning.html#cb82-27" aria-hidden="true" tabindex="-1"></a><span class="co"># determine predicted occupancy based on threshold value of</span></span>
<span id="cb82-28"><a href="machine-learning.html#cb82-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.5</span></span>
<span id="cb82-29"><a href="machine-learning.html#cb82-29" aria-hidden="true" tabindex="-1"></a>pred.occ <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(log.predict <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb82-30"><a href="machine-learning.html#cb82-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-31"><a href="machine-learning.html#cb82-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate a confusion matrix</span></span>
<span id="cb82-32"><a href="machine-learning.html#cb82-32" aria-hidden="true" tabindex="-1"></a>ctab <span class="ot">=</span> <span class="fu">table</span>(pred.occ, test<span class="sc">$</span>occ)</span>
<span id="cb82-33"><a href="machine-learning.html#cb82-33" aria-hidden="true" tabindex="-1"></a><span class="fu">dimnames</span>(ctab) <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">Actual =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">Predicted =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb82-34"><a href="machine-learning.html#cb82-34" aria-hidden="true" tabindex="-1"></a>ctab</span></code></pre></div>
<pre><code>      Predicted
Actual  0  1
     0 39  8
     1  1  2</code></pre>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="machine-learning.html#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate error rate, sensitivity and specificity</span></span>
<span id="cb84-2"><a href="machine-learning.html#cb84-2" aria-hidden="true" tabindex="-1"></a>err <span class="ot">=</span> <span class="fu">round</span>((ctab[<span class="dv">1</span>, <span class="dv">2</span>] <span class="sc">+</span> ctab[<span class="dv">2</span>, <span class="dv">1</span>])<span class="sc">/</span><span class="fu">sum</span>(ctab), <span class="dv">2</span>)</span>
<span id="cb84-3"><a href="machine-learning.html#cb84-3" aria-hidden="true" tabindex="-1"></a>sens <span class="ot">=</span> <span class="fu">round</span>(ctab[<span class="dv">2</span>, <span class="dv">2</span>]<span class="sc">/</span>(ctab[<span class="dv">2</span>, <span class="dv">1</span>] <span class="sc">+</span> ctab[<span class="dv">2</span>, <span class="dv">2</span>]), <span class="dv">2</span>)</span>
<span id="cb84-4"><a href="machine-learning.html#cb84-4" aria-hidden="true" tabindex="-1"></a>spec <span class="ot">=</span> <span class="fu">round</span>(<span class="dv">1</span> <span class="sc">-</span> ctab[<span class="dv">1</span>, <span class="dv">2</span>]<span class="sc">/</span>(ctab[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">+</span> ctab[<span class="dv">1</span>, <span class="dv">2</span>]), <span class="dv">2</span>)</span>
<span id="cb84-5"><a href="machine-learning.html#cb84-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-6"><a href="machine-learning.html#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;error rate=&quot;</span>, err, <span class="st">&quot;; sensitivity=&quot;</span>, sens, <span class="st">&quot;; specificity=&quot;</span>,</span>
<span id="cb84-7"><a href="machine-learning.html#cb84-7" aria-hidden="true" tabindex="-1"></a>    spec))</span></code></pre></div>
<pre><code>[1] &quot;error rate=0.18; sensitivity=0.67; specificity=0.83&quot;</code></pre>
<p>You may want to verify for yourself that the sample function randomly selects rows out of a dataframe</p>
<p>You’ll notice that by dividing the data up this way we have only 50 observations in our testing data. We can of course use different percentages to divide up our one dataset into training and testing sets, but models tend to have poorer performance when trained on fewer observations. On the other hand, the small testing dataset may tend to overestimate the test error rate for the model fit, as compared to error rates obtained on a larger amount of data.</p>
<p>And what about the effects of that random sampling? If we repeat the process of randomly splitting the sample set into two parts, we will get a somewhat different estimate for the error rate on testng data each time, because different observations will be randomly included in the dataset each time. Sometimes the differences between error rates on different testing datasets can be rather large. For example, by random selection an observation which is a huge outlier could be included in one small testing dataset, but not in another, resulting is very different error rates. To guard against undue influence of single observations in our small test dataset we could do the routine of randomly sampling to obtain testing and training sets several times, and look at the average of our testing data performance.</p>
<div id="k-fold-cross-validation" class="section level3" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> k-Fold Cross-Validation</h3>
<p>One way of implementing this type of resampling scheme is k-fold cross-validation. With this method, we randomly divide the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a testing set, and the model is fit on the remaining k − 1 folds. This procedure is repeated k times, and each time, a different group of observations is treated as the testing set. We then average the error rates from each test fold. We can even repeat the entire procedure several times in <strong>repeated k-fold cross-validation</strong>.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="machine-learning.html#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># example code for iterated cross-validation set up</span></span>
<span id="cb86-2"><a href="machine-learning.html#cb86-2" aria-hidden="true" tabindex="-1"></a>reps <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb86-3"><a href="machine-learning.html#cb86-3" aria-hidden="true" tabindex="-1"></a>nfolds <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb86-4"><a href="machine-learning.html#cb86-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-5"><a href="machine-learning.html#cb86-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>reps) {</span>
<span id="cb86-6"><a href="machine-learning.html#cb86-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-7"><a href="machine-learning.html#cb86-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># generate array containing fold-number for each sample</span></span>
<span id="cb86-8"><a href="machine-learning.html#cb86-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (row)</span></span>
<span id="cb86-9"><a href="machine-learning.html#cb86-9" aria-hidden="true" tabindex="-1"></a>    foldsset <span class="ot">&lt;-</span> <span class="fu">rep_len</span>(<span class="dv">1</span><span class="sc">:</span>nfolds, <span class="fu">nrow</span>(data))</span>
<span id="cb86-10"><a href="machine-learning.html#cb86-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-11"><a href="machine-learning.html#cb86-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create subsetes of data based on the random fold</span></span>
<span id="cb86-12"><a href="machine-learning.html#cb86-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># assignment</span></span>
<span id="cb86-13"><a href="machine-learning.html#cb86-13" aria-hidden="true" tabindex="-1"></a>    folds <span class="ot">&lt;-</span> <span class="fu">sample</span>(foldsset, <span class="fu">nrow</span>(data))</span>
<span id="cb86-14"><a href="machine-learning.html#cb86-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-15"><a href="machine-learning.html#cb86-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># actual cross validation: we allow each fold to act as</span></span>
<span id="cb86-16"><a href="machine-learning.html#cb86-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the test data in turn</span></span>
<span id="cb86-17"><a href="machine-learning.html#cb86-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nfolds) {</span>
<span id="cb86-18"><a href="machine-learning.html#cb86-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-19"><a href="machine-learning.html#cb86-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># split of the data</span></span>
<span id="cb86-20"><a href="machine-learning.html#cb86-20" aria-hidden="true" tabindex="-1"></a>        fold <span class="ot">&lt;-</span> <span class="fu">which</span>(folds <span class="sc">==</span> k)</span>
<span id="cb86-21"><a href="machine-learning.html#cb86-21" aria-hidden="true" tabindex="-1"></a>        data.train <span class="ot">&lt;-</span> data[<span class="sc">-</span>fold, ]</span>
<span id="cb86-22"><a href="machine-learning.html#cb86-22" aria-hidden="true" tabindex="-1"></a>        data.test <span class="ot">&lt;-</span> data[fold, ]</span>
<span id="cb86-23"><a href="machine-learning.html#cb86-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-24"><a href="machine-learning.html#cb86-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># train and test your model with data.train and</span></span>
<span id="cb86-25"><a href="machine-learning.html#cb86-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># data.test</span></span>
<span id="cb86-26"><a href="machine-learning.html#cb86-26" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb86-27"><a href="machine-learning.html#cb86-27" aria-hidden="true" tabindex="-1"></a>}  <span class="co"># repeat for the desired number of interactions </span></span></code></pre></div>
<div id="exercise-3-logistic-regression-as-a-binary-classifier" class="section level4" number="5.2.1.1">
<h4><span class="header-section-number">5.2.1.1</span> Exercise 3: Logistic regression as a binary classifier</h4>
<p>Try to use logistic regression on your own, with a pre-existing dataset in the MASS package. Start by installing the MASS package, load the library and load the data. We’ll be using the Pima.tr data in the MASS package, which describes risk factors for diabetes. Type help(Pima.tr) or ?Pima.tr to get a description of these data. You’ll notice that the “type” variable is our classifier and determines whether the patient has diabetes or not.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="machine-learning.html#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb87-2"><a href="machine-learning.html#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb87-3"><a href="machine-learning.html#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(Pima.tr)</span>
<span id="cb87-4"><a href="machine-learning.html#cb87-4" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(Pima.tr)</span></code></pre></div>
<pre><code>&#39;data.frame&#39;:   200 obs. of  8 variables:
 $ npreg: int  5 7 5 0 0 5 3 1 3 2 ...
 $ glu  : int  86 195 77 165 107 97 83 193 142 128 ...
 $ bp   : int  68 70 82 76 60 76 58 50 80 78 ...
 $ skin : int  28 33 41 43 25 27 31 16 15 37 ...
 $ bmi  : num  30.2 25.1 35.8 47.9 26.4 35.6 34.3 25.9 32.4 43.3 ...
 $ ped  : num  0.364 0.163 0.156 0.259 0.133 ...
 $ age  : int  24 55 35 26 23 52 25 24 63 31 ...
 $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 1 1 1 2 ...</code></pre>
<p>Next construct a logistic regression, to use as a classifier. Examine your output to determine if the regression is significant. We can use “~.” in the formula argument to mean that we use all the remaining variables in the dataset as predictors.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="machine-learning.html#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># run logistic regression</span></span>
<span id="cb89-2"><a href="machine-learning.html#cb89-2" aria-hidden="true" tabindex="-1"></a>Pima.log <span class="ot">&lt;-</span> <span class="fu">glm</span>(type <span class="sc">~</span> ., <span class="at">family =</span> binomial, <span class="at">data =</span> Pima.tr)</span></code></pre></div>
<p>Next, use some testing data to test your classifier. The Prima.te has already been created for you in the MASS package. Use the predict function to get the predicted probabilities, and a threshold value to get classifications. Then construct a confusion matrix to determine how well your predictor did.</p>
<pre><code>      Predicted
Actual  No Yes
   No  200  23
   Yes  43  66</code></pre>
<p>You should notice there there have been some misclassifications at 50%, and that the accuracy is only about 0.8. See if another decision boundary (e.g., 75%) does any better, or use the <em>ROCit</em> library, or other R packages to calculate an optimal threshold or an overall performance across thresholds.</p>
<p>There is a nice package in R, <em>caret</em>, that is a good wrapper for these kinds of tasks, and which is really great for machine learning. We can use it to generate both our confusion matrix, and other statistical info about our classifier</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="machine-learning.html#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb91-2"><a href="machine-learning.html#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(Pima.te<span class="sc">$</span>type, testPima)</span></code></pre></div>
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  200  23
       Yes  43  66
                                          
               Accuracy : 0.8012          
                 95% CI : (0.7542, 0.8428)
    No Information Rate : 0.7319          
    P-Value [Acc &gt; NIR] : 0.002095        
                                          
                  Kappa : 0.5271          
                                          
 Mcnemar&#39;s Test P-Value : 0.019349        
                                          
            Sensitivity : 0.8230          
            Specificity : 0.7416          
         Pos Pred Value : 0.8969          
         Neg Pred Value : 0.6055          
             Prevalence : 0.7319          
         Detection Rate : 0.6024          
   Detection Prevalence : 0.6717          
      Balanced Accuracy : 0.7823          
                                          
       &#39;Positive&#39; Class : No              
                                          </code></pre>
<p>Perhaps the most informative of these stats is the <em>No Information Rate</em> which tests whether our classifier does better than random assignment. We see that our error rate is significantly greater than this no information rate, and so, should be an okay classifier. Also, the <em>Balanced Accuracy Statistic</em> gives an accuracy value that weights both majority and minority classes evenly, and is useful if you have unbalanced class membership in your data. In our case, the accuracy and balance accuracy rates are similar, so we’re doing alright.</p>
<p>The caret package can be use to do lots of the programming required for cross-validation approaches automatically. Going back to the species occurrence data, let’s use the caret package to run a k-fold cross-validation, with 5 folds, rather than simply dividing the data into training and testing sets.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="machine-learning.html#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define the resampling method you want to use, in this</span></span>
<span id="cb93-2"><a href="machine-learning.html#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="co"># case cross-validation with 5 folds</span></span>
<span id="cb93-3"><a href="machine-learning.html#cb93-3" aria-hidden="true" tabindex="-1"></a>train_control <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>)</span>
<span id="cb93-4"><a href="machine-learning.html#cb93-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-5"><a href="machine-learning.html#cb93-5" aria-hidden="true" tabindex="-1"></a><span class="co"># train the model on training set, remember to tell caret</span></span>
<span id="cb93-6"><a href="machine-learning.html#cb93-6" aria-hidden="true" tabindex="-1"></a><span class="co"># that this is a classifier by indicating that occ should</span></span>
<span id="cb93-7"><a href="machine-learning.html#cb93-7" aria-hidden="true" tabindex="-1"></a><span class="co"># be considered a factor variable</span></span>
<span id="cb93-8"><a href="machine-learning.html#cb93-8" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="fu">as.factor</span>(occ) <span class="sc">~</span> temp, <span class="at">data =</span> ivsp, <span class="at">trControl =</span> train_control,</span>
<span id="cb93-9"><a href="machine-learning.html#cb93-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">&quot;glm&quot;</span>, <span class="at">family =</span> <span class="fu">binomial</span>())</span>
<span id="cb93-10"><a href="machine-learning.html#cb93-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-11"><a href="machine-learning.html#cb93-11" aria-hidden="true" tabindex="-1"></a><span class="co"># print cv scores</span></span>
<span id="cb93-12"><a href="machine-learning.html#cb93-12" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>
Call:
NULL

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.9232  -0.6448  -0.4581  -0.2276   1.8596  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -7.0425     1.1779  -5.979 2.25e-09 ***
temp          0.4527     0.0879   5.150 2.60e-07 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 210.76  on 199  degrees of freedom
Residual deviance: 175.95  on 198  degrees of freedom
AIC: 179.95

Number of Fisher Scoring iterations: 5</code></pre>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="machine-learning.html#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(model)</span></code></pre></div>
<pre><code>Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction    0    1
         0 72.5 18.0
         1  5.5  4.0
                           
 Accuracy (average) : 0.765</code></pre>
<p>The caret package neatly does our data resampling, fits the model, gets an average model performance across the 5 testing sets, AND calculates the confusion matrix for us. Not bad!</p>
</div>
</div>
</div>
<div id="tree-based-methods-for-classification" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Tree-based methods for classification</h2>
<p>Tree-based methods for classification involve dividing up regions defined by the predictor variables. For example, simple species identfication trees use this approach. You are looking at a tree: does it have needle- or scale-shaped leaves? Or are the leaves wide? The leaf characteristic predictor is used to divide up the classification possibilities into conifers and deciduous trees (not without errors!). So we repeatedly split the response data into two groups that are as homogeneous as possible. The split is determined by the single predictor that best discriminates among the data. The binary splits continue to partition the data into smaller and smaller groups, or nodes, until the groups are no longer homogeneous. This effort produces a single tree where the binary splits form the branches and the final groups compose the terminal nodes, or leaves.</p>
<div id="classification-and-regression-trees-carts" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Classification and regression trees (CARTs)</h3>
<p>If this type of method is applied with a continuous response variable instead it is called a <em>regression tree</em>, if the response is categorical, it is called a <em>classification tree</em>. Often we refer to both technqiues at the same time as <strong>C</strong>lassification <strong>A</strong>nd <strong>R</strong>egression <strong>T</strong>rees (CARTs). When used as a regression response predictor, this technique differs from standard regression approaches which are global models where the predictive formula is supposed to hold in the entire data space. Instead trees try to partition the data space into small enough parts where we can apply a simple different model on each part.</p>
<p>Let’s try a simple example on the iris data that is built-in to R. Take a look at the data set before your start the example. Then, we’ll first split the data into a training and testing set, and run our classification tree algorithm using the <em>rpart</em> package.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="machine-learning.html#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb97-2"><a href="machine-learning.html#cb97-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.7</span>  <span class="co"># percentage of training set</span></span>
<span id="cb97-3"><a href="machine-learning.html#cb97-3" aria-hidden="true" tabindex="-1"></a>inTrain <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(iris), alpha <span class="sc">*</span> <span class="fu">nrow</span>(iris))</span>
<span id="cb97-4"><a href="machine-learning.html#cb97-4" aria-hidden="true" tabindex="-1"></a>train.set <span class="ot">&lt;-</span> iris[inTrain, ]</span>
<span id="cb97-5"><a href="machine-learning.html#cb97-5" aria-hidden="true" tabindex="-1"></a>test.set <span class="ot">&lt;-</span> iris[<span class="sc">-</span>inTrain, ]</span>
<span id="cb97-6"><a href="machine-learning.html#cb97-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-7"><a href="machine-learning.html#cb97-7" aria-hidden="true" tabindex="-1"></a>mytree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> train.set, <span class="at">method =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb97-8"><a href="machine-learning.html#cb97-8" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb97-9"><a href="machine-learning.html#cb97-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mytree)</span>
<span id="cb97-10"><a href="machine-learning.html#cb97-10" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(mytree)</span></code></pre></div>
<div class="figure"><span id="fig:firstree"></span>
<img src="_main_files/figure-html/firstree-1.png" alt="This figure shows a classification tree using the iris dataset. The top of the plot has label 'Petal.Length&lt;2.45' and splits into two groups: 'setosa' and 'Petal.Width&lt;1.75'. The 'Petal.Width&lt;1.75' group splits into two smaller groups: 'versicolor' and 'virginica'." width="960" />
<p class="caption">
Figure 5.5: A simple plot of a tree classifier for the iris data
</p>
</div>
<p>We can see that petal length is used to distinguish species <em>I.setosa</em> from the other two species, and then petal width classifies into <em>I. versicolor</em> or <em>I. virginia</em>. The model of course includes more information than this regarding the number of observations aggregating to each branch of the tree etc. More detailed information can be obtained from <em>summary(mytree)</em>, or just typing mytree. A nicer plot, with more details can also be obtained with rpart.plot library.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="machine-learning.html#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb98-2"><a href="machine-learning.html#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(mytree)</span></code></pre></div>
<div class="figure"><span id="fig:fancytree"></span>
<img src="_main_files/figure-html/fancytree-1.png" alt="The figure shows a classification tree using the iris dataset. The top right of the figure shows a legend with labels 'setosa', 'versicolor' and 'virginica' which are denoted by an orange square, a gray square and a green square respectively. The top of the plot has a textbox with label 'setosa .36 .30 .33 100%'. Beneath the textbox is the label 'Petal.Length &lt; 2.5' which branches off into options 'yes' to the left of the label and 'no' to the right. Following the 'yes' option is an orange textbox with label 'setosa 1.00 .00 .00 36%'. Following the 'no' option is a light green textbox with label 'virginica .00 .48 .52 64%'. Beneath the textbox is the label 'Petal.Width &lt; 1.8' which splits off into a gray textbox with label 'versicolor .00 .89 .11 33%' and a green textbox with label 'virginica .00 .03 .97 30%'." width="672" />
<p class="caption">
Figure 5.6: A nicer plot of a tree classifier for the iris data made with the rpart.plot package
</p>
</div>
<p>This plot (Fig. <a href="machine-learning.html#fig:fancytree">5.6</a>), in addition to the factor that splits each branch, also tells us the percentage of the data in each class, and the percentage that travels down each branch in each class. Starting at the top, each species makes up roughly a third of the data, after the petal length branch, travelling down the petal length greater than or equal to 2.5, all I.setosa observations all on the other side of the split, and we are left with data divided evenly between the <em>I.versicolor</em> and <em>I. viriginica</em>. The petal length &lt; 4.8 branch separates out these two species, with some error in classification.</p>
<p>The algorithm determines which variable to split based on <strong>impurity</strong>, or how similar points are within a group. If all data points are identical, then impurity is zero. Impurity increases as points become more dissimilar. Impurity is calculated differently for different kinds of trees. For classification trees: the Gini index, which reflects the proportion of responses in each level of a categorical variable is often used. The Gini index is calculated as: <span class="math inline">\(Gini=1-\sum p_i\)</span>, where <span class="math inline">\(p_i\)</span> is the proportion of observations in each class. The Gini index is small when many observations fall into a single category, so the split is made at the single variable which minimizes the Gini index. Some classifiers use the Shannon-Weiner index instead, which has similar properties.</p>
<p>Using this tree classifier, we can make predictions for our testing data, and get a confusion matrix</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="machine-learning.html#cb99-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(mytree, test.set, <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb99-2"><a href="machine-learning.html#cb99-2" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(pred, test.set<span class="sc">$</span>Species)</span></code></pre></div>
<pre><code>Confusion Matrix and Statistics

            Reference
Prediction   setosa versicolor virginica
  setosa         18          0         0
  versicolor      0          9         0
  virginica       0          4        14

Overall Statistics
                                          
               Accuracy : 0.9111          
                 95% CI : (0.7878, 0.9752)
    No Information Rate : 0.4             
    P-Value [Acc &gt; NIR] : 9.959e-13       
                                          
                  Kappa : 0.8649          
                                          
 Mcnemar&#39;s Test P-Value : NA              

Statistics by Class:

                     Class: setosa Class: versicolor Class: virginica
Sensitivity                    1.0            0.6923           1.0000
Specificity                    1.0            1.0000           0.8710
Pos Pred Value                 1.0            1.0000           0.7778
Neg Pred Value                 1.0            0.8889           1.0000
Prevalence                     0.4            0.2889           0.3111
Detection Rate                 0.4            0.2000           0.3111
Detection Prevalence           0.4            0.2000           0.4000
Balanced Accuracy              1.0            0.8462           0.9355</code></pre>
<p>Our accuracy is pretty good for all species, and significantly greater than the no information rate.</p>
</div>
<div id="tree-pruning" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Tree pruning</h3>
<p>So how does the model decide when to stop? Presumably you could continue to build out the tree until every single observation is a node. Another way to phrase this question is: how do you prevent the model from overfitting the data? The answer is: pruning (the best part about this classification method is the metaphorical gardening language!). Pruning is the act of overgrowing the tree and then cutting it back. Ultimately pruning should yield a tree that optimizes the trade-off between complexity and predictive ability.</p>
<p>Pruning begins by creating a nested series of trees of increasing number of branches, from 0 (no splits) to however many can be reasonably obtained from the data. For each number of branches, an optimal tree can be recovered, i.e., one that minimizes the overall misclassification rate.
To select the tree of optimal size we use cross-validation. For a given tree size, cross-validation divides the data into equal portions, removes one portion from the data, builds a tree using the remaining portion, and then calculates the error between the observed data and the predictions.
This procedure is repeated for each of the remaining portions and then the overall error is summed across all subsets of the data.
This is done for each of the nested trees. The tree of optimal size is then determined based on the smallest tree that is with in 1 standard error of the minimum error observed across all trees.</p>
<p>Even with pruning, a single CART is likely to overfit the data, particularly when there are many, many predictors, and thus is not very good for prediction. One way to get around this is to build a bunch of different, non-nested, trees on subsets of the data, and then average accross them. Because any given tree is constructed with only a portion of the data, the likelihood of overfitting is drastically reduced. Moreover, averaging across many trees reduces the impact of anomalous results from a single tree. This is the idea of <strong>ensemble learning</strong>, or combining many ‘weak learners’ (individual trees) to produce one ‘strong learner’ (the ensemble).</p>
</div>
<div id="random-forests" class="section level3" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> Random Forests</h3>
<p>One type of ensemble decision tree is a random forest. Random forests use a bootstrapped sample of the data, and only a portion of the predictors to construct each tree. This procedure ensures that each individual tree is independent from the others, making it a much more accurate method than some other ensemble learning techniques (e.g., bagging). As well, since both the data and the predictors are subsampled, these models can be fit to more predictors than there are observations. This seems a little counterintuitive, but can be a real benefit for ecological data which typically suffers from low replication.</p>
<p>Let’s try an ensemble decision tree on the iris data. We will use the <em>randomForest</em> package. Note that we do not have to split our data into training and testing sets now, the randomForest package is already doing this sort of thing for us.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="machine-learning.html#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb101-2"><a href="machine-learning.html#cb101-2" aria-hidden="true" tabindex="-1"></a>RF.model <span class="ot">=</span> <span class="fu">randomForest</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris)</span>
<span id="cb101-3"><a href="machine-learning.html#cb101-3" aria-hidden="true" tabindex="-1"></a>RF.model</span></code></pre></div>
<pre><code>
Call:
 randomForest(formula = Species ~ ., data = iris) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 5.33%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          5        45        0.10</code></pre>
<p>Our classification is pretty good with a misclassification rate on the <strong>O</strong>ut <strong>O</strong>f <strong>B</strong>ag data of only 4% (this is the equivalent to the test data from a cross-validation), and errors for individual species from 0 to 0.06%. We might like to look at what the model is doing, but unlike a single CART, random forests do not produce a single visual, since of course the predictions are averaged across many hundreds or thousands of trees.</p>
<p>When building random forests, there are three tuning parameters of interest: node size, number of trees, and number of predictors sampled at each split. Careful tuning of these parameters can prevent extended computations with little gain in error reduction. For example, the plot below (Fig. <a href="machine-learning.html#fig:rfoob">5.7</a>) shows how the overall OOB error rate, and the error rate for each of the three species, changes with the size of the forest (the number of trees).</p>
<div class="figure"><span id="fig:rfoob"></span>
<img src="_main_files/figure-html/RFOOB-1.png" alt="This figure shows a plot with y-axis labelled 'Error' and x-axis labelled 'trees'. The top right of the plot shows a legend with labels 'OOB', 'setosa', 'versicolor' and 'virginica' which are denoted by a black solid line, a red dashed line, a light green dotted line and a blue dotted and dashed line respectively. The 'OOB', 'versicolor' and 'virginica' variables show fluctuations in error at the beginning and middle of the plot but flatline towards the second half of the plot. The 'setosa' variable remains shows 0 error and is parallel with the x-axis." width="672" />
<p class="caption">
Figure 5.7: Out of bag error, and individual classification errors for the three species classes in the random forest model
</p>
</div>
<p>Obviously with fewer trees the error rate is higher, but as more trees are added you can see the error rate decrease and more or less flatten out. In the above plot (Fig <a href="machine-learning.html#fig:rfoob">5.7</a>), we could easily reduce the number of trees down to 300 and experience relatively little loss in predictive ability. This is easy to do:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="machine-learning.html#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="fu">update</span>(RF.model, <span class="at">ntree =</span> <span class="dv">300</span>)</span></code></pre></div>
<pre><code>
Call:
 randomForest(formula = Species ~ ., data = iris, ntree = 300) 
               Type of random forest: classification
                     Number of trees: 300
No. of variables tried at each split: 2

        OOB estimate of  error rate: 4%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          3        47        0.06</code></pre>
<p>So we see little change in our error rate.</p>
<p>Despite not yielding a single visualizable tree, we can get information about the random forest model. One metric is the relative importance of the predictors. By ranking predictors based on how much they influence the response, random forests may be a useful tool for selecting predictors before trying another framework, such as CART. Importance can be obtained using the importance function, and plotted using the <em>varImpPlotfunction()</em>:</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="machine-learning.html#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(RF.model)</span></code></pre></div>
<div class="figure"><span id="fig:vipp"></span>
<img src="_main_files/figure-html/vipp-1.png" alt="This figure shows a plot with title 'RF.model' that has y-axis with variables 'Petal.Width', 'Petal.Length', 'Sepal.Length' and 'Sepal.Width' from top to bottom and x-axis with label 'MeanDecreaseGini'. There are 4 points on the graph. Each point corresponds to one of the y-axis variables. The mean decrease in Gini Index is at the lowest for 'Sepal.Width' followed by 'Sepal.Length', then 'Petal.Length' and lastly is at the highest for 'Petal.Width'." width="672" />
<p class="caption">
Figure 5.8: Variable importance plot for our random forest model of the iris data
</p>
</div>
<p>Variable importance reports the mean decrease in the Gini Index for each predictor (Fig. <a href="machine-learning.html#fig:vipp">5.8</a>). If you recall, the Gini index is a measure of impurity for categorical data. For each tree, each predictor in the OOB sample is randomly permuted (aka, shuffled around) and passed to the tree to obtain the error rate. The error rate from the unpermuted OOB is then subtracted from the error rate on the permuted OOB data, and averaged across all trees. When this value is large, it implies that a variable had a strong relationship with the response. That is, the model got much worse at predicting the data when that variable was permuted. As we already knew, Petal.Length and Petal.Width are the two most important variables.</p>
<p>One other useful aspect of random forests is getting a sense of the partial effect of each predictor given the other predictors in the model. This has analogues to partial correlation plots in linear models. We can construct a partial effects response by holding each value of the predictor of interest constant (while allowing all other predictors to vary at their original values), passing it through the random forest, and predicting the responses. The average of the predicted responses are plotted against each value of the predictor of interest (the ones that were held constant) to see how the effect of that predictor changes based on its value. This exercise can be repeated for all other predictors to gain a sense of their partial effects.</p>
<p>The function to calculate partial effects in the randomForest package is <em>partialPlot()</em>. Let’s look at the effect of Petal.Length:</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="machine-learning.html#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="fu">partialPlot</span>(RF.model, iris, <span class="st">&quot;Petal.Length&quot;</span>, <span class="at">main =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;log odds&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:PEplot"></span>
<img src="_main_files/figure-html/PEplot-1.png" alt="This figure shows a plot with y-axis labelled 'log odds' and x-axis labelled 'Petal.Length'. The plot shows a line that begins where 'log odds' is highest and trends downwards as 'Petal.Length' increases." width="672" />
<p class="caption">
Figure 5.9: Partial effect of petal length in the random forest model of the iris data
</p>
</div>
<p>The y-axis is a bit tricky to interpret (Fig. <a href="machine-learning.html#fig:PEplot">5.9</a>). Since we are dealing with classification trees, y on the logit scale, and is the probability of success. In this case, the partial plot has defaulted to the first class, which is <em>I. setosa</em>. The plot says that there is a high chance of successfully predicting this species from Petal.Length when Petal.Length is less than around 2.5 cm, after which point the chance of successful prediction drops off precipitously. This is actually quite reassuring as this is the first split identified way back in the very first CART (where the split was &lt; 2.45 cm).</p>
<p><strong>Missing data</strong></p>
<p>Its worth noting that the default behavior of randomForest is to refuse to fit trees with missing predictors. You can, however, specify a few alternative arguments: the first is na.action = na.omit, which removes the rows with missing values outright. Another option is to use na.action = na.roughfix, which replaces missing values with the median (for continuous variables) or the most frequent level (for categorical variables). Missing responses are harder: you can either remove that row, or use the function rfImpute to impute values. The imputed values are the average of the non-missing observations, weighted by their proximity to non-missing observations (based on how often they fall in terminal nodes with those observations). rfImpute tends to give optimistic estimates of the OOB error.</p>
<p><strong>What else?</strong></p>
<p>We’ve just provided a small sampler of classification methods here that will get you started on your way. But other methods such as <strong>L</strong>inear <strong>D</strong>iscriminent <strong>A</strong>nalysis (<strong>LDA</strong>), <strong>A</strong>rtificial <strong>N</strong>eural <strong>N</strong>etworks (<strong>ANN</strong>), k-nearest neighbors, and <strong>S</strong>upport <strong>V</strong>ector <strong>M</strong>achines (SVM) are also useful for building classification models.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-to-multivariate-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="optimization.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
