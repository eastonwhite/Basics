[["index.html", "Further Git and GitHub 1 Prerequisites", " Further Git and GitHub Andrew Edwards 05/07/2021 1 Prerequisites This is a sample book written in Markdown. You can use anything that Pandocs Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],["intro.html", "2 Introduction", " 2 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2021) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],["literature.html", "3 Literature", " 3 Literature Here is a review of existing methods. "],["introduction-to-git-and-github.html", "4 Introduction to Git and GitHub 4.1 Motivation 4.2 Getting set up for the first time 4.3 Using Git and GitHub 4.4 Beyond the basics", " 4 Introduction to Git and GitHub 4.1 Motivation As a biology graduate student or a professional biologist in a university or government setting, why might you want to use Git and GitHub? And what are Git and GitHub anyway? Scientists (including students) are working far more collaboratively than in the past This involves both sharing code and writing up results There is a push towards open science  including your code as part of a scientific paper We have called this a TTT approach: Transparent  a clear and open way to show data, code, and results, enabling reproducibility Traceable  a clear link from database queries and code to final results (numbers, tables, and graphs in a document) Transferable  it should be feasible for another person to reproduce work and build upon it with a minimal learning curve Using Git and GitHub in your workflow greatly enables this, both when working alone and in a team. Git keeps track of the latest versions of your files, such as computer code or write up for results as you work on them. It also allows you to go back to any previous version of your files (this is version control). (It also does much more). GitHub is a website that hosts a repository of your work code and enables users to easily collaborate. Your repositories can be either public or private. We use Git and GitHub extensively: to collaborate on writing code and producing documents (such as this entire document!). to easily share code publically for scientific papers, and update it if necessary. when working alone to retain a methodical workflow. Example application  Pacific Hake stock assessment Under a formal Agreement between the Canadian and US governments, a team of four of us (two from each country) conduct an annual stock assessment for Pacific Hake (Merluccius productus) off the west coast of Canada and the US. The assessment is used to manage the stock, which is of important ecological and economic value ($100 million export value in Canada). We fit complex population models to data to make projections about future health of the stock under different levels of catch. There is a very short turnaround (five weeks) between getting the final data, doing the analyses (model runs can take many hours of computer time) and submitting the assessment document, which is typically &gt;200 pages and contains numerous figures and tables available here. Prior to 2016, the document was assembled in Word, requiring lots of editing and amaglamating of files, often late at night. Now we share our code via GitHub, automate a lot of the document production using knitr (similar to Rmarkdown, covered in Module 2). So with four people constantly working on the same large document, we need to ensure we are keeping up-to-date with each other, can all produce the latest version, and have identical folder structures on each others computers. The alternative of emailing files back and forth is: very inefficient, prone to errors, just painful. What we can avoid Using GitHub it is easy to see what text/code collaborators have changed, avoiding things like the following, for which it hard to see where to get started: We may want to keep old versions of files (and email them back and forth), but without GitHub we can end up with a veritable gong show: We can avoid having to co-ordinate having only one person working on the latest version of a document, so we dont get things like: Can avoid multiple versions of a file that then have to be carefully merged: While GoogleDocs, for example, is fine for collaborating on a short document, it isnt suitable for sharing complex code, or complex documents that are somewhat automatically updated. GitHub advantages Say youve off on a two-week hike while your collaborators have been diligently working away and they have edited 15 new files of code in five folders, added four data sets, and created five new pages of text towards a manuscript. You can easily catch up with them (get all their changes onto your computer) with a few simple commands. You dont even have to pester them to ask what theyve done, as you can check it yourself. So rather than this conversation: You: Hey, Im back from my awesome trip and saw some bears. What have you been doing with the project? Likely reply: Glad you had fun. Im busy on something else right now. Er, where were we at when you left? You can have this one: You: Hey, Im back from my awesome trip and saw some bears. I went through your commits on GitHub and everything looks great. Shall I get on those Methods Issues you assigned me? Likely reply: Glad you had fun, looking forward to hearing about it. Im busy on something else right now so, yes, resolving those issues will be great, thanks. And the project keeps moving in an efficient way. By having code shared publically, it is easy to answer questions, such as this one I received: Rather than go searching on my laptop for the code that I hadnt looked at for six months, I could click on the link the questioner sent and answer very quickly, with a simple link to the file I am referring to (there is no ambiguity): You can even ask who last edited a particular line of code/text (GitHub amusingly calls it Blame): You can properly keep track of (and discuss) Issues to be thought about or fixed, rather than having things in emails that get forgotten: Important: You still have all your work locally on your computer. So if your internet access goes down or GitHub is unavailable (which of course will only happen when you have a deadline) you can still carry on with your work. Why this course? Delving into the Git and GitHub world online it can feel like you need a computer science degree to get started. This may not be surprising as Git was writting by the guy who wrote the operating system Linux, to help people collaborate on writing the operating system Linux. But it means that, for example, the second paragraph of the Wikipedia Git page says: As with most other distributed version control systems, and unlike most clientserver systems, every Git directory on every computer is a full-fledged repository with complete history and full version-tracking abilities, independent of network access or a central server. Say what??? That is fairly incomprehensible to those without strong computer science backgrounds. The aim of this module is to introduce biologists to the world of Git and GitHub, while avoiding a lot of the technical details. However, once you have mastered the basics then it should be easier to delve deeper. Our target audience is: graduate level biology students biology faculty government scientists scientists in non-governmental organisations in fact anyone wanting to learn these tools This work is extended from lectures and exercises developed by Chris Grandin and myself as part of a Fisheries and Oceans Canada workshop. (Luckily Chris does have a computer science degree, and so was able to get some of us going with Git and GitHub several years ago). These tools are now widely used within our organisation. Computer language For sharing code, it doesnt matter what language your code is in (R, Matlab, Python, C, ), as we will just be sharing text files. There is a learning curve, but once you get going you only really need a few main commands. Unfortunately the hardest bit is actually getting everything set up. 4.2 Getting set up for the first time Before you start using Git you need to set up your computer to use it, and install a few other programs that are useful. This is a one-time setup and once it is done, you will be able to easily create new projects or join others in collaboration. We have tested the installations as much as feasible. If you have an issue then search the internet, as it may be due to some configuration on your particular computer. This module is for any operating system: Windows, MacOS, Linux or Unix. 4.2.1 What you will end up having installed These are programs/things you will need (instructions are on the next slides). Obviously skip any that you already have working. A GitHub account A text editor that isnt Notepad Git on your computer Diffmerge or something similar for comparing changes to files (not completely necessary) Markdown Pad 2 or Chrome extension or something similar for viewing Markdown files (not completely necessary) 4.2.2 Get a GitHub account Sign up for GitHub: http://github.com If possible, choose a user name that will make sense to colleagues, e.g. andrew-edwards or cgrandin, not pink-unicorn. Desirable: attach a photo (headshot) to your profile. This makes it easy for collaborators to identify you. 4.2.3 Text Editor You must have a text editor that is aware of outside changes in a file. This is necessary because if you have a file open in the editor and you download an updated version of the file, you want the editor to ask you if you want to use the updated version. We know that Emacs, Xemacs and maybe Vim are okay, as is RStudiofor.R (and other) files. Notepad is not okay. But you can download and install Notepad++ which is fine: https://notepad-plus-plus.org/download/v7.3.3.html 4.2.4 Install the Git application on your machine See https://git-scm.com/downloads for downloading instructions for Windows, MAC and Linux/Unix It seems best to accept the default options, except NOT Notepad or Vim (unless you use Vim) as the default editor. 4.2.5 Git shell For this course we will use a simple git shell to type commands (rather than a point-and-click Graphical User Interface). This is for several reasons: Commands are the same across operating systems. It is easier to demonstrate (and remember) a few simple commands, rather than follow a cursor moving across a screen. Learning the text commands will give you a good understanding of how Git and GitHub work. It is easier to Google for help when you get stuck or want to learn about more advanced options. Commands are quick, and you can usually the up arrow (or ctrl-up-arrow) to retrieve recent commands, or auto-complete commands using . 4.2.6 Git shell, RStudio There are many Graphical User Interfaces that are available, as described at https://git-scm.com/downloads/guis. Many (but not all) biologists use R in RStudio for their analyses. There is Git functionality built in to RStudio that we (TODO: SOMEONE?) will demonstrate later. I use magit which works in the text editor emacs (which for years I have used for pretty much everything, such as editing files, running R, Matlab, etc.). But I would not have been able to learn magit without first knowing the Git commands from using the shell. For now we will stick with the Git shell for the aforementioned reasons. It will also give you a better understanding of Git and GitHub, and emphasise that you can use Git for any files, not just R code. 4.2.7 Powershell and posh-git Download a Powershell (a shell window in which you can type commands, presumably the power part means its more powerful than a basic version) and then posh-git following the instructions at https://github.com/dahlbyk/posh-git Do the Installation and Using posh-git sections. If you dont understand some options (I dont!) just pick the simplest, usually the first. The next slides are from our course about three years ago (and were for Windows). So they may be out of date (though first one is recent tips from a colleague). [Maybe we should see https://upg-dh.newtfire.org/explainGitShell.html] 4.2.8 One-time authentication The first time you get set up or start using Git, there will be some one-timeauthentication to connect to your GitHub account. Follow any instructions. 4.2.9 Configure the Git application Windows Create a github directory, such as C:. It is fine to put it in a differentpath, but make sure there are no spaces or special charactersanywherein the fullpath. This is where you want to be saving your work that you are tracking with Git. TODO: Andy has to reinstall anyway and will write something here. Think its just following instructions. MAC Create the directory ~/github Enjoy a beverage TODO: Check with Luwen if it is that simple 4.2.10 Install the difftool The difftool will be used to examine differences between different versions of files and also to simplify merging of branches and collaborators code. There are many programs that can be used but for consistency we will use Diffmerge. It is nice to have but not essential if you have trouble installing it. Install Diffmerge: https://sourcegear.com/diffmerge/downloads.php The configuration for directing git to use Diffmerge will be done below. 4.2.11 Cloning the git-course repository On the GitHub webpage, sign into your account and navigate to: https://github.com/quantitative-biology/module-1-git Windows: Open the Git shell and run the following command to clone the repository (clone means copy all files in the repository to your computer): git clone https://github.com/quantitative-biology/module-1-git MAC: Open terminal and change to the GitHub directory: cd ~/github then run the clone command: git clone https://github.com/quantitative-biology/module-1-git You now have the files for the GitHub course on your computer 4.2.12 Copy the .gitignore file Git uses a configuration file for your account info, name to use when committing, aliases for commands, and other things. Open up the misc sub-directory in the git-module-1 directory and copy the file .gitconfig. For Windows, copy this file (overwrite the existing file) to: C:\\Users\\YOUR-COMPUTER-USER-NAME\\.gitconfig, where YOUR-COMPUTER-USER-NAME is your username on your computer, not your GitHub account name. For MAC, copy this file (overwrite the existing file) to: ~/.gitconfig 4.2.13 Edit the .gitconfig file Use your favourite editor to edit the new file (not the one ingit-course/misc). Change the [user] settings to reflect your information. Change the [difftool] and [diffmerge] directories so they point to the location where you have DiffMerge (if it installed okay).. For Windows the location should be: C:\\Program Files\\SourceGear\\Common\\DiffMerge\\sgdm.exe For MAC the location should be: /usr/local/bin/diffmerge If you could not install [difftool] or [diffmerge] then delete those lines in your .gitconfig file. 4.2.14 MAC only: make your output pretty On the MAC, change the ~/github directory and run the following command: git config global color.ui.auto This will make your git output colored in a similar way to the Windows powershell version. 4.2.15 Markdown Pad Each project has an associated README.md file that appears on its GitHub homepage.The extension .md stands for Markdown and is just an ASCii text file that contains simple formatting (such as bold or italics). There are two options we have used to readmarkdown files, choose one: The Markdown Pad 2 editor/viewer which is easy to use: http://markdownpad.com. Just get the free version. The Chrome extension for markdown viewing: https://chrome.google.com/webstore/detail/markdown-viewer/ckkdlimhmcjmikdlpkmbgfkaikojcbjk?hl=en. 4.3 Using Git and GitHub This section also has a recorded lecture to demonstrate the main concepts and ideas. The video is available here TODO and the slides from the talk are here TODO, though the notes below mostly replicate the slides. 4.3.1 Definitions Lets start with some definitions: Repository  essentially a directory containing all your files for a project (plus some files that Git uses). Git  a program that allows you to efficiently save ongoing versions of your files (`version control). GitHub  a website that hosts your repositories so that you can easily share code and collaborate with colleagues. Basically, the idea is that you work on your files in a repository on your computer, use Git on your computer when you are happy to keep your changes, and use GitHub to easily share the files. Here you will learn the important steps: Creating  create a new repository on GitHub Cloning  copying it to your local computer Committing  the crux of working with Git Collaborating  efficiently work with colleagues Conflicts  fixing conflict changes (happens rarely) 4.3.2 Creating a new repository Sign into your GitHub account, click on the Repositories tab, and press the New button. Give your repository a name. Lets call it test. Check Initialize this repository with a README. Leave Add .gitignore and Add a license set to None Click Create repository. You now have a new repository on the GitHub website. Next we will clone it onto your computer. 4.3.3 Cloning your new repository Copy the full URL (web address) of your test repository. Open the Git shell and navigate to your C://github directory (or whatever you called it when you created it in the setup instructions  its the place you are going to save all your Git repositories). Run the following command to clone your repository: git clone URL where URL is the url of your newly created repository (paste should work). You should now have a subdirectory called github/test on your computer. In Git shell, change to that directory (with cd test). So clone is Git speak for copying something from GitHub onto your local computer. This example has just one file (README.md). But the process is the same for a repository with multiple files and multiple directories, and the complate file sturcture is fully preserved. Windows only: Storing your credentials When you are using the Git shell for the very first time on Windows, issue the following command: git config --global credential.helper wincred This means that you dont have to repeatedly enter you GitHub password (just do it when you are first prompted). 4.3.4 Committing Create a new file, newFile.txt, in the github/test directory. Add a line of text at the start of the file and save it. Check the status of your (test) repository: git status It should say that you have an Untracked file called newFile.txt. You want to tell Git to start tracking it, by using: git add. gitignore Type git status again. You should see that the file is listed as a new file under Changes to be commited. Lets now commit it: git commit -a -m \"Add newFile.txt.\" The commit message (in the quotes) should be a useful message saying what the commit encapsulates (more on that later). Push the commit to GitHub: git push Check (refresh) the GitHub webpage and see your commit and the uploaded file. What just happened? We just used three of the main Git commands: git add &lt;filename&gt;  tell Git to start keeping track of changes to this file. You only need to tell Git this once. git commit -a -m \"Message.\"  committing your changes, which means tell Git you are happy with your edits and want to save them. git push  this sends your commit to the GitHub website. You always have your files stored locally on your computer (as usual), even if you dont add them or commit changes. When you push to GitHub then your colleagues can easily fetch (retrieve) them. Keyboard aliases (shortcuts) Now, git commit -a -m \"Message.\" is a bit much to type, so we have an alias for it: git com \"Message.\" This is defined in the .gitconfig file you installed in the git-setup instructions into C:\\Users\\YOUR-USER-NAME\\.gitconfig (for Windows). YOu can also add your own commands to that file. The -a means commit all changes of files that Git is tracking, and -m is to include a message. Since we usually want to do both of these, git com \"Message.\" is a useful shortcut. But it is important to realise it is an alias if searching online for help. Similarly: git s  for git status git p  for git push git d  for git diff git f  for git fetch From now on we will mostly use the aliases. Use the full commands if the .gitconfig file didnt work for you. Edit Readme.md Edit the Readme.md file. Add some simple comments describing the project such as: A test repository for learning Git. Look over the changes, commit them, and push them to your GitHub repository: git s git d (or git diff)  this gives a simple look at the differences between the last committed version and your current version (of all files; only one in this case) git com Initial edit of Readme.md git p (or git push) Refresh your GitHub web page and you should see your text (the Readme.md file is what is shown on the main page of your repo). If you got Diffmerge installed okay, instead of git diff you can do git difftool. This opens up, in turn, each file that changed since your last commit and shows you the differences. This is useful for changes that are more complex than can be easily see in the quick git d. 4.3.5 Exercise 1: create, edit and commit simpleText.txt Create a text file simpleText.txt in your local test repository. Add a line of text at the start and save it. Predict what git s will tell you, then type it in the Git shell to check. Add the file to the repository using the git commands: git add simpleText.txt git s  not necessary but useful to check you understand what is changing before you commit git com \"Adding simpleText.txt\" git p Add some more test to simpleText.txt then git com \"Message.\" and git p. Repeat this a few times to get the hang of it. git com frequently and git p occasionally (you do not have to push every commit), while intermittently doing git s and git d to understand whats changing. Keep and eye on your commits by refreshing the GitHub page. In reality when writing code/text you wont be committing quite so frequently, as your focus will be on the writing. Adding multiple files at once Often you add multiple files in a new directory. When you run git s, you will see a large list of Untracked files. They can be all added at once by simply adding the whole directory. 4.3.6 Exercise 2: multiple files Do the following, to get the idea of creating multiple files in a folder and committing that folder. Create a new directory in your test repository, using your normal method. Call it new-stuff. Add a few new test files to that directory called test1.txt, test2.txt, etc. Put some example text in one or more of them if you want. On the command line, check the status: git s You will see a listing showing the new-stuff/ directory in Untracked files. To add all the new files in preparation for a commit, issue the command: git add new-stuff/ Check the status of the repository again with git s It will now show all files in Changes to be committed Commit the changes: git com \"Added new-stuff directory.\" Push the changes to GitHub: git p Check your GitHub webpage and see your commit and that the files have been uploaded. That works no matter how many files are in your new-stuff directory. There could be a hundred and its the same command. Wildcard symbol * This is useful to know (no need to do it as part of the exercise): To add multiple files with similar names you can use the wildcard * symbol. You just added (told Git to keep track of) the new files in your new-stuff/ directory. If you add more new files to that directory, you will have to tell Git to track those also. This is because they are new  you havent told Git about them yet. Say you have 10 new files called idea1.txt, idea2.txt, , idea 10.txt. Instead of typing git add new-stuff/idea1.txt git add new-stuff/idea2.txt etc. you can just use the wildcard *: git add new-stuff/idea*.txt or even just git add new-stuff/*.txt or git add new-stuff/*.*. The .gitignore file But what if you dont want to add all the files that you create? Each repository can have a .gitignore file, in the root directory of the repository. Such a file has names of files (such as my-secret-notes.txt) or wildcard names (such as _*.pdf_ or _*.doc_ ) that will be completely ignored by Git. For an example, see https://github.com/pacific-hake/hake-assessment/blob/master/.gitignore, noting that the # can be used for comments. When sharing a repository with others, you want to share your code (for example, R, Python or Matlab code) and maybe data, but generally not share the output (such as figures that the code generates; more on this later). For reproducible research your colleague (or anyone) should be able to run your code to generate the results. Some programs you run may make temporary files that dont need to be tracked by Git, the names of which should also be included in your .gitignore. When sharing code or collaborating you want to keep your repository as clean as possible and not clutter it up with files that other people dont need. So when you run git s and see untracked files that you dont want to be tracked, add them (or a suitable wildcard expression) to your .gitignore file so that they are not added inadvertently. This will also simplify your workflow (you dont need to keep being reminded that you have untracked files). If you are on MacOS and you find that folders have a .DS_Store file in them, then create (and add and commit) a .gitignore file with .DS_Store as a line. Git Workflow You have now learnt the basics of using Git. By creating a public repository on GitHub you can now release your code to the world! You can also choose the private repository option when creating a repository, so that you can control who can see it. Go into Settings--Manage Access to add collaborators. 4.3.7 Collaborating Now we will show how to collaborate with colleagues, which is where the usefulness of Git will become more apparent. There are a few different ways to collaborate using Git and GitHub. We will focus on the following one since it is the simplest, and is what you need to collaborate with colleagues. Concept: there is a project where people contribute to a main repository that is considered the master copy. Everyone clones directly from the creators repository. All collaborators push their commits to the main repository (the creator has to add them as collaborators once on GitHub). Since the creator has to grant permission, you wont have just anyone contributing to (and maybe messing up your work), just your trusted collaborators. But you have to trust your team to not mess things up (more on that later!). Okay, so in the video we demonstrated the following: Kim creates new repo called collaborate (and clones it to her computer). Andy clones it also. On GitHub, Kim gives Andy push access to her collaborate repo. Both do some edits (create some new simple text files). For Andy to get Kims updates (and vice versa), it was just: git fetch (or just git f)  fetches the latest version of the repository from GitHub onto your computer. Your local files have not yet changed (check them), but Git has the changes stored on your computer (?!?). git rebase  updates your local repository (the committed files on your computer) with the changes you have just fetched, merging both peoples work together. git p  pushes the merged changes back up to GitHub so that the other person can get them. That is the basic workflow. We also showed an example of git p not being allowed for Person A because there are recent commits on GitHub (by Person B) that Person A has not yet merged into their local version of the repository. Here is an example of the error message you get: While a bit lengthy, the error message is useful. It forces you to get the other persons work before you push yours. You do this by: git f git rebase. So to be allowed to push, just fetch to get the new commits onto your computer, and then rebase to combine the commits into your local version. Then you can git push. Here is a full screenshot (g is just a shortcut for git). The green up arrow number 8 tells me I have 8 commits to push to GitHub. The yellow arrows I think of as just implying I need to do a rebase (before doing that I might browse through the other persons commits on GitHub): After the rebase I was allowed to push and then everything is up to date. A bit more about git rebase Andy commits local changes, tries to git push but is told to first git fetch (to get Kims changes from GitHub). Andy does git fetch and then git rebase. What git rebase does is basically rewind to the last common commit that both people had, and then add one persons commits and the others. Andy then does git push to push his commits to GitHub (from where Kim will fetch them when shes ready). Providing there are no conflicts, this will work fine. Another option is to do a git merge, which basically creates a new commit that merges both peoples work together. Our groups used to use git merge and now use git rebase; some people dont like git merge because it adds extra commits. For a more in-depth understanding see https://reflectoring.io/git-rebase-merge/ for one of the clearer explanations out there concerning rebase v merge. Note that the error in the above screenshot (when I could not git push) told me that I might want to do git pull. This is basically git fetch git merge in one command, but it seems preferable to do git fetch git rebase. Fixing a conflict A conflict happens when two people have edited the same line(s) of the same file. Conflicts happen relatively rarely and can be generally avoided by co-ordinating with collaborators so that you are working on different files. But, they will happen and you need to know how to resolve them. Git forces you to explicitly decide whose changes to keep  this is a good thing, since you want a human to make such a decision. In the video we demonstrated a conflict. Fixing a conflict The best approach I have found to fixing a conflict is the following: Trying git rebase will tell you there is a conflict. git rebase --abort  do this to abort the rebase attempt. git merge  this will tell you there is a conflict. Open the file(s) with the conflict and edit the text (see below). git add &lt;filename(s)&gt;  you have to then add the files that had the conflict (I am not sure why this is necessary, I just do it). git com \"&lt;message&gt;\"  in your commit message you can explain how you fixed the conflict. This is useful so that your collaborators know you have resolved a conflict (they can look at the commit to see if they are happy with it). The merge message will tell you which files are conflicting. Open those files one by one, and you will see the conflicted section bracketed like the following: &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Line(s) of text/code which are currently in your file. ======= Line(s) of text/code which are trying to merge in, but conflict. &gt;&gt;&gt;&gt;&gt;&gt; origin/main where origin/main refers to the version you have fetched from GitHub. All you do is remove the line(s) of text that you do not want to keep (or edit the line(s) to be something else entirely), and remove the bracketing lines &lt;&lt;&lt;... and &gt;&gt;&gt;..., and the ====== line. Save each conflicted file and then (as mentioned previously): git add &lt;filename(s)&gt; git com &quot;Kept Kim&#39;s edits as more consistent with remaining text.&quot; git p 4.3.8 Exercise 3: collaborating on a single repository If you have a colleague available, try what we just did: Person 1 creates a new repository on GitHub and clone to their computer. Give the Person 2 push access to the repository (on the repo page on GitHub: Settings  Manage access  Invite a collaborator) Person 2 clones to their computer Both create a simple text file (use different filenames), add some text and, as usual, add, commit, and push. git fetch and git rebase to get the other persons file. Continue editing either file, committing, and pushing. If you get the push error (shown earlier), refresh GitHub repository site to see recent commits (click on the XX commits link). You can easily spot the other persons recent commits. Click on one (the bold message) to see details. Purposefully create a conflict (both edit the same line of the same file). Resolve it as described earlier. In practice you wont commit so frequently when working, but this is good to get the hang of it. Congratulations Congratulations, you now know the few basic commands and functionality needed to collaborate with Git and GitHub. It takes a bit of practice, but it is very powerful. 95% of the time, this is all you are doing: Change some code. git s git d git com &quot;My commit message&quot;` git p (the git s and git d are useful to check you have changed only what you think you have changed). If GitHub does not allow you to push: git fetch git rebase If conflicts, then git rebase --abort git merge fix the conflicts manually and then git add &lt;conflicted file(s)&gt; git com &quot;Message to explain what you did&quot; git p Change some code and repeat! The next section gives slightly more advanced background that should further improve your understanding (including why Git is useful even when not collaborating or sharing your code), plus tips for improving your workflow. 4.4 Beyond the basics Here are some Git and GitHub concepts and tips that go beyond the basics that we just covered. 4.4.1 Workflow tips Realise that you still edit and save your files in the usual way on your computer. If you dont do Git commits you will still have the latest versions of your files on your computer (as you would if you werent using Git). So if you do get stuck with Git you can carry on working as normal (though you probably do want to try and fix it at some point). When collaborating: If working closely with others, when you start each day (or after a break) then make sure you are up to date and have all their commits. Refresh the GitHub page for you repository, and git fetch (or just git f)and git clone if needed. (To be safe you can git f and git s to check). We find it helpful to co-ordinate our work (Slack is useful for this, or use GitHub Issues for complex discussion  see below), so that if multiple people are working at the same time, you are at least not working on exactly the same parts, just to reduce conflicts. Commit fairly frequently and write helpful commit messages (so your colleagues get an idea of what youve done in each commit). Push less frequently, and dont push code that doesnt work  that will annoy your colleagues. And then they (and probably you) may both spend time fixing it. To see who last edited a particular piece of code, when viewing the file on GitHub click Blame (you can even click on the square icon to go to the previous commit that changed that line): GitHub Issues GitHub Issues are very useful for discussing issues with your repo. For our annual Pacific Hake assessment we have used them extensively over the years: The Issues tab lists our current Open issues  we have 20, of which five (the most recently posted) are shown here. We are currently in-between assessments (and not working on it), so we have created Issues that we want to think about or deal with for next year. This avoids forgetting about ideas or losing them in old emails. Issues are intuitive to use. There is a bright green New Issues button to create new ones, you give a title and then write some details, people can reply, you can assign people to look at them, and you can close them. In the above screenshot you can see that we have closed 815 issues (this was over several years). Useful tip: when doing a commit that refers to an Issue, if you refer to the Issue number (with #&lt;number&gt;) in your commit message, then after pushing that commit the Issue on GitHub will automatically mention and link to the commit: git com \"Add more options to fancy_function(), #21.\" will mention the commit when you look at the issue. You can even automatically close the issue by saying closes #21 in your commit message: git com \"Add more options to fancy_function(), closes #21.\" Issues are particularly useful to avoid cluttering up code with commented notes or ideas that you may easily not come back to, or avoiding endless emails that end up getting overlooked. You dont have to fix an Issue to close it, you can decide not to pursue, but at least you have made a decision. (We also use Slack a lot to communicate, but moreso for quick questions or bouncing ideas around  Issues are better for stuff that you want to come back to at some point). You may receive emails regarding Issues, but if you use GitHub a lot you will see Notifications (the blue dot on the bell in the top-right corner when signed in on GitHub) and that will show you new Issues of repositories you are involved with, or if anyone has updated an Issue. GitHub organizations If you will frequently collaborate with colleagues, you can create an Organization on GitHub and invite collaborators to it (click on your GitHub photo in the top-right corner, Settings, Organizations). Then they will automatically have access to all repositories created under the Organization. You can choose the security settings. 4.4.2 So Ive made some changes but dont really want to keep them  git stash If youve changed some code but have not committed it, and then maybe got in a mess and just want to go back to your last commit, you can stash your changes git stash and to include a message (for your future self): git stash save &quot;Message&quot; This stashes them away such that they can be retrieved later This is handy. You may think you dont want to keep those changes, but sometimes you may later wish you had kept them somehwere. Note this only for files that Git is tracking (i.e. files that have been added at some point). You can have multiple stashes, seen by doing: git stash list To retrieve the last stash: git stash pop TODO: check rules for other stashed things. TODO: Often youre working on something but its not finished, but you want to push it. Stash, then make branch then copy files in. Tricky to do all with git, so do manually. TODO: see my README for details, and test what to do 4.4.3 Pull requests TODO 4.4.4 The power to go back With Git you can revert back to any previous state of your repository. This is very powerful, though slightly scary at first. Do this with your test repository, that should have some files in it from the earlier excercise: git s to make sure you are all up-to-date (commit and/or push if necessary). In File Explorer (or whatever you use) look at your repository, you should see all your files, including the new-stuff\\ directory. Look at the commit tab on GitHub for your test repo and click on the clipboard icon to copy the HASH number thingy to the clipboard . In Git shell: git checkout HASH (where HASH is the pasted HASH, or git co HASH using our Alias) Look at File Explorer again  your new-stuff directory should have  disappeared!! (If it hasnt disappeared then open it  the test files, i.e. test1.r, test2.r, etc. should be gone, but your text editor may have saved backup versions; manually delete them plus the new-stuff/ directory.) You are now back to the very first version of your repo! Powerful and scary. Now, to get your files back to the most recent version you had committed: git checkout main (it used to be git checkout master, the names have recently changed). Thats it! Check that your files are back. All this means that you can revert to any previous commit in your repository. This is very reassuring. For example you have some complex code that you realise is now a complete mess and you want to go back to yesterdays version of everything. In practice you rarely actually do this, but its very comforting to know that you can. Consequently, your workflow is less cluttered and more tractable than having to save multiple versions of the same files with dates in the filename, such as this nightmare: Retrieving older work in practice I think there are fancy ways that Git can replace a current file with a version from an earlier commit. But, in practice (especially since you rarely want to do this) it is a bit safer to do the following: Say you are up-to-date (git s says all is good), but your program my_code.R just isnt working and you want to go back to the version you had yesterday at commit number abc123. git co abc123 (or git checkout abc123) to checkout the earlier commit, which includes the old version of my_code.R that you want get. Copy my_code.R to a new file my_code_old.R. In the shell you can just do this with cp my_code.R my_code_old.R. Do NOT edit my_code.R or make any changes, as you may end up with a scary DETACHED HEAD warning. git co main to checkout the latest version again. Since you have NOT done git add my_code_old.R, Git is not tracking my_code_old.R and so it is just sitting in your folder as normal. Now you can manually copy what you want from my_code_old.R into my_code.R to fix your problem. It could be the full file, or just some part of it. Then commit as normal. At some point you can delete my_code_old.R so it is not hanging around, but you dont have to. (Though maybe make a note in it as to which commit it was from, in case you do need it again). 4.4.5 So how does Git do all this? By now youre probably wondering how Git keeps track of everything. Git does not keep versions of code, it keeps commits. The commits are kept track of using a HASH key which is a generated 40-digit key in hexadecimal (base 16). The hashes are what you see on GitHub and in various places when you use Git shell. By stitching all the commits back together again, Git can recreate all your code. There is a hidden .git/ directory in each repository. Look at the .git/objects/ subdirectory. Each subdirectory name is the first two digits of a HASH. The rest of the digits of the HASH are the filenames in the subdirectory. You can basically think of the hashes as representing commits (apparently they can also be blobs and trees, whatever they might be). I think of the files in the subdirectories containing the differences between each commit. Because of these structures, Git can go back and rebuild any or all files at any commit, and even have different directory structures at each commit. Since Git is keeping track of differences between files, this all works best for plain ASCii (text) files, such as .R, .txt, .Rmd, etc. Git does work for binary files, such as .xls, .docx, .RData, but since changes to the files are not easily saved (Git essentially has to resave the whole file at each commit), this is not very efficient and may make your repository large. Such files will be fully resaved every time they are changed. Think of a binary file as something that you cannot open in a text editor and read (it does not contain simple ASCii letters and numbers). Exceptions: often you may have an image or photo or other type file that you need to share for a document, but it isnt going to keep changing. So thats fine to commit. An example of why you should not commit binary files: A collaborator was running some R code (and correctly committed the .R files so that I could run it), but also committed the results, which included .pdf, .png and .RData files, which can get quite large. But, these latter files got updated every time the code was run. So changing one line of the .R code (which Git deals with very efficiently), and running that code and committing, resulted in the new .pdf etc. files being fully saved (since Git cannot just save the difference from the last commit because they are binary files). Even if, say, one point changes on a figure in a graph in a .pdf file, Git has to save the whole new version. This ended up with .git/objects/pack (whatever that might be!) being 2.8Gb. I needed space quickly on my computer so just deleted four files in .git/objects/pack, which freed up 1.6Gb. Note that I still had the actual final versions of files (as you would if not using Git), but just not the full repository history. However, when I tried to later do some work and then commit I got lots of fatal errors with scary messages like bad object HEAD and the awesomely titled You are on a branch yet to be born: I just had to start again from scratch (reclone I think). Take-home message: Dont mess with the .git directory!! 4.4.6 Git terminology At some point you will likely need to search online for some help (often questions are posted and answered on the excellent stackoverflow website). A bit more understanding of terminology will help you. Remember that Git keeps commits. Several of these commits have pointers to them that have special names: HEAD points to the commit you are currently on in the Git shell. main or master is the default branch when you set up a repository on GitHub (there are two names because of recent changes on GitHub). 4.4.7 Branching So far we have only worked on the main branch. Sometimes you want to create a new branch that branches off from the main branch. Its bit like a tree branching, except that at some point you want your new branch to be merged back into main. For example, you may want to try adding some new code to your project, but dont want to break what is already there. You may do this even if working alone, but its especially useful if you are collaborating, or if, say, you have an R packages hosted on GitHub that anyone may be downloading  you dont want to annoy them by pushing experimental code that doesnt work. So you would create a new branch, work on that new branch (i.e. commit changes to the new branch), and when you are happy with your new changes you can easily merge it all back into main. Working on a new branch When creating a new branch, your starting point is identical to the branch you were when you created the new one. In the Git shell navigate into your test repository: cd test Depending on your set up, you should see main indicated somewhere (if not do git s and it should say On branch main. Make sure you are up-to-date and have committed all changes (git s, and commit if necessary). Create a new branch called temp, this will be based off the latest commit of the main branch you are currently on: git checkout -b temp (We have an alias for that: git cb temp`). You will be automatically placed in the new branch called temp, and commits you make will now occur in that branch only. Make and commit some changes (e.g. add a new file)  these will now be on your temp branch. You can push to GitHub. The first time you try git p, the Git shell will tell you that you need to type the following so that future pushes go to the new branch: git --set-upstream origin BRANCH-NAME Check the GitHub webpage to see that your branch was pushed. You repository page (that will still be looking at your main branch) may tell you that there is a temp branch with more recent commits than main. If not then if you click on the main drop-down menu: it should give you the option to look at your new temp branch. (The 1 branch in the above image should also say 2 branches). You can now view your new file in your new temp branch on GitHub. A graphical way to see and understand branching is to click on InsightsNetwork to see the Network graph. The Network Graph is a useful visualization tool, where each commit is shown as a point on the graph (the numbers along the top are the dates). You can hover your mouse over a commit to see who committed it and the commit message. You can click to see full details of the commit. The Network Graph is particularly useful if you or others are working on multiple branches, or to check details about merges. Okay, back in your Git shell you can easily switch back to your original main branch: git checkout main (or the alias git co main). You will see that the file you just added is gone, because it only exists in the temp branch at this moment. Imagine that in your temp branch you did several commits to create a new function in your code, or have added some new text to a report. Now you are happy with what youve done you want to merge it back into the main branch. To view all local branches: git branch There is an asterisk next to the branch you are currently in. To switch to another branch (main in our case): git checkout main To combine the changes from the temp branch: git rebase temp or git merge temp Now the file you created in the temp branch now appears in the main branch. All commits done in the temp branch will now be in the main branch as well. If there was a merge conflict, you must fix it at this point (see earleir). Once youve merged your temp branch into main, you dont really need temp any more and so it is good protocol to delete to keep things tidy: git branch -d temp If you have unmerged changes in a branch, you will not be allowed to delete it, but Git shell will tell you the command to forcibly delete it: git branch -D temp Warning  you wont be able to get any of those changes back once you do this. To remove a branch entirely from GitHub: git p origin --delete BRANCH-NAME 4.4.8 Undoing stuff If you make a commit followed by other commits, then realize you want to undo the earlier commit, you use revert: git revert HASH where HASH is the hash for the commit you want to undo. Remember that Git shell is smart enough that you only need the first five digits: git revert 1ef1d This actually creates a new commit with the automatic message Revert \"&lt;previous commit message&gt;\". Obviously, you have to be careful with this if youre changing something that was a few commits back, as you might mess up your code. Undoing changes not yet committed If youve made a mess in your working directory and you want to change everything back to the way it was on the last commit: git reset --hard HEAD If youve messed up a single file and just want that one file to go back to the way it was on the last commit: git checkout HEAD &lt;filename_to_restore&gt; Warning  running these commands will delete the changes you have made. Since you have not committed any changes, they will be lost. Make sure you are certain you dont need the changes before running these commands. If you arent sure if you need the changes again in the future, use git stash instead. Changing the commit message in the last commit If you make a commit then realize you want to change it (add more information, fix something that will confuse your colleagues, fix something that will confuse you tomorrow), you can change the commit message: git commit --amend -m \"Correct message.\" This only works on the last commit. If you already pushed the commit before realizing that the message needs modification, do this: git p --force after making the amendment to the commit message. "],["introduction-to-r-markdown.html", "5 Introduction to R Markdown 5.1 Motivation 5.2 Basic idea 5.3 Simple example 5.4 Output format 5.5 Further reading", " 5 Introduction to R Markdown 5.1 Motivation Say you get some tree data from a colleague, and spend a month writing lots of R code to analyse the data. Your code also produces beautiful figures and tables, that you then manually copy into a Word document. You have also written lots of text, and include specific calculated numbers in the text, such as the simple The average tree height was 10.1 m. Then your colleague sheepishly tells you that someone found an error in some of the data, and so you need to redo everything. Your heart sinks with the prospect of re-running all your code and making sure you manually copy the correct new figures into your document. Oh, and you need to redo the tables and check all the numbers in your text. The alternative modern approach is to use R Markdown. The idea is that you can generate a dynamic report. You write code that contains a mixture of your R code and your write up. You can use this for short analyses, scientific manuscripts, or even a complete thesis. This introduction, aimed at biologists, will get you started with the basics. You will then be in a good position to learn more details from the RStudio introduction and the online Definitive Guide to R Markdown. A key concept is that everything is written as code. You do not have to manually point and click anything, or copy and paste figures between directories. So once you understand how something works or have figured out some formatting that you like, you can just copy that code and use it elsewhere. Example application A recent 328-page document we wrote in R Markdown is A reproducible data synopsis for over 100 species of British Columbia groundfish. For each of 113 species, we produced two pages of figures: For each species, the layout of the figures is identical (even to show no data when none are available). Producing each figure and manually inserting them into a Word document would be extremely tedious. Instead, the production of the document is automated using R Markdown. Furthermore, the work is transparent and traceable. Because the code produces the figures (they are not pasted in from somewhere), we can trace back from the R Markdown code to see the R code that: pulled the data from databases fit models generated plots. In particular, we intend to periodically update the document as new data become available. While still a lot of work, it is less daunting knowing that the code is already available. On a practical level, the report has allowed anyone to see the data available, and has consequently increased data transparency between Fisheries and Oceans Canada, the fishing industry, non-governmental organizations, and the public. This is admittedly a very advanced example with a ton of code (several new R packages) and work behind it, but the idea is to show you what is possible. 5.2 Basic idea In the above tree example, if using Word, for example, you would have a sentence that says The average tree height was 10.1 m The 10.1 is hard-wired into your text, and you typed it on from the value 10.1 that your R code calculated (in a variable you calculated, lets say you called it avge_height). In your R Markdown document you have your R code and your text write up. You would equivalently have: Instead of 10.1 you refer directly to the variable avge_height that you have already calculated. When you render your R Markdown document (convert it from code into .pdf, .html or other formats), it automatically fills in the avge_height value as 10.1. The means that the next bit of code (until the next backtick) should be evaluated using R, and the result inserted. This is the basic idea. Then, when your colleague mentions the error (or, say, provides you with extra data) you can just re-run your code and the 10.1 will be automatically updated in your document. This concept extends to your tables and figures  they can all be automatically updated. 5.3 Simple example Here we will generate some data, show some of it in a table, plot it, and show the results of fitting a simple linear regression. Read through this and then you will download and run the code in the exercise. Generate data First well need some libraries: Now generate some data: [1] 50 So we are showing our R code here (we can choose to hide it if we like), and it has been executed, yielding the printed output of the value of n (because of the final line of the code). We can also embed results from R within sentences, for example: We have a sample size of 50. This is done (as mentioned above) by the code: Note that you need the space straight after the r. We can also automatically say that the maximum value of the data is 506.5564788, or round it to a whole number: the maximum value of the data is 507. These were done by: Show some of the data Lets combine the data in a tibble (think of it as a data frame if you dont know what that is): # A tibble: 50 x 2 x y &lt;int&gt; &lt;dbl&gt; 1 1 23.7 2 2 14.4 3 3 33.6 4 4 46.3 5 5 54.0 6 6 58.9 7 7 85.1 8 8 79.1 9 9 110. 10 10 99.4 # ... with 40 more rows (only the first 10 rows get printed here thanks to dplyr). To have a proper table, we can do Table 5.1: The first rows of my data. x y 1 23.70958 2 14.35302 3 33.63128 4 46.32863 5 54.04268 6 58.93875 7 85.11522 8 79.05341 9 110.18424 10 99.37286 (If youre running the code separately the exact style may look different because of settings we have, but pretty much everything is tweakable with the kable and kableExtra packages). Plot then fit a regression data Now to plot the data: To fit and then print the summary regression output from R: Call: lm(formula = y ~ x) Residuals: Min 1Q Median 3Q Max -27.403 -4.366 -1.193 8.319 21.072 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.7071 3.2719 1.133 0.263 x 9.8406 0.1117 88.124 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 11.39 on 48 degrees of freedom Multiple R-squared: 0.9939, Adjusted R-squared: 0.9937 F-statistic: 7766 on 1 and 48 DF, p-value: &lt; 2.2e-16 And for a report we can produce a simple table (including a caption) of output and the regression fit: Table 5.2: Linear regression fit. Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.707115 3.2719100 1.133012 0.2628368 x 9.840634 0.1116686 88.123530 0.0000000 And create a plot: Now, lets go back and change the data The big feature of dynamically generating reports is when you go back and change or update the input data. For example, changing the data in the above example and then re-running it to redo the report. The best way to demonstrate this is for you to do it in the following Exercise. 5.3.1 Exercise In R do library(rmarkdown) to make sure you have the rmarkdown package. If not then install it. Download this file onto your computer and put it where you want to work on this exercise. The file is an R Markdown (.Rmd) file that you can run by either clicking the knitr button in RStudio or doing rmarkdown::render(\"rmark-exercise.Rmd\") in R. Check that this has produced (rendered) an .html file document that looks similar to what you see above (dont worry if the styling is not identical, but the important content should be). Note that the .Rmd file is not repeating all the explanations that we gave above. Carefully read through the .Rmd file and compare it with what you see in the resulting .html. There are some comments in there, denoted by &lt;!-- comment --&gt; to help you, but you should be able to get the idea of how commands in the .Rmd file translate into output in the .html file. Copy the resulting rmark-exercise.html to rmark-exercise-orig.html, change n to 30 in rmark-exercise.Rmd, and re-render it. Compare the two .html files. You have done the same analyses but on different data. You changed one value, n, and consequently all the resulting calculations changed, and these are all updated in your new .html document! You have performed exactly the same analysis on your new data, including creating tables and figures. No copy-and-pasteing of tables, or manually keeping track of which figure corresponds to which analysis. That last bit is the crux of R Markdown. Once you understand that then you should incorporate it into your workflow. You can easily run identical analyses on two data sets, or do 10 runs of a model with different parameters, and easily compare the output. You can even get a bit clever with your writing by including an R ifelse statement to somewhat automate the text. Here is the results of some code (that will be included in the file): So the maximum value of \\(y\\) is 507, which is greater than the special value of 400. The greater than or less than part is given by But you have to be careful and think about all possibilities  what if \\(y=399.9\\)? 5.4 Output format To keep it simple, in the Exercise we have set the output to be .html. For scientific documents, .pdf are preferable. For this you will need LaTeX installed. For decades, LaTeX has been the standard typesetting system for writing mathematical papers. In 2002, Friedrich Leisch created Sweave, to weave together R code for calculations and LaTeX code for writing up results into a .pdf file. (It was called Sweave because S was the precursor language to R, plus I expect Sweave sounded better than Rweave). Sweave motivated knitr (to knit together R code with text write ups) by Yihui Xie, which also allows html and other output formats. R Markdown was then created to allow simpler use of knitr, without learning lots of LaTeX commands. To create .pdf output, and use LaTeX for writing equations and customising your output, if you dont have LaTeX installed it then see the simple instructions another book by Yhiui Xie et al., the R Markdown cookbook. 5.5 Further reading As we said, the idea of this module was to give you a simple introduction to using R Markdown. The Definitive Guide to R Markdown is highly recommended (and often where you end up when Googling for how to do something), but you do not need to understand all of it (I know that Pandoc is doing stuff behind the scenes, but have managed to not need much more than that). Chapter 2 goes through the basics, including many things that we glossed over above, such as the code between the --- at the top of rmark-exercise.Rmd. For me to explain it properly, I would have to refer to the guidebook anyway. In practice, once you have something working in the style you like, you can just copy those settings for each new project. It can be handy to keep a readme file of containing commands/options/tips that you use a lot. "],["introduction-to-multivariate-analysis.html", "6 Introduction to multivariate analysis 6.1 Multivariate resemblance 6.2 Cluster Analysis 6.3 Ordination", " 6 Introduction to multivariate analysis In this module well be disccusing multivariate quantitative methods. Analyses such as linear regression, where we relate a response, y, to a predictor variable, x, are univariate techniques. If we have multiple responses, \\(y_1...y_n\\), and multiple predictors, \\(x_1...x_n\\), we need multivariate approaches. For example, we may wish to understand how both precipitation and soil type are related to plant community composition. In this question, we may be tracking the abundance of over a dozen different species and many different sites with different types of soil, precipitation and other environmental factors. You can easily see that this is not a situation that ordinary univariate approaches are designed to handle! There are many types of multivariate analysis, and in this module and the next, we will only describe some of the most common ones. We can think of these different types of analysis as laying at different ends of a spectrum of treating the data as discrete vs continuous, and relying on identfying a reponse variable a priori versus letting the data tell us about explanatory features, i.e., latent variables (Fig. 6.1. Figure 6.1: Types of multivariate analysis 6.1 Multivariate resemblance The starting point for a lot of the classic multivariate methods is to find metrics that describe how similar two individuals, samples, sites or species might be. A natural way to quantify similarity is to list those characters that are shared. For example, what genetic or morphological features are the same or different between two species? A resemblance measure quantifies similarity by adding up in some way the similarities and differences between two things. We can express the shared characters of objects as either: similarity (S), which quantifies the degree of resemblance or dissimilarity (D) which quantifies the degree of difference. 6.1.1 Binary Similarity metrics The simplest similarity metric just tallys the number of shared features. This is called a binary similarity metric, since we are just indicating a yes or no for each characteristic of the two things we wish to compare (Table 6.1). Table 6.1: List of shared attributes for two things Attribute Object 1 Object 2 Similarity Attribute 1 1 0 x Attribute 2 0 1 x Attribute 3 0 0 &lt;U+2713&gt; Attribute 4 1 1 &lt;U+2713&gt; Attribute 5 1 1 &lt;U+2713&gt; Attribute 6 0 0 &lt;U+2713&gt; Attribute 7 0 1 x Attribute 8 0 0 &lt;U+2713&gt; Attribute 9 1 1 &lt;U+2713&gt; Attribute 10 1 0 x We could also use a shared lack of features as an indicator of similarity. The simple matching coefficient uses both shared features, and shared absent features to quantify similarity as \\(S_m=\\frac{a+d}{a+b+c+d}\\), where a refers to the number of characteristics that object 1 possesses and b is the number that object 2 possesses and so on (see Table 6.2). Table 6.2: Summary of shared and absent atributes Object 1 Object 2 Present Absent Object 1 Present a b Object 2 Absent c d We can further categorize similarity metrics as symmetric, where we regard both shared presence and shared absence as evidence of similarity, the simple matching coefficient, \\(S_m\\) would be an example of this, or asymmetric, where we regard only shared presence as evidence of similarity (that is, we ignore shared absences).Asymmetric measures are most useful in analyzing ecological community data, since it is unlikely to be informative that two temperature zone communities lack tropical data, or that aquatic environments lack terrestrial species. The Jaccard index is an asymmetric binary similarity coefficient calculated as \\(S_J=\\frac{a}{a+b+c}\\), while the quite similar Sørenson index is given as \\(S_S=\\frac{2a}{2a+b+c}\\), and so gives greater weight to shared similarities. Both metrics range from 0 to 1, where a value of 1 indicates complete similarity. Lets try an example. In the 70s, Watson (1974) compared the zooplankton species present in Lake Erie and Lake Ontario. We can use this information to compare how similar the communities in the two lakes were at this time. We can see that they shared a lot of species (Table 6.3)! Table 6.3: Species presence and absence in lake Erie and lake Ontario (data from from Watson 1974) species erie ontario 1 1 1 2 1 1 3 1 1 4 1 1 5 1 1 6 1 1 7 1 1 8 1 1 9 1 1 10 1 1 11 1 1 12 1 1 13 1 1 14 1 1 15 1 1 16 1 1 17 1 1 18 1 1 19 1 0 20 0 1 21 0 0 22 0 0 23 0 0 24 0 0 We can calculate the similarity metrics quite easily using the table() function, where 1 indicates presence and 0 indicates absence. I have stored the information from Table 6.3 in the the dataframe lksp. Im just going grab the presences and absences, since I dont need the species names for my calculation. tlake=table(lksp[,c(&quot;erie&quot;,&quot;ontario&quot;)]) tlake ontario erie 1 0 1 18 1 0 1 4 a=tlake[1,1] b=tlake[1,2] c=tlake[2,1] d=tlake[2,2] S_j=a/(a+b+c) S_j [1] 0.9 S_s=2*a/(2*a+b+c) S_s [1] 0.9473684 When a disimilarity or similarity metric has a finite range, we can simply convert from one to the other. For example, for similarities that range from 1 (identical) to 0 (completely different), dissimilarity would simply be 1-similarity. 6.1.2 Quantitative similarity &amp; dissimilarity metrics While binary similarity metrics are easy to understand, there are a few problems. These metrics work best when we have a small number of characteristics and we have sampled very well (e.g., the zooplankton in Lake Erie and Ontario). However, these metrics are biased against maximum similarity values when we have lots of charactersitics (or species) and poor sampling. In addition, we sometimes have more information than just a yes or no which we could use to further characterize similarity. Quantiative similarity and dissimilarity metrics make use of this information. Some examples of quantitative similarity metrics are: Percentage similarity (Renkonen index), Morisitas index of similarity (not dispersion) and Horns index. Quantitive dissimilarity metrics are perhaps more commonly used. In this case, we often talk about the distance between two things. Distances are of two types, either dissimilarity, converted from analogous similarity indices, or specific distance measures, such as Euclidean distance, which doesnt have a counterpart in any similarity index. There are many, many such metrics, and obviously, you should choose the most accurate and meaningful distance measure for a given application. Legendre &amp; Legendre (2012) offer a key on how to select an appropriate measure for given data and problem (check their Tables 7.4-7.6). If you uncertain, then choose several distance measures and compare the results. Euclidean Distance Perhaps the mostly commonly used, and easiest to understand, dissimilarity, or distance, measure is Euclidian distance. This metric is zero for identical sampling units and has no fixed upper bound. Euclidean distance in multivariate space is derived from our understanding of distance in a Cartesian plane. If we had two species abundances measured in two different samples, we could then plot the abundance of species 1 and species 2 for each sample on a 2D plane, and draw a line between them. This would be our Euclidean distance: the shortest path between the two points (Fig. 6.2). Figure 6.2: Euclidean Distance We know that to calculate this distance we would just use the Pythagorean theorem as \\(c=\\sqrt{a^2+b^2}\\). To generalize to n species we can say \\(D^E_{jk}=\\sqrt{\\sum^n_{i=1}(X_{ij}-X_{ik})^2}\\), where Euclidean distance between samples j and k, \\(D^E_{jk}\\), is calculated by summing over the distance in abundance of each of n species in the two samples. Lets try an example. Given the species abundances in Table 6.4, we can calculate the squared difference in abundance for each species, and sum that quantity. Table 6.4: Species abundance and distance calculations for two samples sample j sample k \\((X_j-X_k)^2\\) Species 1 19 35 256 Species 2 35 10 625 Species 3 0 0 0 Species 4 35 5 900 Species 5 10 50 1600 Species 6 0 0 0 Species 7 0 3 9 Species 8 0 0 0 Species 9 30 10 400 Species 10 2 0 4 TOTAL 131 113 3794 Then all we need to do is to take the square root of the sum to obtain the Euclidean distance. Did you get the correct answer of 61.6? Of course, R makes this much easier, I can calculate Euclidan distance using the dist() function, after creating a matrix of the two columns of species abundance data from my original eu dataframe. dist(rbind(eu$j[1:10], eu$k[1:10]), method = &quot;euclidean&quot;) 1 2 61.59545 There are many other quantitative dissimilarity metrics. For example, Bray Curtis dissimilarity is frequently used by ecologists to quantify differences between samples based on abundance or count data. This measure is usually applied to raw abundance data, but can be applied to relative abundances. It is calculated as: \\(BC_{ij}=1-\\frac{C_{ij}}{S_{i}+S_{j}}\\), where \\(C_{ij}\\) is the sum over the smallest values for only those species in common between both sites, \\(S_{i}\\) and \\(S_{j}\\) are the sum of abundances at the two sites. This metric is directly related to the Sørenson binary similarity metric, and ranges from 0 to 1, with 0 indicating complete similarity. This is not at distance metric, and so, is not appropriate for some types of analysis. 6.1.3 Comparing more than two communities/samples/sites/genes/species What about the situation where we want to compare more than two communtiies, species, samples or genes? We can simply generate a dissimilarity or similarity matrix, where each pairwise comparison is given. In the species composition matrix below (Table 6.5), sample A and B do not share any species, while sample A and C share all species but differ in abundances (e.g. species 3 = 1 in sample A and 8 in sample C). The calculation of Euclidean distance using the dist() function produces a lower triangular matrix with the pairwise comparisons (Ive included the distance with the sample itself on the diagonal). The Euclidan distance values suggest that A and B are the most similar! Euclidean distance puts more weight on differences in species abundances than on difference in species presences. As a result, two samples not sharing any species could appear more similar (with lower Euclidean distance) than two samples which share species that largely differ in their abundances. Table 6.5: Species abundance versus species presence and Euclidean distance sample A sample B sample C species 1 0 1 0 species 2 1 0 4 species 3 1 0 8 dist(t(meu[2:4]), method=&quot;euclidean&quot;, diag=TRUE) A B C A 0.000000 B 1.732051 0.000000 C 7.615773 9.000000 0.000000 There are other disadvantages as well, and in general, there is simply no perfect metric. For example, you may dislike the fact that Euclidean distance also has no upper bound, and so it becomes difficult to understand how similar two things are (i.e., the metric can only be understood in a relative way when comparing many things, Sample A is more similar to sample B than sample C, for example). You could use a Bray-Curtis dissimilarity metric, which is quite easy to interpret, but this metric will also confound differences in species presences and differences in species counts (Greenacre 2017). The best policy is to be aware of the advantages and disadvantages of the metrics you choose, and interpret your analysis in light of this information. 6.1.4 R functions There are a number of functions in R that can be used to calculate similarity and dissimilarity metrics. Since we are usually not just comparing two objects, sites or samples, these functions can help make your calculations much quicker when you are comparing many units. dist() offers a number of distance measures (e.g. euclidean,canberra and manhattan). The result is the distance matrix which gives the dissimilarity of each pair of objects, sites or samples. the matrix is an object of the class dist in R. vegdist() (library vegan). The default distance used in this function is Bray-Curtis distance, which is considered more suitable for ecological data. dsvdis() (library labdsv) Offers some other indices than vegdist (e.g. ruzicka (or Rika), a quantitative analogue of Jaccard, and roberts. For full comparison of dist, vegdist and dsvdis,see http://ecology.msu.montana.edu/labdsv/R/labs/lab8/lab8.html. dist.ldc() (library adespatial) Includes 21 dissimilarity indices described in Legendre &amp; De Cáceres (2013), twelve of which are not readily available in other packages. Note that Bray-Curtis dissimilarity is called percentage difference (method = percentdiff). designdist() (library vegan) Allows one to design virtually any distance measure using the formula for their calculation. daisy() (library cluster) Offers euclidean, manhattan and gower distance. distance() (library ecodist) Contains seven distance measures, but the function is more for demonstration (for larger matrices, the calculation takes rather long). 6.2 Cluster Analysis When we have a large number of things to compare, an examination of a matrix of similarlity or dissimilatiry metrics can be tedious or even impossible to do. One way to visualize the similarity among units is to use some form of cluster analysis. Clustering is the grouping of data objects into discrete similarity categories according to a defined similarity or dissimilarity measure. We can contrast clustering, which assumes that units (e.g., sites, communities, species or genes) can be grouped into discrete categories based on similarity, with ordination, which treats the similarity between units as a continuous gradient (well discuss ordination in section 6.3). We can use clustering to do things like discern whether there are one or two or three different communities in three or four or five sampling units. It is used in many fields, such as machine learning, data mining, pattern recognition, image analysis, genomics, systems biology, etc. Machine learning typically regards data clustering as a form of unsupervised learning, or from our figure above (Fig 6.1), as a technique that uses latent variables because we are not guided by a priori ideas of which variables or samples belong in which clusters. 6.2.1 Hierarchical clustering: groups are nested within other groups. Perhaps the most familiar type of clustering is hierarchical. There are two kinds: divisive and agglomerative. In the divisive method, the entire set of units is divided into smaller and smaller groups. The agglomerative method starts with small groups of few units, and groups them into larger and larger clusters, until the entire data set is sampled (Pielou, 1984). Of course, once you have more than two units, you need some way to assess similarlity between the clusters. There are a couple of different methods here. Single linkage assigns the similairty between clusters to the most similar units in each cluster. Complete linkage uses the similarity between the most dissmilar units in each cluster, while average linkage averages over all the units in each cluster (Fig. 6.3). Figure 6.3: Different methods of determining similarity between clusters Single Linkage Cluster Analysis Single linkage cluster analysis is one of the easiest to explain. It is hierarchical, agglomerative technique. We start by creating a matrix of similarity (or dissimilarity) indices between the units we want to compare. Then we find the most similar pair of samples, and that will form the 1st cluster. Next, we find either: (a) the second most similar pair of samples or (b) highest similarity between a cluster and a sample, or (c) most similar pair of clusters, whichever is greatest. We then continue this process until until there is one big cluster. Remember that in single linkage, similarity between two clusters = similarity between the two nearest members of the clusters. Or if we are comparing a sample to a cluster, the similarity is defined as the similarity between sample and the nearest member of the cluster. cls=data.frame(a=c(5,6,34,1,12),b=c(10,5,2,3,4), c=c(10,59,32,3,40), d=c(2,63,10,29,45), e=c(44,35,40,12,20)) clsd=dist(t(cls), method=&quot;euclidean&quot;) round(clsd,0) a b c d b 33 c 60 71 d 76 76 36 e 51 62 48 66 Figure 6.4: Example of using a dissimilarity matrix to construct a single-linkage cluster diagra 6.2.2 R functions Agglomerative approach (bottom-up) hclust() calculates hierarchical cluster analysis and has its own plot function. agnes() (library cluster) Contains six agglomerative algorithms, some not included in hclust. Divisive approach (top-down) diana() 6.2.3 How many clusters? The hiearchical methods just keep going until all objects are included (agglomerative methods), or are each in their own group (divisive methods). However, neither endpoint is very useful. How do we select the number of groups? There are metrics and techniques to make this decision more objective, however, in this introduction, well just mention that, for hierarchical methods, you can determine the number of groups a given degree of similarity, or set the number of groups and find the degree of similarity that results in that number of groups. Lets try. Well use the cutree() function that works on cluster diagrams produced by the hclust() function (Fig. @ref{fig:hclustfig)). If we set our dissimilarity threshold at 40, we find that there are three groups: a&amp;b, c&amp;d, and e in its own group. Figure 6.5: Cluster diagram produced by the function hclust with cut-off line at euclidean distance=40 for group membership a b c d e 1 1 2 2 3 6.2.4 Other clustering methods There are other means of clustering data of course. Partitional clustering is the division of data objects into non-overlapping subsets, such that each data object is in exactly one subset 6.2.4.1 K-means clustering In one version of this, k-means clustering, each cluster is associated with a centroid (center point), and each data object is assigned to the cluster with the closest centroid. In this method, the number of clusters, K, must be specified in advance. Our method is: Choose the number of K clusters Select K points as the initial centroids Calculate the distance of all items to the K centroids Assign items to closest centroid Recompute the centroid of each cluster Repeat from (3) until clusters assignments are stable K-means has problems when clusters are of differing sizes and densities, or are non-globular shapes. It is also very sensitive to outliers. 6.2.4.2 Fuzzy C-Means Clustering In contrast to strict (or hard) clustering approaches, fuzzy (soft) clustering methods allow multiple cluster memberships of the clustered items. This is commonly achieved by assigning to each item a weight of belonging to each cluster. Thus, items at the edge of a cluster, may be in a cluster to a lesser degree than items at the center of a cluster. Typically, each item has as many coefficients (weights) as there are clusters that sum up for each item to one. 6.2.5 Exercise: Cluster analysis of isotope data Our first step is to download and import the dataset Dataset_S1.csv from Perkins et al. 2014 (see url below). This data contains 15N and 13C signatures for species from different food webs. Unfortunately, this data is saved in an .xlsx file. To read data into R one of the easiest options is to use the read.csv() function with the argument on a .csv file. These Comma Separated Files are one of your best options for reproducible research. They are human readable and easily handled by almost every type of software. In contrast Microsoft Excel uses a propriatory file format, is not fully backwards compatible, and although widely used, is not human readable. As a result, we need special tools to access this file outside of Microsoft software products Well download the data set using download.file(), and read it using the R library openxlsx (see example below).Once you have successfully read your data file into R, take a look at it! Type iso (or whatever you named your data object) to see if the data file was read in properly. Some datasets will be too large for this approach to be useful (the data will scroll right off the page). In that case, there are a number of commands to look at a portion of the dataset. You could use a command like names(iso). One of the best things to do is plot the imported data. Of course, this is not always possible with very large datasets, but this set should work. Use the plot() function plotting 15N vs 13C to take a quick look. library(openxlsx) urlj=&quot;https://doi.org/10.1371/journal.pone.0093281.s001&quot; download.file(urlj, &quot;p.xlsx&quot;, mode=&quot;wb&quot;) iso=read.xlsx(&quot;p.xlsx&quot;) plot(iso$N~iso$C, col=as.numeric(as.factor(iso$Food.Chain)),xlim=c(-35, 0), pch=as.numeric(as.factor(iso$Species)), xlab=&quot;13C&quot;, ylab=&quot;15N&quot;) legend(&quot;topright&quot;, legend=unique(as.factor(iso$Food.Chain)),pch=1, col=as.numeric(unique(as.factor(iso$Food.Chain))), bty=&quot;n&quot;, title=&quot;Food chain&quot;) legend(&quot;bottomright&quot;,legend=as.character(unique(as.factor(iso$Species))), pch=as.numeric(unique(as.factor(iso$Species))), bty=&quot;n&quot;) Figure 6.6: Isotope data from Perkins et al (2014) We are going to use this data set to see if a cluster analysis on 15N and 13C can identify the foodweb. That is we are going to see if the latent variables identified by our clustering method match up to what we think we know about the data. Our first step is to create a dissimilarity matrix, but even before this, we must select that part of the data that we wish to use, just the 15N and 13C data, not the other components of the dataframe. In addition, our analysis will be affected by the missing data. So lets get remove those rows with missing data right now using the complete.cases() function. The function returns a value of TRUE for every row in a dataframe that no missing values in any column. So niso=iso[complete.cases(mydata),], will be a new data frame with only complete row entries. The function dist() will generate a matrix of the pairwise Euclidean distances between pairs of observations. Now that you have a dissimilarity matrix, you complete a cluster analysis. The function hclust() will produce a data frame that can be sent to the plot() function plotted to visualize the recommended clustering. The method used to complete the analysis is indicated below the graph. Please adjust the arguments of the function to complete a single linkage analysis (look at help(hclust)) to determine the method to do this). str(iso) &#39;data.frame&#39;: 165 obs. of 7 variables: $ Replicate : num 1 2 3 4 5 6 7 8 9 10 ... $ Food.Chain : chr &quot;Wheat&quot; &quot;Wheat&quot; &quot;Wheat&quot; &quot;Wheat&quot; ... $ Species : chr &quot;Plant&quot; &quot;Plant&quot; &quot;Plant&quot; &quot;Plant&quot; ... $ Tissue : chr &quot;Leaf&quot; &quot;Leaf&quot; &quot;Leaf&quot; &quot;Leaf&quot; ... $ Lipid.Extracted: chr &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... $ C : num -30.1 -31.7 -30.1 -30.9 -31 ... $ N : num -3.47 -2.68 3.42 1.27 6.2 ... diso&lt;-dist((iso[,c(&quot;C&quot;,&quot;N&quot;)]), method=&quot;euclidean&quot;) p=hclust(diso,method=&quot;single&quot;) plot(p, cex=0.5,main=&quot;&quot;) Figure 6.7: Cluster figure of isotope data from Perkins et al. 2014) When you graph your cluster using plot(), you notice that there are many individual measurements, but there are only a few large groups. Does it look like there is an outlier? If so, you may want to remove this point from the data set, and then rerun the analysis. The row numbers are used as labels by default, so this is easy to do (niso=niso[-5,]). Remember to remove the point from the dataframe that has all the food chain info in it, otherwise you will have problems plotting later. When you examine the data set, you noted that there are 4 Food.chain designations. We will use the cutree() function to cut our cluster tree to get the desired number of groups (4), and then save the group numbers to a new column in our original dataframe. For example, iso$clust&lt;- cutree(p,4).We can then plot the data using colours and symbols to see how well our clustering works niso=iso[complete.cases(iso),] niso=niso[-5,] net=niso[,c(&quot;C&quot;,&quot;N&quot;)] diso&lt;-dist((net), method=&quot;euclidean&quot;) p=hclust(diso, method=&quot;single&quot;) plot(p,labels=FALSE, main=&quot;&quot;) Figure 6.8: Single linkage clustering on Perkins et al (2014) data with outlier removed niso$clust&lt;-cutree(p,k=4) plot(iso$N~iso$C, col=as.numeric(as.factor(niso$clust)),xlim=c(-35, 0), pch=as.numeric(as.factor(niso$Species)), xlab=&quot;13C&quot;, ylab=&quot;15N&quot;) legend(&quot;topright&quot;, legend=unique(as.factor(niso$clust)),pch=1, col=as.numeric(unique(as.factor(niso$clust))), bty=&quot;n&quot;, title=&quot;cluster&quot;) legend(&quot;bottomright&quot;,legend=as.character(unique(as.factor(niso$Species))), pch=as.numeric(unique(as.factor(niso$Species))), bty=&quot;n&quot;,title=&quot;Species&quot;) Figure 6.9: Data from Perkins et al (2014) data with grouping from single linkage clustering superimposed It doesnt look like our cluster algorithm is matching up with our Food.chain data categories very well. Wheat- and Nettle-based distinguished, which makes sense when you consider that both of these plants use a C3 photosynthesis system. If you are not happy with the success of this clustering algorithm you could try other variants(.g., complete linkage) and a different number of groups. Lets try a non-hierarchical cluster analysis on the same data to see if it works better. The kmeans() function requires that we select the required number of clusters ahead of time (we want 4, so kclust=kmeans(niso[,c(C, N)], 4)), we can then save the assigned clusters to our dataframe and plot in a similar way Figure 6.10: K means clustering on Perkins et al (2014) data It looks like kmeans has the same problem with distinguishing C3 plant-based foodwebs. But we still get three groups that roughly map onto our information about the data. 6.3 Ordination While cluster analysis lets us visualize multivariate data by grouping objects into dscrete categories, ordination uses continuous axes to help us accomplish the same task. Physicists grumble if space exceeds four dimensions, while biologists typically grapple with dozens of dimensions (species and/or samples). In effect, we order this multivariate data in order to produce a low dimensional picture (i.e., a graph in 1-3 dimensions). Just like cluster analysis, we will use similarity metrics to accomplish this. Also like cluster anlaysis, simple ordination is not a statistical test: it is a method of visualizing data. Essentially, we find axes in the data that explain a lot of variation, and rotate so we can use that axis as one of our dimensions of visual representation(Fig. 6.12). Another way to think about it, is that we are going to summarize the raw data, which has many variables, p, by a smaller set of synthetic variables, k (Fig. 6.11). If the ordination is informative, it reduces a large number of original correlated variables to a small number of new uncorrelated variables. But it really is a bit of a balancing act between clarity of representation, ease of understanding, and oversimiplication. We will lose information in this data reduction, and if that information is important, then we can make the multivariate data harder to understand! Also note that if the original variables are not correlated, then we wont gain anything with ordinaton. Figure 6.11: Ordination as data reduction. We summarize data with many variables (p) by a smaller set of derived or synthetic variables (k) Figure 6.12: Synthetic axis rotation in ordination There are lots of different ways to perform an ordination, but most methods are based on extracting the eigenvalues of a similarity matrix. The four most commonly used methods are: Principle Component Analysis (PCA), which is the main eigenvector-based method, Correspondence Analysis (CA) which is used used on frequency data, Principle Coordinate Analysis (PCoA) which works on dissimilarity matrices, and Non Metric Multidimensional Scaling (NMDS) which is not an eigenvector method, instead it represents objects along a predetermined number of axes. Table 6.6: Domains of Application of Ordination Methods (adpated from Legendre &amp; Legendre 2012) Method Distance Variables Principal component analysis (PCA) Euclidean Quantitative data Correspondence analysis (CA) X^2 Non-negative, quantitiative or binary data; species frequencies or presence/absence data Principal coordinate analysis (PCoA), metric (multidimensional) scaling, classical scaling Any Quantitative, semiquantitative, qualitative, or mixed Nonmetric multidimensional scaling (nMDS) Any Quantitative, semiquantitative, qualitative, or mixed Legendre &amp; Legendre (2012) provide a nice summary of when you should use each method Table 6.6 6.3.1 Principal Components Analysis (PCA) Principal Components Analysis is probably the most widely-used and well-known of the standard multivariate methods. It was invented by Pearson (1901) and Hotelling (1933), and first applied in ecology by Goodall (1954) under the name factor analysis (NB principal factor analysis is also a synonym of PCA). Like most ordination methods, PCA takes a data matrix of n objects by p variables, which may be correlated, and summarizes it by uncorrelated axes (principal components or principal axes) that are linear combinations of the original p variables. The first k components display as much as possible of the variation among objects. PCA uses Euclidean distance calculated from the p variables as the measure of dissimilarity among the n objects, and derives the best possible k-dimensional representation of the Euclidean distances among objects, where \\(k &lt; p\\) . We can think about this spatially. Objects are represented as a cloud of n points in a multidimensional space with an axis for each of the p variables. So the centroid of the points is defined by the mean of each variable, and the variance of each variable is the average squared deviation of its n values around the mean of that variable (i.e., \\(V_i= \\frac{1}{n-1}\\sum_{m=1}^{n}{(X_{im}-\\bar{X_i)}^2}\\)). The degree to which the variables are linearly correlated is given by their covariances \\(C_{ij}=\\frac{1}{n-1}\\sum_{m=1}^n{(X_{im}-\\bar{X_i})(X_{jm}-\\bar{X_j})}\\). The objective of PCA is to rigidly rotate the axes of the p-dimenional space to new positions (principal axes) that have the following properties: they are ordered such that principal axis 1 (or the principal component has the highest variance, axis 2 has the next highest variance etc, and the covariance among each pair of principal axes is zero (the principal axes are uncorrelated) (Fig. 6.13). Figure 6.13: Selecting the synthetic axes in ordination So our steps are to compute the variance-covariance matrix of the data, calculate the eigenvalues of this matrix and then calculate the associated eigenvectors. Then, the jth eigenvalue is the variance of the jth principle component and the sum of all the eigenvalues is the total variance explained. The proportion of variance explained by each component is the eigenvalue for the component divided by the total variance explained, while the loadings are the eigenvectors. Dimensionality reduction is the same as first rotating the data with the eigenvalues to be aligned with the principle components, then using only the components with the greatest eigenvalues. 6.3.2 Exercise: PCA on the iris data Lets try an example. Were going to use a sample dataset in R and the base R version of PCA to start exploring this data analysis technique. Get the iris dataset into memory by typing data(iris). Take a look at this dataset using the head(), str() or summary() functions. For a multivariate data set, you would like to take a look at the pairwise correlations. Remember that PCA cant help us if the variables are not correlated. Lets use the pairs() function to do this data(&quot;iris&quot;) str(iris); summary(iris[1:4]) &#39;data.frame&#39;: 150 obs. of 5 variables: $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Sepal.Length Sepal.Width Petal.Length Petal.Width Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 Median :5.800 Median :3.000 Median :4.350 Median :1.300 Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 pairs(iris[1:4],main=&quot;Iris Data&quot;, pch=19, col=as.numeric(iris$Species)+1) Figure 6.14: correlation atrix for the iris data The colours let us see the data for each species, the plots are the pairwise plotting of each pair of the 4 variables (Fig. 6.14). Do you see any correlations? If there seem to be some correlations we might use PCA to visualize the 4 dimensional variable space. Lets rush right in and use the prcomp() function to run a PCA on the numerical data in the iris dataframe. Save the output from the function to a new variable name so you can look at it when you type that name. The str() function will show you what the output object includes. If you use the summary() function, R will tell you what proportion of the total variance is explained by each axis. There is a problem though, lets examine the variance in the raw data. Use the apply() function to quickly calculate the variance in each of the numeric columns of the data as apply(iris[,1:3], 1, var). What do you see? Are the variances of each the columns comparable? pca &lt;- prcomp(iris[,1:4]) summary(pca) Importance of components: PC1 PC2 PC3 PC4 Standard deviation 2.0563 0.49262 0.2797 0.15439 Proportion of Variance 0.9246 0.05307 0.0171 0.00521 Cumulative Proportion 0.9246 0.97769 0.9948 1.00000 apply(iris[,1:4], 2, var) Sepal.Length Sepal.Width Petal.Length Petal.Width 0.6856935 0.1899794 3.1162779 0.5810063 Using covariances among variables only makes sense if they are measured in the same units, and even then, variables with high variances will dominate the principal components. These problems are generally avoided by standardizing each variable to unit variance and zero mean as \\(X_{im}^{&#39;}=\\frac{x_{im}-\\bar{X_i}}{sd_i}\\) where sd is the standard deviation of variable i. After standardizaton, the variance of each variable is 1 and the covariances of the standardized variables are correlations. If you look at the help menu, the notes for the use of prcomp() STRONGLY recommend standardizing the data. To do this there is a built in option. We just need to set scale=TRUE. Lets try again with data standardization. Save your new PCA output to a different name. Well compare to the unstandardized data in a moment. Take a look at the summary. p &lt;- prcomp(iris[,1:4], scale=TRUE) summary(p) Importance of components: PC1 PC2 PC3 PC4 Standard deviation 1.7084 0.9560 0.38309 0.14393 Proportion of Variance 0.7296 0.2285 0.03669 0.00518 Cumulative Proportion 0.7296 0.9581 0.99482 1.00000 Now we need to determine how many axes to use to interpret our analysis. For 4 variables it is easy enough to just look that the amount of variance. For larger numbers of variables a plot can be useful. The screeplot() function will output the variance explained by each of the principle component axes, and you can make a decision based on that (e.g., screeplot(pca2, type=lines)). An ideal curve should be steep, then bend at an elbow  this is your cutting-off point  and after that flattens out. To deal with a not-so-ideal scree plot curve you can apply the Kaiser rule: pick PCs with eigenvalues of at least 1. Or you can select using the proportion of variance where the PCs should be able to describe at least 80% of the variance. It looks like synthetic axes 1 &amp; 2 explain most of the variation. So lets plot those out. A PCA plot displays our samples in terms of their position (or scores) on the new axes. We can add information about how much variation each axis explains, and colour our points to match species identity. In this 2D representation of 4 dimensional space, it looks like species versicolor and viriginica group together (Fig. 6.15). Figure 6.15: PCA plot for the iris data We can also plot information about influence the various characterisitics are having on each of the axes. The eigenvectors used for the rotation give us this information. So lets just print that out Table 6.7: Eigenvectors (or loadings) for each variable and synthetic axis PC1 PC2 PC3 PC4 Sepal.Length 0.52 -0.38 0.72 0.26 Sepal.Width -0.27 -0.92 -0.24 -0.12 Petal.Length 0.58 -0.02 -0.14 -0.80 Petal.Width 0.56 -0.07 -0.63 0.52 We can see that a lot of information is coming from the petal variables for PC1, but less from the sepal variables (Table ??). We can plot this out to show how strongly each variable affects each principle component (or synthetic axis). We can see that petal width and length are aligned along the PC1 axis, while PC2 explains more variation in sepal width (Fig @ref(fig: loadplot)). To interpret the variable plot remember that positively correlated variables are grouped close together (e.g., petal length and width). Variables with about a 90 angle are probably not correlated, while negatively correlated variables are positioned on opposite sides of the plot origin (~180 angle; opposed quadrants). However, the direction of the axes is arbitrary! The distance between variables and the origin measures the contribution of the variables to the ordination. A shorter arrow indicates its less importance for the ordination. Variables that are away from the origin are well represented. Avoid the mistake of interpreting the relationships among variables based on the proximities of the apices (tips) of the vector arrows instead of their angles in biplots. Another way to portray this imformation is to create a biplot which, in addition to the coordinates of our samples on the synthetic axes PC1 and PC2, also provides information about how the variables align along the synthetic axes. (Fig (fig:pactwo). I should note that I have used an arbitrary scaling to display the variable loadings on each axis. Some of the R packages will use a specific scaling that will emphasize particular parts of the plot, either preserving the Euclidean distances between samples or the correlations/covariances between variables (e.g., vegan). Figure 6.16: PCA plot for the iris data Principal components analysis assumes the relationships among variables are linear, so that the cloud of points in p-dimensional space has linear dimensions that can be effectively summarized by the principal axes. If the structure in the data is nonlinear (i.e., the cloud of points twists and curves its way through p-dimensional space), the principal axes will not be an efficient and informative summary of the data. For example, in community ecology, we might use PCA to summarize variables whose relationships are approximately linear or at least monotonic (e..g, soil properties might be used to extract a few components that summarize main dimensions of soil variation). However, in general PCA is generally not useful for ordinating community data because relationships among species are highly nonlinear. This nonlinearity can leaad to characterisitc artifacts, where, for example, community trends along environmental gradients appear as horseshoes in PCA ordinations because low species density at opposite extremes of an environmental gradiant appear relatively close together. 6.3.3 Principle Coordinates Analysis (PCoA) The PCoA method may be used with all types of distance descriptors, and so might be able to avoid sum problems of PCA. Although, a PCoA computed on a Euclidean distance matrix gives the same results as a PCA conducted on the original data 6.3.3.1 R functions for PCoA cmdscale()(stats) -base R, no package needed smacofSym() (library smacof) wcmdscale()(vegan) pco()(ecodist) pco()(labdsv) pcoa()(ape) dudi.pco()(ade4) 6.3.4 Nonmetric Multidimensional Scaling (NMDS) Like PCoA, the method of nonmetric multidimensional scaling (nMDS), produces ordinations of objects from any resemblance matrix. However, nMDS compresses the distances in a non-linear way and its algorithm is computer-intensive, requiring more computing time than PCoA. PCoA is faster for large distance matrices. This ordinaton method does not to preserve the exact dissimilarities among objects in an ordination plot, instead it represent as well as possible the ordering relationships among objects in a small and specified number of axes. Like PCoA, nMDS can produce ordinations of objects from any dissimilarity matrix.The method can also cope with missing values, as long as there are enough measures left to position each object with respect to a few others. nMDS is not an eigenvalue technique, and it does not maximise the variability associated with individual axes of the ordination. In this computational method the steps are: Specify the desired number m of axes (dimensions) of the ordination. Construct an initial configuration of the objects in the m dimensions, to be used as a starting point of an iterative adjustment process. (tricky: end result may depend on this. A PCoA ordination may be a good start. Otherwise, try many independent runs with random initial configurations) Try to position the objects in the requested number of dimensions in such a way as to minimize how far the dissimilarities in the reduced-space configuration are from being monotonic to the original dissimilarities in the association matrix The adjustment goes on until the stress value cannot be lowered, or until it reaches a predetermined low value (tolerated lack-of-fit). Most NMDS programs rotate the final solution using PCA, for easier interpretation. NMDS often achieves a less deformed representation of the dissimilarity relationships among objects than a PCoA in the same number of dimensions. We can use a Shephard plot to get information about the distortion of representation. A Shepard diagram compares how far apart your data points are before and after you transform them (ie: goodness-of-fit) as a scatter plot. On the x-axis, we plot the original distances. On the y-axis, we plot the distances output by a dimension reduction algorithm. A really accurate dimension reduction will produce a straight line. However since information is almost always lost during data reduction, at least on real, high-dimension data, so Shepard diagrams rarely look this straight. Lets try this for the iris data. We can evaluate the quality of the NMDS solution by checking the Shephard plot as : stressplot(nMDS, main = Shepard plot). In addition to the original dissimilarity and ordination distance, the plot displays two correlation-like statistics on the goodness of fit. The nonmetric fit is given by \\(R^2\\), while he linear fit is the squared correlation between fitted values and ordination distances (Fig. ??). There is some deformation here, but in general the representation is not so bad. Run 0 stress 0.03775523 Run 1 stress 0.05537364 Run 2 stress 0.04367521 Run 3 stress 0.04804007 Run 4 stress 0.03775525 ... Procrustes: rmse 1.133709e-05 max resid 3.616212e-05 ... Similar to previous best Run 5 stress 0.03775522 ... New best solution ... Procrustes: rmse 7.621844e-06 max resid 7.564749e-05 ... Similar to previous best Run 6 stress 0.05918357 Run 7 stress 0.06031975 Run 8 stress 0.03775524 ... Procrustes: rmse 6.321677e-06 max resid 2.684336e-05 ... Similar to previous best Run 9 stress 0.04355784 Run 10 stress 0.04367522 Run 11 stress 0.05059727 Run 12 stress 0.03775521 ... New best solution ... Procrustes: rmse 3.167733e-06 max resid 1.37392e-05 ... Similar to previous best Run 13 stress 0.05361269 Run 14 stress 0.05317214 Run 15 stress 0.04804014 Run 16 stress 0.03775526 ... Procrustes: rmse 1.217405e-05 max resid 5.425564e-05 ... Similar to previous best Run 17 stress 0.03775524 ... Procrustes: rmse 8.064894e-06 max resid 3.494195e-05 ... Similar to previous best Run 18 stress 0.04804008 Run 19 stress 0.03775522 ... Procrustes: rmse 3.889243e-05 max resid 0.0001678761 ... Similar to previous best Run 20 stress 0.06144867 *** Solution reached nMDS often achieves a less deformed representation of the dissimilarity relationships among objects than a PCoA in the same number of dimensions. But nMDS is a computer-intensive iterative technique exposed to the risk of suboptimum solutions. In comparison, PCoA finds the optimal solution by eigenvalue decomposition. 6.3.4.1 R functions for NMDS metaMDS() (vegan) isoMDS( ) (MASS) 6.3.5 Exercise: Ordination We are going to use the vegan package, and some built in data with it to run the nMDS. varespec is a data frame of observations of 44 species at 24 sites. Well calculate both an NMDS and a PCoA using the (cmdscale() function) on the bray-curtis distance matrix of these data. In each case, we will specify that we want 2 dimensions as our output. library(vegan) data(varespec) disimvar=vegdist(varespec, method = &quot;bray&quot;) nMDS &lt;- metaMDS(varespec, distance=&quot;bray&quot;, k=2) Square root transformation Wisconsin double standardization Run 0 stress 0.1843196 Run 1 stress 0.2212141 Run 2 stress 0.2209227 Run 3 stress 0.2085515 Run 4 stress 0.1825658 ... New best solution ... Procrustes: rmse 0.0416743 max resid 0.1520615 Run 5 stress 0.195049 Run 6 stress 0.2398057 Run 7 stress 0.1955836 Run 8 stress 0.2114447 Run 9 stress 0.1967393 Run 10 stress 0.2419376 Run 11 stress 0.1825658 ... New best solution ... Procrustes: rmse 6.745298e-05 max resid 0.0002312025 ... Similar to previous best Run 12 stress 0.1845801 Run 13 stress 0.2141967 Run 14 stress 0.2166093 Run 15 stress 0.1843196 Run 16 stress 0.1948413 Run 17 stress 0.2048307 Run 18 stress 0.2234893 Run 19 stress 0.1985582 Run 20 stress 0.1825658 ... Procrustes: rmse 1.255124e-05 max resid 3.837387e-05 ... Similar to previous best *** Solution reached PCoA &lt;- cmdscale(disimvar, k = 2, eig = T, add = T ) If we look at the object PCoA we see eigenvalues, one for each of the 20 sites, and the new 2-D coordinates for each site. We can plot the results as plot(PCoA$points). In fact, lets plot the PCoA and the NMDS side by side to see if they differ, using the par(mfrow()) functions. In this case, our species are the variables and our sites/samples are the objects of our attention. Theres a lot of species, so we wont draw the arrows, well just show their position on the biplot. "],["classification.html", "7 Classification 7.1 Multivariate logistic regression 7.2 Exercise: Logistic regression as a classifier", " 7 Classification Classification is the task of assigning data objects, such as sites, species or images to predetermined classes. Determining what class of data object you have is a question that usually turns on multiple predictors. For example, to classify leaf images to different species predictors such as size, shape and colour may be used. you have sattelite data, you may need to classify the differnt pixels of the image as agricultural, forest, or urban. So for classifcation tasks our response variabel, y, is qualitative or categorical (e.g. gender). There are many methods that can be employed for this task ranging from logistic regression to random forest techqniques. While some of these methods are classic multivariate methods, others, like random forest classifiers, are machine learning tasks. Machine learning is an application of artificial intelligence. The computer algorithm finds a solution to the classifcation problem without being explicitly programmed to do so. 7.1 Multivariate logistic regression On of the simplest classification methods is to use logistic regression. Lets take a simple, but common example. A non-native species has been introduced to a region, and we would like to know what percentage of the region would be suitable habitat, in order to get an idea of risks of impact. Well start with a univariate version of the problem. Lets assume we think that average temperature controls habitat suitability, and we have presencce absence data for the species accross a range of different sites. Could we use simple regression to answer the question of whether a given area is suitable habitat? Figure 7.1: Species presence absence and rainfall As we can see in Fig. @ref{fig:log}, the linear regression does not make a lot of sense for a response variable that is restricted to the values of 0 and 1. The regression line \\(\\beta_0+\\beta_1x\\) can take on any value between negative and positive infinity, but we dont know how to interpret values greater than 1 or less than zero. The regression line almost always predicts wrong value for y in classification problems. Instead of trying to predict y, we can try to predict p(y = 1), i.e., the probability that a species is in an area. We need a function that gives outputs between 0 and 1: logistic regression is one solution. In this model, probability of y for a given value of x, p(x) is given as: \\[\\frac{e^{\\beta_0+\\beta_1x}}{1+e^{\\beta_0+\\beta_1x}}\\]. Rearranging, we have: \\[\\frac{p(x)}{1-p(x)}=e^{\\beta_0+\\beta_1x},\\] where the lefthand side is called the log-odds or logit. We can see that the logistic regression model has a logit that is linear in x: \\[\\log{\\frac{p(x)}{1-p(x)}}=e^{\\beta_0+\\beta_1x}\\]. The logistic function will always produce an S-shaped curve, so regardless of the value of x, we will obtain a sensible prediction. Lets apply this model to our non native species data. 1 2 3 4 5 6 0.04112748 0.04458053 0.04533242 0.05951435 0.20909427 0.67290516 7.2 Exercise: Logistic regression as a classifier Well start by installing the MASS package, and then the the Pima.tr data in the MASS package, since there is a binary response. Install the MASS package, load the library and load the data. Type help(Pima.tr) or ?Pima.tr to get a description of these data. Youll notice that the type variable is our classifier and determines whether the patient has diabetes or not. Next construct a logistic regression, to use as a classifier. Examine your output, to determine if the regression is significant &#39;data.frame&#39;: 200 obs. of 8 variables: $ npreg: int 5 7 5 0 0 5 3 1 3 2 ... $ glu : int 86 195 77 165 107 97 83 193 142 128 ... $ bp : int 68 70 82 76 60 76 58 50 80 78 ... $ skin : int 28 33 41 43 25 27 31 16 15 37 ... $ bmi : num 30.2 25.1 35.8 47.9 26.4 35.6 34.3 25.9 32.4 43.3 ... $ ped : num 0.364 0.163 0.156 0.259 0.133 ... $ age : int 24 55 35 26 23 52 25 24 63 31 ... $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 1 1 1 2 ... Call: glm(formula = type ~ ., family = binomial, data = Pima.tr) Deviance Residuals: Min 1Q Median 3Q Max -1.9830 -0.6773 -0.3681 0.6439 2.3154 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -9.773062 1.770386 -5.520 3.38e-08 *** npreg 0.103183 0.064694 1.595 0.11073 glu 0.032117 0.006787 4.732 2.22e-06 *** bp -0.004768 0.018541 -0.257 0.79707 skin -0.001917 0.022500 -0.085 0.93211 bmi 0.083624 0.042827 1.953 0.05087 . ped 1.820410 0.665514 2.735 0.00623 ** age 0.041184 0.022091 1.864 0.06228 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 256.41 on 199 degrees of freedom Residual deviance: 178.39 on 192 degrees of freedom AIC: 194.39 Number of Fisher Scoring iterations: 5 The . in the formula argument means that we use all the remaining variables in data as covariates. Note: we used the glm() function to perform logistic regression by passing in the family=binomial argument. But if we use glm() to fit a model without passing in the family argument, then it performs linear regression, just like the lm() function. Next, well use some testing data, to test our classifier. The Prima.te has already been created for you in the MASS package. Well use the predict function to get a classification of the data. Note that the output is a probability. To complete the classification we need to make a decision about where to divide up the classes. Lets just use a 50% probability, and then construct a confusion matrix to determine how well our predictor did. Predicted Actual No Yes No 200 23 Yes 43 66 [1] 0.8012048 Notice there there have been some missclassifications at 50%, and that the accuracy is only about 80%. See if another decision boundary (e.g., 75%) does any better. There is a nice package in R, caret, that is a great wrapper for machine learning tasks. We can use it to generate both our confusion matrix, and other statistical info about our classifier Confusion Matrix and Statistics Reference Prediction No Yes No 200 23 Yes 43 66 Accuracy : 0.8012 95% CI : (0.7542, 0.8428) No Information Rate : 0.7319 P-Value [Acc &gt; NIR] : 0.002095 Kappa : 0.5271 Mcnemar&#39;s Test P-Value : 0.019349 Sensitivity : 0.8230 Specificity : 0.7416 Pos Pred Value : 0.8969 Neg Pred Value : 0.6055 Prevalence : 0.7319 Detection Rate : 0.6024 Detection Prevalence : 0.6717 Balanced Accuracy : 0.7823 &#39;Positive&#39; Class : No Perhaps the most informative of these stats is the No Information Rate which tests whether our classifier does better than random assignment. Also the balanced accuracy stat, gives an accuracy value that weights both majority and minority classes evenly, if you have unbalanced membership. Finally, you may want to visualize your classfier results. A simple way is to code the colours and symbols. Lets use the ggplot() system since it has a nice method for dealing with two different categorical objects (although in other respects is has a cryptic interface!). ##LDA 7.2.1 Classification trees Classification trees also output the predicted class for a given sample. Lets try this on the iris data. Well first split the data into a training and testing set, using a built in function in the caret library. And then run our tree algorithm in the rpart package. n= 105 node), split, n, loss, yval, (yprob) * denotes terminal node 1) root 105 68 versicolor (0.3238095 0.3523810 0.3238095) 2) Petal.Length&lt; 2.35 34 0 setosa (1.0000000 0.0000000 0.0000000) * 3) Petal.Length&gt;=2.35 71 34 versicolor (0.0000000 0.5211268 0.4788732) 6) Petal.Length&lt; 4.95 40 3 versicolor (0.0000000 0.9250000 0.0750000) * 7) Petal.Length&gt;=4.95 31 0 virginica (0.0000000 0.0000000 1.0000000) * Examine the output, and then try to plot it Finally, using this tree classifer, we can make predictions for our testing data, and get a confusion matrix Confusion Matrix and Statistics Reference Prediction setosa versicolor virginica setosa 16 0 0 versicolor 0 11 3 virginica 0 2 13 Overall Statistics Accuracy : 0.8889 95% CI : (0.7595, 0.9629) No Information Rate : 0.3556 P-Value [Acc &gt; NIR] : 1.581e-13 Kappa : 0.833 Mcnemar&#39;s Test P-Value : NA Statistics by Class: Class: setosa Class: versicolor Class: virginica Sensitivity 1.0000 0.8462 0.8125 Specificity 1.0000 0.9062 0.9310 Pos Pred Value 1.0000 0.7857 0.8667 Neg Pred Value 1.0000 0.9355 0.9000 Prevalence 0.3556 0.2889 0.3556 Detection Rate 0.3556 0.2444 0.2889 Detection Prevalence 0.3556 0.3111 0.3333 Balanced Accuracy 1.0000 0.8762 0.8718 7.2.2 Random Forests Lets try an ensemble decision tree on the iris data. We use the randomForest package on the training data. Note that we do not have to split our data into training and testing now, random Forest is already doing this sort of thing for us. Call: randomForest(formula = Species ~ ., data = iris) Type of random forest: classification Number of trees: 500 No. of variables tried at each split: 2 OOB estimate of error rate: 4.67% Confusion matrix: setosa versicolor virginica class.error setosa 50 0 0 0.00 versicolor 0 47 3 0.06 virginica 0 4 46 0.08 Our classification is pretty good with a misclassification rate of only 4%. We might like to looks at what the model is doing but unlike a single CART, random forests do not produce a single visual, since of course the predictions are averaged across many hundreds or thousands of trees. The plot shows how the Out Of Bag error rate (proportion of misclassifications) for each of the three species changes with the size of the forest (the number of trees). Obviously with few trees the error rate is high, but as more trees are added you can see the error rate decrease and eventually flatten out. When building random forests, there are three tuning parameters of interest: node size, number of trees, and number of predictors sampled at each split. Careful tuning of these parameters can prevent extended computations with little gain in error reduction. For example, in the above plot, we could easily reduce the number of trees down to 300 and experience relatively little loss in predictive ability: Call: randomForest(formula = Species ~ ., data = iris, ntree = 300) Type of random forest: classification Number of trees: 300 No. of variables tried at each split: 2 OOB estimate of error rate: 4.67% Confusion matrix: setosa versicolor virginica class.error setosa 50 0 0 0.00 versicolor 0 47 3 0.06 virginica 0 4 46 0.08 So we see an increase in our error rate, but not much. Despite not yielding a single visualizable tree, one of the major advantages of random forests is that they can provide a measure of relative importance. By ranking predictors based on how much they influence the response, RFs may be a useful tool for whittling down predictors before trying another framework, such as CART of linear models. Importance can be obtained using the importance function, and plotted using the varImpPlotfunction: The table reports the mean decrease in the Gini Index, which if you recall, is a measure of impurity for categorical data. For each tree, each predictor in the OOB sample is randomly permuted (aka, shuffled around) and passed to the tree to obtain the error rate (again, Gini index for categorical data, MSE for continuous). The error rate from the unpermuted OOB is then subtracted from the error rate on the permuted OOB data, and averaged across all trees. When this value is large, it implies that a variable had a strong relationship with the response (aka, the model got much worse at predicting the data when that variable was permuted). The plot communicates the same data as in the table, with points farther along the x-axis deemed more important. As we already knew, Petal.Length and Petal.Width are the two most important variables. One other useful aspect of random forests is getting a sense of the partial effect of each predictor given the other predictors in the model. (This has analogues to partial correlation plots in linear models.) This is done by holding each value of the predictor of interest constant (while allowing all other predictors to vary at their original values), passing it through the RF, and predicting the responses. The average of the predicted responses are plotted against each value of the predictor of interest (the ones that were held constant) to see how the effect of that predictor changes based on its value. This exercise can be repeated for all other predictors to gain a sense of their partial effects. The function to calculate partial effects is partialPlot. Lets look at the effect of Petal.Length: The y-axis is a bit tricky to interpret. Since we are dealing with classification trees, its on the logit scale, so its the probability of success. In this case, the partial plot has defaulted to the first class, which represents I. setosa. This plot says that there is a high chance of successfully predicting this species from Petal.Length when Petal.Length is less than around 2.5 cm, after which point the chance of successful prediction drops off precipitously. This is actually quite reassuring as this is the first split identified way back in the very first CART (where the split was &lt; 2.45 cm). Missing data Its worth noting that the default behavior of randomForest is to refuse to fit trees with missing predictors. You can, however, specify a few alternative arguments: the first is na.action = na.omit, which removes the ros with missing values outright. Another option is to use na.action = na.roughfix, which replaces missing values with the median (for continuous variables) or the most frequent level (for categorical variables). Missing responses are harder: you can either remove that row, or use the function rfImpute to impute values. The imputed values are the average of the non-missing observations, weighted by their proximity to non-missing observations (based on how often they fall in terminal nodes with those observations). rfImpute tends to give optimistic estimates of the OOB error. "],["optimization.html", "8 Optimization 8.1 Introduction 8.2 Fundamentals of Optimization 8.3 Regression 8.4 Iterative Optimization Algorithms 8.5 Calibration of Dynamic Models 8.6 Uncertainty Analysis and Bayesian Calibration 8.7 References", " 8 Optimization 8.1 Introduction To improve is to make better; to optimize is to make best. Optimization is the act of identifying the extreme (cheapest, tallest, fastest) over a collection of possibilities. You may recall studying optimization in introductory calculus where you may have solved simple design problems over the possible sizes of fences, or boxes, or something similar. Optimization over design space (also called decision space) is a critical feature of many engineering tasks, and has a role in most areas of applied science, including biology. Examples include optimal manipulation of biological systems (e.g. optimal harvesting or optimal drug dosing) or optimal design of biological systems (e.g. robust synthetic genetic circuits). A complementary task is optimal experimental design, which aims to identify the best experiment from a collection of possibilities. Model calibration, to be discussed further below, provides another example; here we seek the best fit version of a model from a collection of possible options. Beyond its use in design, optimization is also used to investigate natural systems (i.e. in `pure science), in cases where nature presents us with an optimal version of a given phenomenon. For example, the principle of entropy maximization (equivalently energy minimization) justifies a wide variety of phenomena, from the shapes of soap bubbles to the configuration of proteins. Darwinian evolution provides another mechanism for optimization. Assuming evolution has arrived at optimal designs, we can apply optimization to understand a variety of biological phenomena, from metabolic flux distribution, to brain wiring, to optimal foraging strategies. This module introduces optimization methods in R. Applications and illustrations are be drawn from a range of biological domains. The simple optimization tasks addressed by introductory calculus can be accomplished with paper-and-pencil calculations. As shown below, while the fundamental principles introduced by those exercises carry forward, most optimization tasks of interest in biology demand more extensive computational resources. 8.2 Fundamentals of Optimization Figure 1 illustrates some basics terminology associated with optimization. The graph of a function \\(f\\) of a single variable \\(x\\) is shown, defined over a domain \\([a,b]\\). In the context of optimization, we can think of each \\(x\\)-value in the interval \\([a,b]\\) as one possible scenario (e.g. fence length, harvesting rate, etc.). The function \\(f\\) maps those choices to some objective (e.g. total area, long-term annual yield) that we wish to optimize (either maximize or minimize). The global extrema (i.e. maximum and minimum) represent the results we wish to achieve. Figure 1: Extreme Values More generally, we define local extrema (maxima and minima) as cases that appear to be optimal if we restrict our attention only to nearby possibilities (i.e. \\(x\\)-values). There is an extensive theory of optimization methods; unfortunately most of these are dedicated to identifying local optima. These approaches cannot directly identify global optimum  instead they identify candidate global optima, which then can be compared to identify the best result. (A convenient special case occurs for problems where every local optimum is a global optimum; these are called convex optimization problems. Unfortunately, they occur only rarely in addressing biological phenomena.) 8.2.1 Fermats Theorem To illustrate these fundamentals, consider the following two academic examples that rely on basic calculus, specifically on Fermats Theorem, which states that local extrema occur at points where the tangent line to a functions graph is horizontal, i.e. at points where the derivative (i.e. the slope of the tangent) is zero (Figure 2). Figure 2: Fermats Theorem: local extrema occur at points where the tangent line is horizontal. Some local extrema are also global extrema. Example 1. Identify the value of \\(x\\) for which the function \\(f(x) = x^2+3x-2\\) is minimized. Solution: Taking the derivative, we find \\(f&#39;(x) = 2x+3\\). To find the point(s) where the derivative is zero, we solve: \\(f&#39;(x) = 0 \\Leftrightarrow 2x+3 = 0\\Leftrightarrow x = -\\frac{3}{2} = -1.5\\). As shown in Figure 4, the single local minimum is the global miminum in this case, so \\(x=-1.5\\) is the desired solution. Figure 3: Graph of \\(f(x) = x^2+3x-2\\) (Example 1) Example 2. Identify the value of \\(x\\) at which the function \\(f(x) = 3x^4-4x^3-54x^2+108x\\) is minimized. Solution: Taking the derivative, we find \\(f&#39;(x) = 12x^3-12x^2-108x+108 = 12(x-3)(x-1)(x+3)\\). To find the point(s) where the derivative is zero, we solve: \\(f&#39;(x) = 0 \\Leftrightarrow x = 3, 1, -3\\). As shown in Figure 4, two of these points are local minima. One (\\(x=-3\\)) is where the global minimum occurs. Figure 4: Graph of \\(f(x) = 3x^4-4x^3-54x^2+108x\\) (Example 2) 8.3 Regression 8.3.1 Linear Regression As mentioned above, finding the best fit from a family of models is a common optimization task in science. The simplest example of this task is linear regression: determining a line of best fit through a given set of points. To illustrate, consider the dataset of \\((x,y)\\) pairs shown in Figure XYZ, which we can label as \\((x_1, y_1)\\), \\((x_2, y_2)\\), ldots, \\((x_N, y_N)\\). Several lines are displayed. The optimization task here is to identify the best line: the one that best captures the trend in the data. To specify this task mathematically, we need to decide on a measure of quality of fit. We start by recognizing that the line will (typically) fail to pass through most of the points in the dataset. Thus, at each point \\(x_i\\) we can define an error, which is the difference between the observed \\(y\\)-value and the \\(y\\)-value on the line. If we call the line \\(y=mx+b\\), then the error at \\(x_i\\) can be defined as \\(e_i = y_i-(mx_i+b)\\). We then need to combine these errors into a single measure of teh quality of fit. This is typically done by squaring the errors and adding them together. (Squaring ensures that both under- and over-estimations contribute equally.) We define the sum of squared errors (SSE) as : \\((y_1-(mx_1+b))^2 + (y_2-(mx_2+b))^2 + \\cdots +(y_N-(mx_N+b))^2 = e_1^2+e_2^2+ \\cdots e_N^2\\). We can now pose the model-fitting task as an optimization problem. For each line (that is, each assignment of numerical values to \\(m\\) and \\(b\\)), we associate a corresponding SSE. We seek the values of \\(m\\) and \\(b\\) for which the SSE is a global minimum. Example 3. Consider a simplified version of linear regression, in which we know that our model (line) should pass through the origin (0,0). That is, instead of lines \\(y=mx+b\\), we will consider only lines of the form \\(y=mx\\). (We thus have a single parameter to identify: the slope \\(m\\).) To keep the algebra as simple as possible well take a tiny dataset consisting of just two points: \\((x_1, y_1) = (2,3)\\) and \\((x_2, y_2)= (5.4)\\), as indicated in Figure 5. The line passes through points \\((2,2m)\\) and \\((5, 5m)\\). Figure 5: Example 1: finding a line of best fit through the origin In this simple case, the sum of squared errors is: \\[\\begin{equation*} \\mbox{SSE} = e_1^2+e_2^2 = (3-2m)^2+(4-5m)^2 \\end{equation*}\\] To apply Fermats theorem we take the derivative and identify any values of \\(m\\) for which it is zero: \\[\\begin{eqnarray*} \\frac{d}{dm} \\mbox{SSE} &amp;=&amp; 2(3-2m)(-2)+2(4-5m)(-5) \\\\ &amp;=&amp; -4(3-2m) - 10(4-5m) = -12 +8m+-40+50m = -52+58m. \\end{eqnarray*}\\] The only local extremum (and hence the only candidate for global minimum) is then \\(m=52/58 = 26/29\\). The analysis in Example 3 can be extended to determine the general solution of the linear regression task [Fairway, 2002]. The solution formula is as follows: Linear regression formula The best fit line \\(y=mx+b\\) to the dataset \\((x_1, y_1)\\), \\((x_2, y_2)\\), , \\((x_n, y_n)\\) is given by \\[\\begin{eqnarray*} m&amp;=&amp; \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2} \\\\ b&amp;=&amp; \\bar{y}-m\\bar{x}, \\\\ \\end{eqnarray*}\\] where \\(\\bar{x}\\) and \\(\\bar{y}\\) are the averages \\[\\begin{eqnarray*} \\bar{x}=\\frac{1}{n} \\sum_{i=1}^n x_i \\ \\ \\ \\bar{y}=\\frac{1}{n} \\sum_{i=1}^n y_i \\end{eqnarray*}\\] This formula is somewhat unwieldy, but is straightforward to implement. In R, the command lm implements this formula, as the following example illustrates. Well work with a dataset called cars that is included in R. Its shown below (via the plot() command). #plot(cars) The lm() command applies the formula described above to arrive at the line of best fit (i.e. the minimizer of the sum of squared errors. We use teh command in the form lm(dist~speed), referring to the labels for the data (as shown above) to generate the parameters (\\(b\\) and \\(m\\)) that specify the line \\(y=mx+b\\). (The attach command ensures that the data is accessible.) #attach(cars) #lm(dist~speed) The intercept is -17.579 and the slope is 3.932. We can plot the data together with the line of best fit with the abline command: #attach(cars) #plot(cars) #abline(-17.579, 3.932) 8.3.2 Nonlinear Regression Nonlinear regression is the task of fitting a nonlinear model (e.g. a curve) to a dataset. The setup for this task is identical to linear regression. We begin by selecting a parameterized family of models (i.e. curves), and aim to identify the model (i.e. curve) that minimizes the sum of squared errors when compared to the data. The family of models can be chosen based on an understanding of the mechanism that relates the input and output data. For example, if we are investigating the rate law for a single-substrate enzyme-catalysed reaction, we might choose the family of curves specified by Michaelis-Menten kinetics: \\[\\begin{eqnarray*} V = \\frac{V_{\\mbox{max}} S}{K_M+S} \\end{eqnarray*}\\] Our goal would then be to identify values for the parameters \\(V_{\\mbox{max}}\\) and \\(K_M\\) that minimize the sum of squared errors when comparing with observed data. Regression via linearizing transformation In several important cases, the nonlinear regression task can be transformed into a linear regression task. In the case of Michaelis-Menten kinetics, several linearizing transformations have been proposed (e.g. EadieHofstee and LineweaverBurk [Cho and Lim, 2018]). Another example commonly encountered in Biology is fitting exponential curves (e.g. population growth or drug clearance). In those cases, a logarithm modifies the data so that a linear trend is captured: e.g. \\(y = e^{rt}\\), after applying a logarithm becomes \\(y^{trans} = \\ln(y) = \\ln (e^{rt})= rt\\). Linear regression on transformed data \\((t_i, y^{trans}_i)\\) then provides an estimate of the value of \\(r\\). Unfortunately, linearizing transformations are only available in a handful of special cases. Moreover, they often introduce biases that can make interpretation of the resulting model difficult. In general, the nonlinear regression task must be addressed directly. An example of the general procedure in R follows. We begin by defining a dataset against which we will fit a Michaelis-Menten curve. MM_data_old &lt;- structure(list(S = c(4.32, 2.16, 1.08, 0.54, 0.27, 0.135, 3.6, 1.8, 0.9, 0.45, 0.225, 0.1125, 2.88, 1.44, 0.72, 0.36, 0.18, 0.9, 0), v = c(0.004835714, 0.004192308, 0.003553846, 0.002576923, 0.001661538, 0.001064286, 0.004407692, 0.004671429, 0.0039, 0.002857143, 0.00175, 0.001057143, 0.004907143, 0.004521429, 0.00375, 0.002764286, 0.001857143, 0.001121429, 0)), .Names = c(&quot;S&quot;, &quot;V&quot;), class = &quot;data.frame&quot;, row.names = c(NA,-19L)) S &lt;- c(4.32, 2.16, 1.08, 0.54, 0.27, 0.135, 3.6, 1.8, 0.9, 0.45, 0.225, 0.1125, 2.88, 1.44, 0.72, 0.36, 0.18, 0.9, 0) V &lt;- c(0.004835714, 0.004192308, 0.003553846, 0.002576923, 0.001661538, 0.001064286, 0.004407692, 0.004671429, 0.0039, 0.002857143, 0.00175, 0.001057143, 0.004907143, 0.004521429, 0.00375, 0.002764286, 0.001857143, 0.001121429, 0) MM_data=cbind(S,V) plot(MM_data) Like the lm command, the nls function takes as inputs the dataset and the model. In addition, nls requires that the user provides guesses for the values of the parameters to be estimated (in this case \\(V_{\\mbox{max}}\\) and \\(K_M\\)). In this case, we can roughly estimate \\(V_{\\mbox{max}} \\approx 0.005\\) as the maximal \\(V\\)-value that can be achieved, and \\(K_M \\approx 0.2\\) as the \\(S\\)-value at which \\(V\\) reaches half of its maximal value. model.nls &lt;- nls(V ~ Vmax * S/(Km+S),start = list(Km = 0.3, Vmax = 0.005)) summary(model.nls) Formula: V ~ Vmax * S/(Km + S) Parameters: Estimate Std. Error t value Pr(&gt;|t|) Km 0.4772154 0.1362895 3.501 0.00274 ** Vmax 0.0053061 0.0004804 11.046 3.53e-09 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.0006588 on 17 degrees of freedom Number of iterations to convergence: 5 Achieved convergence tolerance: 2.434e-06 This output from nls provides the parameter estimates that characterize the best-fit model (\\(K_M = 0.4772154\\) and \\(V_{\\mbox max} = 0.0053061\\)), as well as additional information about the confidence of this estimate (some of which will be discussed further below). We can now plot the best fit model along with the dataset: params &lt;- summary(model.nls)$coeff[,1] #extracting the parameter estimates plot(MM_data) curve((params[2]*x)/(params[1]+x), from = 0, to = 4, add=TRUE, col=&#39;firebrick&#39;) From the implementation of nls, the reader might have the mistaken impression that nonlinear regression and linear regression are very similar tasks. Although the problem set-up is similar (first chose family of curves, then minimize SSE), the optimization task is very different, and the solution procedures follow very different protocols. (The first hint of this is the need to provide ``guesses to nls.) As we saw above, the solution to the linear regression task can be stated as a general formula. For nonlinear regression, no such formula exists. Worse, as well discuss in the next section there is no procedure (algorithm) that is guaranteed to find the solution! 8.4 Iterative Optimization Algorithms The best techniques for addressing the general nonlinear regression task are iterative global optimization routines. As well discuss below, these algorithms all follow a basic idea: we start with a guess which established initial estimates for the parameter values and then take steps through parameter space to improve the quality of that estimate. In the exercise above, the nls command executed this kind of algorithm, which is why it requires that the user supply an initial guess. 8.4.1 Gradient Descent A simple iterative optimization algorithm is gradient descent, which can be understood intuitively via a thought experiment. Imagine finding your way to a valley bottom in a think fog. The fog obscures your vision so that you can only detect changes in elevation in your immediate vicinity. To make your way to the valley bottom, it would be reasonable to take each step of your journey in the direction of steepest decline. This strategy is guaranteed to lead to a local minimum, but cannot guarantee arrival at the lowest point: the global minimum. That this, the search may lead to a local minimum at which there is no direction of local descent, and so the algorithm gets `stuck. Mathematically, the local change in elevation is determined by evaluating the optimization objective at points nearby the current estimate, and then using those to determine the direction of steepest descent. (technically, this involves a linearization of the function at the current position, or equivalently a calculation of the gradient vector). A step is then taken in the direction, and the process is repeated from this updated estimate. To implement this algorithm, a number of details have to be specified: how long should each step be? How many steps should be taken? Or should there be some other termination condition that will trigger the end of the journey? (Each of these decisions involve a trade-off, typically a trade-off of precision vs. computation time. For instance, taking very small steps will guarantee a smooth path down the steepest route, but might take a very long time to complete the journey. Termination conditions are often specified in terms of the local topography: the algorithm stops when the current estimate is at a sufficiently flat position (no downhill direction detected). To illustrate the gradient descent approach, consider the following algorithm, which incorporates two termination conditions: a maximum number of allowed iterations,and a threshold for shallowness: library(numDeriv) # contains functions for calculating gradient #define function that implement gradient descent. Inputs are the objective function f, the initial parameter estimate x0, the maximum number of iterations to be applied, the size of each step, and a threshold gradient below which the landscape is considered flat (and so iterations are terminated) grad.descent = function(f, x0, step.size=0.05, max.iter=200, stopping.deriv=0.01, ...) { n = length(x0) # record the number of parameters to be estimated (i.e. the dimension of the parameter space) xmat = matrix(0,nrow=n,ncol=max.iter) # initialize a matrix to record the sequence of estimates xmat[,1] = x0 # the first row of matrxi xmat is the initial estimate &#39;guess&#39; for (k in 2:max.iter) { # Calculate the gradient (a vector indicating steepness and direct of greatest ascent) grad.current = grad(f,xmat[,k-1],...) # Check termination condition: has a flat valley bottom been reached? if (all(abs(grad.current) &lt; stopping.deriv)) { k = k-1; break 3 } # Step in the opposite direction of the grad xmat[,k] = xmat[,k-1] - step.size * grad.current } xmat = xmat[,1:k] # Trim any unused columns from xmat return(list(x=xmat[,k], xmat=xmat, k=k)) } ####Example 1 Well begin by demonstrating the performance of this algorithm on a simple function with a single local minimum. In this case, we expect that the minimum (valley bottom) will be reached from any initial guess. Starting from four distinct points, the algorithm follows the paths shown below. The global minimum was reached in every case. ####Example 2 Next, lets consider a function that has multiple local minima. The second plot below is interactive, allowing you to rotate the surface. Again, well apply gradient descent from a set of initial guess positions: Here we see that from some initial guess values, the algorithm successfully reaches the global minimuml from others, it gets stuck at a local minimum. The default optimization algorithm used by nls is the Gauss-Newton method, which is a generalization of Newtons method for solving nonlinear equations (which may be familiar from introductory calculus). This is a refinement of gradient descent in which the local curvature of the function is used to project the position of the bottom of the local valley assuming that the surface is shaped like a parabola. On parabolic surfaces, this achieves descent to the bottom in one iteration. On non-parabolic surfaces (i.e.~most surfaces), the results is a sequence of iterations, but often many fewer than would be required using gradient descent. Both gradient descent and Gauss-Newton method are design to reach the bottom of the valley in which the initial guess lies. If thats not the global minimum, then the algorithm will not be successful. So, how does one select a good initial guess? Unfortunately, theres no general answer to this question. In many cases, one can use previous knowledge of the system under investigation to begin with a solid initial guess. If no such previous knowledge is available, sometimes a `guess is all we have. In those cases, we may have little confidence that the algorithm will arrive at a global minimum. The simplest way to gain some confidence of achieving a global minimum is the multi-start strategy: choose many initial guesses, and run the algorithm from each (as in the example above). This can be computationally expensive, but if the initial guesses are spread widely over the parameters space, one can have hope that the global minimum will be achieved. A number of methods have been developed to complement the multi-start approach. These are known as global optimization routines. They are also known as heuristic methods, because their performance cannot be guaranteed in general: there are no guarantees that theyll find the global minimum, nor are there solid estimates of how many iterations will be required for them to carry out a satisfactory search of the parameter space. We will consider two commonly used heuristic methods: simulated annealing and genetic algorithms. 8.4.1.1 Simulated Annealing Simulated annealing is motivated by the process of annealing: a heat treatment by which the physical properties of metals can be altered. Simulated annealing is an iterative algorithm; the algorithm starts at an initial guess, and then steps through the search space. In contrast with gradient descent, there is a random element to the process followed by simulated annealing. Consequently, the path followed from a particular initial condition wont be repeated if the algorithm is run again from the same point. (Algorithms that incorporate randomness are often referred to as Monte Carlo methods, in reference to the European gambling centre.) At each step, the algorithm begins by identify a candidate next position. (This point could be selected by a variant of gradient descent, or some other method). The value of the objective at this candidate point is then compared with the objective value at the current point. If the objective is lower at the new point (i.e. this step takes the algorithm downhill), then the step is taken and a new iteration begins. If the value of the objective is larger at the candidate (i.e. the step makes things worse), the step can still be taken, but only with a small probability. Both the size of the candidate steps and the probability of accepting wrong (uphill) steps are tied to a cooling schedule: a decreasing `temperature profile: at high temperatures, large steps are considered and wrong steps are taken frequently; as the temperature drops, only smaller steps are considered, and fewer wrong steps are allowed. By analogy, imagine a ping-pong ball resting on a curved landscape. One strategy to move the ball to the lowest valley bottom is to shake the table. Mirroring simulated annealing, we could begin by applying violent shakes (high temperature) which would result in the ball bouncing across much of the landscape. By slowly reducing the severity of the shaking, the ball would settle into a local minimum at a valley bottom. The hope is that if the cooling schedule is well-chosen, the ball would have sampled many valleys, and would end up at the bottom of the lowest valley. Simulated annealing is often combined with a multi-start strategy to further ensure broad sampling of the search space. To illustrate, well apply simulated annealing to the optimization task in Example 2 above, using the same initial guess points. Well use the GenSA library to implement the algorithm (add link https://cran.r-project.org/web/packages/GenSA/GenSA.pdf). Calls to GenSA requires that we specify the objective function, an initial guess, and upper and lower bounds for the search values for each parameter. (Optional input parameters allow the user to modify internal features of the algorithm such as the cooling schedule and stepping protocol.) library(GenSA) out0 &lt;- GenSA(par = x0, lower = c(-2,-2), upper = c(2,2),fn = complicatedFun) out0[c(&quot;value&quot;,&quot;par&quot;)] $value [1] -4.906786 $par [1] 2.0000000 0.6014984 The result of this call is recorded in the variable out0 which indicates the minimal value of the objective achieved (-4.906786) and the parameter values at which this minimum occurs \\((x,y)= (2.0000000, 0.6014984)\\). Next, well call GenSA form each of the initial guesses that were provided to the gradient descent algorithm above. $value [1] -4.906786 $par [1] 2.0000000 0.6014984 $counts [1] 52490 $value [1] -4.906786 $par [1] 2.0000000 0.6014984 $counts [1] 54649 $value [1] -4.906786 $par [1] 2.0000000 0.6014984 $counts [1] 53020 $value [1] -4.906786 $par [1] 2.0000000 0.6014984 $counts [1] 52325 $value [1] -4.906786 $par [1] 2.0000000 0.6014984 $counts [1] 52470 We see that simulated anneaing has avoided getting stuck in the local minima, and has achieved the same optimized value from every initial guess. The plot below provides some insight into how the simulated annealing run proceeded. Iterations (steps) are shown along the horizontal axis. The vertical axis shows values of the objective function. The blue points represents function value at teh current position, while the red shows the minimum achieved so far. The minimum is found rather quickly, but the algorithm continues to jump around (and occasionally get temporarily stuck at local minima) in an attempt to widely explore the search space. ####Example 4 To give the simulated annealing algorithm a more challeneging task, consider the following function, which has many local minima. Here we started from two different starting points and they resulted in the same final position. $value [1] -306.7205 $par [1] -76.14595 191.15515 $counts [1] 51580 $value [1] -306.7205 $par [1] -76.14595 191.15515 $counts [1] 52120 The plot below shows how simulated annealing works for the two initial points respectively. The red line records the current minimum. The blue points represents function value. Although they have different starting position, it can be seen from the plots the optimized values are the same. We next consider a heuristic algorithm in which multiple paths through the search space are followed simultaneously. 8.4.1.2 Genetic Algorithms Genetic algorithms are inspired by Darwinian evolution. The algorithm begins with the specification of a population of initial guesses. At each iteration of the algorithm, this population evolves toward improved estimates of the global minimum. This evolution step involves three substeps: selection, mutation, and cross-over. In the selection step, the population is pruned by removing a fraction that are not sufficiently fit (where fitness corresponds to the value of the objective function being minimized). Then, mutations are introduced into the remaining population by adding small random perturbations to their position in the search space. Finally, a new generation is generated by crossing members of the current population. This can be done in several ways; the simplest is to generate crosses as averages of the numerical values of the two parents. The hope is that, through several generations, this process will lead to a population with high fitness (minimal objective) after a thorough exploration of the search space. Genetic algorithms are a subset of the more general class of evolutionary algorithms all of which involve simultaneous exploration of the search space through multiple paths. To implement a genetic algorithm, well make use of the ga function (https://cran.r-project.org/web/packages/GA/GA.pdf). As with simulated annealing, the call to ga requires that we specify the objective function, lower and upper bounds to define the search space, and. maximizes the objective function. In this case, to get the global minimum, a negative sign needs to be added in front of the function.The larger the maximum iteration is, the closer the value is to the actual global minimum. The seed can also be set to different values, but the final optimized value would be the same. GA | iter = 1 | Mean = 1.086156 | Best = 4.412655 GA | iter = 2 | Mean = 1.292277 | Best = 4.412655 GA | iter = 3 | Mean = 0.8966117 | Best = 4.5356307 GA | iter = 4 | Mean = 2.325734 | Best = 4.535631 GA | iter = 5 | Mean = 3.225968 | Best = 4.535631 GA | iter = 6 | Mean = 3.635158 | Best = 4.578217 GA | iter = 7 | Mean = 3.877847 | Best = 4.578217 GA | iter = 8 | Mean = 3.992020 | Best = 4.608003 GA | iter = 9 | Mean = 3.762169 | Best = 4.608003 GA | iter = 10 | Mean = 3.583764 | Best = 4.608003 GA | iter = 11 | Mean = 4.131960 | Best = 4.608003 GA | iter = 12 | Mean = 4.256510 | Best = 4.608003 GA | iter = 13 | Mean = 3.830520 | Best = 4.608003 GA | iter = 14 | Mean = 3.896934 | Best = 4.608003 GA | iter = 15 | Mean = 3.999068 | Best = 4.608003 GA | iter = 16 | Mean = 3.890321 | Best = 4.608003 GA | iter = 17 | Mean = 3.244979 | Best = 4.608003 GA | iter = 18 | Mean = 3.380700 | Best = 4.608003 GA | iter = 19 | Mean = 4.101646 | Best = 4.608003 GA | iter = 20 | Mean = 4.041709 | Best = 4.608003 GA | iter = 21 | Mean = 3.867888 | Best = 4.608003 GA | iter = 22 | Mean = 3.693844 | Best = 4.608003 GA | iter = 23 | Mean = 3.806633 | Best = 4.608003 GA | iter = 24 | Mean = 3.581247 | Best = 4.608003 GA | iter = 25 | Mean = 3.955851 | Best = 4.608003 GA | iter = 26 | Mean = 4.129783 | Best = 4.608003 GA | iter = 27 | Mean = 3.784814 | Best = 4.608003 GA | iter = 28 | Mean = 4.171595 | Best = 4.608003 GA | iter = 29 | Mean = 3.873681 | Best = 4.608003 GA | iter = 30 | Mean = 3.706429 | Best = 4.608003 GA | iter = 31 | Mean = 3.721338 | Best = 4.608003 GA | iter = 32 | Mean = 4.271014 | Best = 4.608003 GA | iter = 33 | Mean = 3.951294 | Best = 4.608003 GA | iter = 34 | Mean = 3.985901 | Best = 4.608003 GA | iter = 35 | Mean = 4.097654 | Best = 4.608003 GA | iter = 36 | Mean = 3.510574 | Best = 4.608003 GA | iter = 37 | Mean = 4.065270 | Best = 4.608003 GA | iter = 38 | Mean = 4.065450 | Best = 4.608003 GA | iter = 39 | Mean = 4.041116 | Best = 4.608003 GA | iter = 40 | Mean = 3.710881 | Best = 4.608003 GA | iter = 41 | Mean = 4.189354 | Best = 4.608003 GA | iter = 42 | Mean = 4.116101 | Best = 4.608003 GA | iter = 43 | Mean = 4.322074 | Best = 4.608003 GA | iter = 44 | Mean = 3.956914 | Best = 4.608003 GA | iter = 45 | Mean = 3.933653 | Best = 4.608003 GA | iter = 46 | Mean = 3.757792 | Best = 4.608003 GA | iter = 47 | Mean = 4.031257 | Best = 4.608003 GA | iter = 48 | Mean = 4.236144 | Best = 4.608003 GA | iter = 49 | Mean = 4.428410 | Best = 4.608003 GA | iter = 50 | Mean = 4.446344 | Best = 4.608003 GA | iter = 51 | Mean = 4.045708 | Best = 4.608003 GA | iter = 52 | Mean = 4.152684 | Best = 4.608003 GA | iter = 53 | Mean = 3.433529 | Best = 4.608003 GA | iter = 54 | Mean = 3.434301 | Best = 4.608003 GA | iter = 55 | Mean = 4.102205 | Best = 4.608003 GA | iter = 56 | Mean = 4.432986 | Best = 4.608003 GA | iter = 57 | Mean = 3.846399 | Best = 4.608003 GA | iter = 58 | Mean = 4.324348 | Best = 4.608003 GA | iter = 59 | Mean = 4.265420 | Best = 4.608003 GA | iter = 60 | Mean = 4.257668 | Best = 4.608003 GA | iter = 61 | Mean = 3.781037 | Best = 4.608003 GA | iter = 62 | Mean = 3.594471 | Best = 4.608003 GA | iter = 63 | Mean = 3.905878 | Best = 4.608003 GA | iter = 64 | Mean = 3.564578 | Best = 4.608003 GA | iter = 65 | Mean = 3.904829 | Best = 4.608003 GA | iter = 66 | Mean = 3.724331 | Best = 4.608003 GA | iter = 67 | Mean = 3.822168 | Best = 4.608003 GA | iter = 68 | Mean = 3.330559 | Best = 4.608003 GA | iter = 69 | Mean = 3.825713 | Best = 4.608003 GA | iter = 70 | Mean = 3.761733 | Best = 4.608003 GA | iter = 71 | Mean = 3.862769 | Best = 4.608003 GA | iter = 72 | Mean = 4.393930 | Best = 4.608003 GA | iter = 73 | Mean = 4.291426 | Best = 4.608003 GA | iter = 74 | Mean = 4.252299 | Best = 4.608003 GA | iter = 75 | Mean = 3.989491 | Best = 4.608003 GA | iter = 76 | Mean = 4.117121 | Best = 4.608003 GA | iter = 77 | Mean = 4.037263 | Best = 4.608003 GA | iter = 78 | Mean = 3.223005 | Best = 4.608003 GA | iter = 79 | Mean = 3.385697 | Best = 4.608003 GA | iter = 80 | Mean = 3.952331 | Best = 4.786492 GA | iter = 81 | Mean = 4.012027 | Best = 4.786492 GA | iter = 82 | Mean = 3.497586 | Best = 4.786492 GA | iter = 83 | Mean = 3.660046 | Best = 4.786492 GA | iter = 84 | Mean = 3.445934 | Best = 4.786492 GA | iter = 85 | Mean = 3.461604 | Best = 4.786492 GA | iter = 86 | Mean = 3.656681 | Best = 4.786492 GA | iter = 87 | Mean = 3.296952 | Best = 4.786492 GA | iter = 88 | Mean = 3.934535 | Best = 4.786492 GA | iter = 89 | Mean = 3.900382 | Best = 4.786492 GA | iter = 90 | Mean = 3.930024 | Best = 4.786492 GA | iter = 91 | Mean = 3.657472 | Best = 4.796268 GA | iter = 92 | Mean = 3.872905 | Best = 4.796268 GA | iter = 93 | Mean = 4.329420 | Best = 4.796268 GA | iter = 94 | Mean = 4.349302 | Best = 4.796268 GA | iter = 95 | Mean = 4.078848 | Best = 4.796268 GA | iter = 96 | Mean = 3.357519 | Best = 4.796268 GA | iter = 97 | Mean = 3.818968 | Best = 4.796268 GA | iter = 98 | Mean = 4.233977 | Best = 4.796268 GA | iter = 99 | Mean = 3.427798 | Best = 4.796268 GA | iter = 100 | Mean = 4.166846 | Best = 4.796268 GA | iter = 101 | Mean = 4.338536 | Best = 4.796268 GA | iter = 102 | Mean = 3.852287 | Best = 4.796268 GA | iter = 103 | Mean = 4.404707 | Best = 4.796268 GA | iter = 104 | Mean = 4.243925 | Best = 4.796268 GA | iter = 105 | Mean = 3.916935 | Best = 4.796268 GA | iter = 106 | Mean = 4.353963 | Best = 4.796268 GA | iter = 107 | Mean = 4.166997 | Best = 4.796268 GA | iter = 108 | Mean = 4.005683 | Best = 4.796268 GA | iter = 109 | Mean = 4.415919 | Best = 4.796268 GA | iter = 110 | Mean = 3.805575 | Best = 4.796268 GA | iter = 111 | Mean = 4.117105 | Best = 4.796268 GA | iter = 112 | Mean = 4.161827 | Best = 4.796268 GA | iter = 113 | Mean = 4.247574 | Best = 4.796268 GA | iter = 114 | Mean = 4.297307 | Best = 4.796268 GA | iter = 115 | Mean = 4.154330 | Best = 4.796268 GA | iter = 116 | Mean = 3.984342 | Best = 4.796268 GA | iter = 117 | Mean = 3.591940 | Best = 4.796268 GA | iter = 118 | Mean = 4.017399 | Best = 4.796268 GA | iter = 119 | Mean = 4.022540 | Best = 4.796268 GA | iter = 120 | Mean = 4.374086 | Best = 4.796268 GA | iter = 121 | Mean = 3.967936 | Best = 4.796268 GA | iter = 122 | Mean = 3.986365 | Best = 4.796268 GA | iter = 123 | Mean = 3.963697 | Best = 4.796268 GA | iter = 124 | Mean = 4.409716 | Best = 4.796268 GA | iter = 125 | Mean = 3.879635 | Best = 4.796268 GA | iter = 126 | Mean = 4.249180 | Best = 4.796268 GA | iter = 127 | Mean = 3.866522 | Best = 4.796268 GA | iter = 128 | Mean = 4.508335 | Best = 4.796268 GA | iter = 129 | Mean = 4.384221 | Best = 4.796268 GA | iter = 130 | Mean = 4.408718 | Best = 4.796268 GA | iter = 131 | Mean = 4.191588 | Best = 4.796268 GA | iter = 132 | Mean = 3.953055 | Best = 4.796268 GA | iter = 133 | Mean = 3.557071 | Best = 4.796355 GA | iter = 134 | Mean = 4.090990 | Best = 4.796355 GA | iter = 135 | Mean = 4.213783 | Best = 4.796355 GA | iter = 136 | Mean = 4.324607 | Best = 4.796355 GA | iter = 137 | Mean = 4.214865 | Best = 4.796355 GA | iter = 138 | Mean = 4.310746 | Best = 4.796355 GA | iter = 139 | Mean = 3.943443 | Best = 4.796355 GA | iter = 140 | Mean = 4.114281 | Best = 4.807607 GA | iter = 141 | Mean = 4.004561 | Best = 4.807607 GA | iter = 142 | Mean = 4.001507 | Best = 4.807607 GA | iter = 143 | Mean = 4.405251 | Best = 4.807607 GA | iter = 144 | Mean = 4.184930 | Best = 4.807607 GA | iter = 145 | Mean = 4.283658 | Best = 4.807607 GA | iter = 146 | Mean = 4.187154 | Best = 4.807610 GA | iter = 147 | Mean = 4.189203 | Best = 4.807610 GA | iter = 148 | Mean = 4.337748 | Best = 4.807610 GA | iter = 149 | Mean = 3.888997 | Best = 4.807610 GA | iter = 150 | Mean = 3.702154 | Best = 4.807610 GA | iter = 151 | Mean = 3.32950 | Best = 4.80761 GA | iter = 152 | Mean = 3.956297 | Best = 4.807610 GA | iter = 153 | Mean = 4.117974 | Best = 4.807610 GA | iter = 154 | Mean = 4.372399 | Best = 4.807610 GA | iter = 155 | Mean = 3.851573 | Best = 4.807610 GA | iter = 156 | Mean = 4.249204 | Best = 4.807610 GA | iter = 157 | Mean = 4.038657 | Best = 4.808121 GA | iter = 158 | Mean = 3.407550 | Best = 4.808121 GA | iter = 159 | Mean = 4.172469 | Best = 4.808121 GA | iter = 160 | Mean = 4.493322 | Best = 4.808121 GA | iter = 161 | Mean = 4.166260 | Best = 4.808121 GA | iter = 162 | Mean = 3.980522 | Best = 4.808121 GA | iter = 163 | Mean = 4.254644 | Best = 4.808121 GA | iter = 164 | Mean = 4.382732 | Best = 4.808396 GA | iter = 165 | Mean = 4.435536 | Best = 4.808396 GA | iter = 166 | Mean = 4.201591 | Best = 4.808396 GA | iter = 167 | Mean = 4.417317 | Best = 4.808396 GA | iter = 168 | Mean = 4.486776 | Best = 4.808396 GA | iter = 169 | Mean = 4.422990 | Best = 4.808396 GA | iter = 170 | Mean = 4.460022 | Best = 4.808396 GA | iter = 171 | Mean = 4.320876 | Best = 4.808396 GA | iter = 172 | Mean = 4.216702 | Best = 4.808396 GA | iter = 173 | Mean = 4.136090 | Best = 4.808396 GA | iter = 174 | Mean = 4.498567 | Best = 4.808396 GA | iter = 175 | Mean = 4.344520 | Best = 4.808396 GA | iter = 176 | Mean = 4.364640 | Best = 4.808396 GA | iter = 177 | Mean = 4.043988 | Best = 4.808396 GA | iter = 178 | Mean = 4.019645 | Best = 4.808396 GA | iter = 179 | Mean = 4.338031 | Best = 4.808396 GA | iter = 180 | Mean = 4.169022 | Best = 4.808396 GA | iter = 181 | Mean = 4.353347 | Best = 4.808396 GA | iter = 182 | Mean = 4.572399 | Best = 4.808396 GA | iter = 183 | Mean = 4.516530 | Best = 4.808396 GA | iter = 184 | Mean = 4.458662 | Best = 4.808396 GA | iter = 185 | Mean = 4.430911 | Best = 4.808396 GA | iter = 186 | Mean = 4.050735 | Best = 4.808396 GA | iter = 187 | Mean = 3.873708 | Best = 4.808396 GA | iter = 188 | Mean = 4.576054 | Best = 4.808396 GA | iter = 189 | Mean = 4.602358 | Best = 4.808396 GA | iter = 190 | Mean = 4.571705 | Best = 4.808396 GA | iter = 191 | Mean = 4.425244 | Best = 4.808396 GA | iter = 192 | Mean = 3.797310 | Best = 4.808396 GA | iter = 193 | Mean = 3.864039 | Best = 4.808410 GA | iter = 194 | Mean = 3.806486 | Best = 4.808410 GA | iter = 195 | Mean = 3.645371 | Best = 4.808410 GA | iter = 196 | Mean = 3.865748 | Best = 4.808410 GA | iter = 197 | Mean = 3.920219 | Best = 4.808410 GA | iter = 198 | Mean = 4.335065 | Best = 4.808410 GA | iter = 199 | Mean = 3.934611 | Best = 4.808410 GA | iter = 200 | Mean = 4.424539 | Best = 4.808410 GA | iter = 201 | Mean = 4.036793 | Best = 4.808410 GA | iter = 202 | Mean = 3.724739 | Best = 4.808410 GA | iter = 203 | Mean = 3.265933 | Best = 4.808410 GA | iter = 204 | Mean = 3.652677 | Best = 4.808410 GA | iter = 205 | Mean = 4.238926 | Best = 4.808410 GA | iter = 206 | Mean = 4.025289 | Best = 4.808410 GA | iter = 207 | Mean = 3.446127 | Best = 4.808410 GA | iter = 208 | Mean = 3.507018 | Best = 4.808410 GA | iter = 209 | Mean = 3.811165 | Best = 4.808410 GA | iter = 210 | Mean = 3.895911 | Best = 4.808410 GA | iter = 211 | Mean = 4.017722 | Best = 4.808410 GA | iter = 212 | Mean = 4.39228 | Best = 4.80841 GA | iter = 213 | Mean = 3.610332 | Best = 4.808410 GA | iter = 214 | Mean = 4.296219 | Best = 4.808410 GA | iter = 215 | Mean = 4.415381 | Best = 4.808410 GA | iter = 216 | Mean = 4.036405 | Best = 4.808410 GA | iter = 217 | Mean = 4.129586 | Best = 4.843301 GA | iter = 218 | Mean = 3.959341 | Best = 4.843301 GA | iter = 219 | Mean = 4.056032 | Best = 4.843301 GA | iter = 220 | Mean = 4.100945 | Best = 4.843301 GA | iter = 221 | Mean = 4.071438 | Best = 4.843301 GA | iter = 222 | Mean = 4.036919 | Best = 4.843301 GA | iter = 223 | Mean = 4.191028 | Best = 4.843301 GA | iter = 224 | Mean = 3.794563 | Best = 4.844892 GA | iter = 225 | Mean = 3.859112 | Best = 4.844892 GA | iter = 226 | Mean = 4.251265 | Best = 4.844892 GA | iter = 227 | Mean = 4.223604 | Best = 4.844892 GA | iter = 228 | Mean = 3.822945 | Best = 4.844892 GA | iter = 229 | Mean = 4.147935 | Best = 4.844892 GA | iter = 230 | Mean = 4.196765 | Best = 4.844892 GA | iter = 231 | Mean = 4.141351 | Best = 4.844892 GA | iter = 232 | Mean = 4.136366 | Best = 4.844892 GA | iter = 233 | Mean = 3.845479 | Best = 4.845029 GA | iter = 234 | Mean = 4.260147 | Best = 4.845029 GA | iter = 235 | Mean = 4.350130 | Best = 4.845029 GA | iter = 236 | Mean = 4.080183 | Best = 4.845029 GA | iter = 237 | Mean = 4.067979 | Best = 4.845029 GA | iter = 238 | Mean = 4.393433 | Best = 4.845029 GA | iter = 239 | Mean = 4.418473 | Best = 4.845029 GA | iter = 240 | Mean = 4.332463 | Best = 4.860066 GA | iter = 241 | Mean = 4.406154 | Best = 4.860066 GA | iter = 242 | Mean = 4.108121 | Best = 4.860066 GA | iter = 243 | Mean = 4.141312 | Best = 4.860066 GA | iter = 244 | Mean = 4.281492 | Best = 4.860066 GA | iter = 245 | Mean = 4.299196 | Best = 4.860066 GA | iter = 246 | Mean = 4.158414 | Best = 4.860066 GA | iter = 247 | Mean = 3.821497 | Best = 4.860066 GA | iter = 248 | Mean = 4.234336 | Best = 4.860066 GA | iter = 249 | Mean = 4.305331 | Best = 4.860066 GA | iter = 250 | Mean = 4.34143 | Best = 4.86390 GA | iter = 251 | Mean = 4.038891 | Best = 4.863900 GA | iter = 252 | Mean = 4.196963 | Best = 4.863900 GA | iter = 253 | Mean = 4.319377 | Best = 4.863900 GA | iter = 254 | Mean = 4.645782 | Best = 4.863900 GA | iter = 255 | Mean = 4.312678 | Best = 4.863900 GA | iter = 256 | Mean = 4.560896 | Best = 4.863900 GA | iter = 257 | Mean = 4.103232 | Best = 4.863900 GA | iter = 258 | Mean = 4.255086 | Best = 4.863900 GA | iter = 259 | Mean = 4.240225 | Best = 4.863900 GA | iter = 260 | Mean = 4.004714 | Best = 4.863900 GA | iter = 261 | Mean = 4.448624 | Best = 4.863900 GA | iter = 262 | Mean = 4.724487 | Best = 4.863900 GA | iter = 263 | Mean = 4.155863 | Best = 4.863900 GA | iter = 264 | Mean = 4.510952 | Best = 4.863900 GA | iter = 265 | Mean = 4.234872 | Best = 4.863900 GA | iter = 266 | Mean = 4.546767 | Best = 4.863900 GA | iter = 267 | Mean = 4.325149 | Best = 4.863900 GA | iter = 268 | Mean = 3.95302 | Best = 4.86390 GA | iter = 269 | Mean = 4.296184 | Best = 4.863900 GA | iter = 270 | Mean = 4.25156 | Best = 4.86390 GA | iter = 271 | Mean = 4.556946 | Best = 4.863900 GA | iter = 272 | Mean = 4.618561 | Best = 4.863900 GA | iter = 273 | Mean = 4.420024 | Best = 4.863900 GA | iter = 274 | Mean = 4.41404 | Best = 4.86390 GA | iter = 275 | Mean = 3.797373 | Best = 4.863900 GA | iter = 276 | Mean = 4.094944 | Best = 4.863900 GA | iter = 277 | Mean = 4.152068 | Best = 4.863900 GA | iter = 278 | Mean = 4.177275 | Best = 4.863900 GA | iter = 279 | Mean = 3.908427 | Best = 4.865740 GA | iter = 280 | Mean = 3.802854 | Best = 4.865740 GA | iter = 281 | Mean = 4.333372 | Best = 4.865740 GA | iter = 282 | Mean = 4.078893 | Best = 4.865740 GA | iter = 283 | Mean = 4.121338 | Best = 4.865740 GA | iter = 284 | Mean = 4.023156 | Best = 4.865740 GA | iter = 285 | Mean = 3.875798 | Best = 4.865740 GA | iter = 286 | Mean = 3.689202 | Best = 4.865740 GA | iter = 287 | Mean = 4.02996 | Best = 4.86574 GA | iter = 288 | Mean = 3.896839 | Best = 4.865740 GA | iter = 289 | Mean = 3.434214 | Best = 4.865740 GA | iter = 290 | Mean = 3.533524 | Best = 4.865740 GA | iter = 291 | Mean = 3.940955 | Best = 4.865740 GA | iter = 292 | Mean = 3.844692 | Best = 4.865740 GA | iter = 293 | Mean = 4.210962 | Best = 4.865740 GA | iter = 294 | Mean = 4.554735 | Best = 4.865740 GA | iter = 295 | Mean = 3.918708 | Best = 4.865740 GA | iter = 296 | Mean = 4.539778 | Best = 4.865740 GA | iter = 297 | Mean = 4.47183 | Best = 4.86574 GA | iter = 298 | Mean = 4.21293 | Best = 4.86574 GA | iter = 299 | Mean = 4.401318 | Best = 4.865740 GA | iter = 300 | Mean = 4.293777 | Best = 4.865740 GA | iter = 301 | Mean = 4.327888 | Best = 4.865740 GA | iter = 302 | Mean = 4.530344 | Best = 4.865740 GA | iter = 303 | Mean = 4.255603 | Best = 4.865740 GA | iter = 304 | Mean = 4.628991 | Best = 4.865740 GA | iter = 305 | Mean = 4.42444 | Best = 4.86574 GA | iter = 306 | Mean = 4.400977 | Best = 4.865740 GA | iter = 307 | Mean = 4.305027 | Best = 4.865740 GA | iter = 308 | Mean = 4.225047 | Best = 4.865740 GA | iter = 309 | Mean = 4.106098 | Best = 4.865740 GA | iter = 310 | Mean = 4.133206 | Best = 4.865740 GA | iter = 311 | Mean = 4.575439 | Best = 4.865740 GA | iter = 312 | Mean = 4.451896 | Best = 4.865740 GA | iter = 313 | Mean = 4.605246 | Best = 4.865740 GA | iter = 314 | Mean = 4.311326 | Best = 4.865740 GA | iter = 315 | Mean = 4.49893 | Best = 4.86574 GA | iter = 316 | Mean = 4.322263 | Best = 4.865740 GA | iter = 317 | Mean = 4.080791 | Best = 4.865740 GA | iter = 318 | Mean = 4.097614 | Best = 4.865740 GA | iter = 319 | Mean = 3.883963 | Best = 4.865740 GA | iter = 320 | Mean = 4.154584 | Best = 4.865740 GA | iter = 321 | Mean = 3.844307 | Best = 4.865740 GA | iter = 322 | Mean = 4.510918 | Best = 4.865740 GA | iter = 323 | Mean = 4.601054 | Best = 4.865740 GA | iter = 324 | Mean = 4.698827 | Best = 4.865740 GA | iter = 325 | Mean = 4.383664 | Best = 4.865740 GA | iter = 326 | Mean = 4.323948 | Best = 4.865740 GA | iter = 327 | Mean = 4.308914 | Best = 4.865740 GA | iter = 328 | Mean = 4.150552 | Best = 4.865740 GA | iter = 329 | Mean = 4.043349 | Best = 4.865740 GA | iter = 330 | Mean = 3.837871 | Best = 4.865740 GA | iter = 331 | Mean = 3.580567 | Best = 4.865740 GA | iter = 332 | Mean = 4.172246 | Best = 4.865740 GA | iter = 333 | Mean = 3.803783 | Best = 4.879897 GA | iter = 334 | Mean = 4.133659 | Best = 4.879897 GA | iter = 335 | Mean = 4.241599 | Best = 4.879897 GA | iter = 336 | Mean = 4.126509 | Best = 4.879897 GA | iter = 337 | Mean = 4.057852 | Best = 4.879897 GA | iter = 338 | Mean = 3.634513 | Best = 4.879897 GA | iter = 339 | Mean = 3.647011 | Best = 4.879897 GA | iter = 340 | Mean = 3.899497 | Best = 4.879897 GA | iter = 341 | Mean = 4.298549 | Best = 4.879897 GA | iter = 342 | Mean = 3.988019 | Best = 4.879897 GA | iter = 343 | Mean = 3.960286 | Best = 4.879897 GA | iter = 344 | Mean = 3.749066 | Best = 4.879897 GA | iter = 345 | Mean = 3.615440 | Best = 4.879897 GA | iter = 346 | Mean = 4.229718 | Best = 4.879897 GA | iter = 347 | Mean = 3.982356 | Best = 4.879897 GA | iter = 348 | Mean = 4.583027 | Best = 4.879897 GA | iter = 349 | Mean = 4.053428 | Best = 4.879897 GA | iter = 350 | Mean = 3.991683 | Best = 4.879897 GA | iter = 351 | Mean = 4.132441 | Best = 4.879897 GA | iter = 352 | Mean = 4.414995 | Best = 4.879897 GA | iter = 353 | Mean = 4.502121 | Best = 4.879897 GA | iter = 354 | Mean = 3.747676 | Best = 4.879897 GA | iter = 355 | Mean = 4.482093 | Best = 4.879897 GA | iter = 356 | Mean = 4.341220 | Best = 4.879897 GA | iter = 357 | Mean = 4.446375 | Best = 4.880104 GA | iter = 358 | Mean = 4.674761 | Best = 4.880104 GA | iter = 359 | Mean = 4.211905 | Best = 4.880104 GA | iter = 360 | Mean = 3.884520 | Best = 4.880104 GA | iter = 361 | Mean = 4.243745 | Best = 4.880104 GA | iter = 362 | Mean = 4.375370 | Best = 4.880104 GA | iter = 363 | Mean = 4.478073 | Best = 4.880104 GA | iter = 364 | Mean = 3.723878 | Best = 4.880104 GA | iter = 365 | Mean = 3.423034 | Best = 4.880104 GA | iter = 366 | Mean = 4.226297 | Best = 4.880104 GA | iter = 367 | Mean = 4.457080 | Best = 4.880104 GA | iter = 368 | Mean = 4.195256 | Best = 4.880104 GA | iter = 369 | Mean = 3.930738 | Best = 4.880104 GA | iter = 370 | Mean = 4.187286 | Best = 4.880104 GA | iter = 371 | Mean = 4.128786 | Best = 4.880104 GA | iter = 372 | Mean = 4.223219 | Best = 4.880104 GA | iter = 373 | Mean = 4.232308 | Best = 4.880321 GA | iter = 374 | Mean = 3.909842 | Best = 4.880321 GA | iter = 375 | Mean = 3.903704 | Best = 4.880321 GA | iter = 376 | Mean = 4.182214 | Best = 4.880321 GA | iter = 377 | Mean = 3.514720 | Best = 4.880321 GA | iter = 378 | Mean = 2.943741 | Best = 4.880321 GA | iter = 379 | Mean = 3.766974 | Best = 4.880321 GA | iter = 380 | Mean = 4.226486 | Best = 4.880321 GA | iter = 381 | Mean = 4.452969 | Best = 4.880321 GA | iter = 382 | Mean = 4.079375 | Best = 4.880321 GA | iter = 383 | Mean = 4.597056 | Best = 4.880321 GA | iter = 384 | Mean = 4.290027 | Best = 4.880321 GA | iter = 385 | Mean = 4.303781 | Best = 4.880321 GA | iter = 386 | Mean = 3.648353 | Best = 4.880321 GA | iter = 387 | Mean = 3.300243 | Best = 4.880321 GA | iter = 388 | Mean = 3.975848 | Best = 4.880321 GA | iter = 389 | Mean = 3.804309 | Best = 4.880321 GA | iter = 390 | Mean = 3.628713 | Best = 4.880321 GA | iter = 391 | Mean = 4.006457 | Best = 4.880321 GA | iter = 392 | Mean = 4.162475 | Best = 4.883355 GA | iter = 393 | Mean = 4.381361 | Best = 4.883355 GA | iter = 394 | Mean = 4.395448 | Best = 4.883355 GA | iter = 395 | Mean = 4.186716 | Best = 4.883355 GA | iter = 396 | Mean = 4.211205 | Best = 4.883355 GA | iter = 397 | Mean = 4.226455 | Best = 4.883355 GA | iter = 398 | Mean = 3.866624 | Best = 4.883355 GA | iter = 399 | Mean = 4.195483 | Best = 4.883355 GA | iter = 400 | Mean = 4.358684 | Best = 4.883355 GA | iter = 401 | Mean = 4.212832 | Best = 4.883355 GA | iter = 402 | Mean = 4.050418 | Best = 4.883355 GA | iter = 403 | Mean = 3.766591 | Best = 4.883355 GA | iter = 404 | Mean = 3.842576 | Best = 4.883355 GA | iter = 405 | Mean = 4.253048 | Best = 4.883355 GA | iter = 406 | Mean = 4.206368 | Best = 4.883355 GA | iter = 407 | Mean = 4.373183 | Best = 4.883355 GA | iter = 408 | Mean = 4.253486 | Best = 4.883355 GA | iter = 409 | Mean = 3.683810 | Best = 4.883355 GA | iter = 410 | Mean = 3.962909 | Best = 4.883355 GA | iter = 411 | Mean = 3.360606 | Best = 4.883355 GA | iter = 412 | Mean = 3.322311 | Best = 4.883355 GA | iter = 413 | Mean = 4.050536 | Best = 4.883355 GA | iter = 414 | Mean = 4.093664 | Best = 4.883355 GA | iter = 415 | Mean = 4.402079 | Best = 4.883355 GA | iter = 416 | Mean = 3.925101 | Best = 4.883355 GA | iter = 417 | Mean = 4.069231 | Best = 4.883355 GA | iter = 418 | Mean = 4.327985 | Best = 4.883355 GA | iter = 419 | Mean = 4.255169 | Best = 4.883355 GA | iter = 420 | Mean = 4.256586 | Best = 4.883355 GA | iter = 421 | Mean = 4.372621 | Best = 4.883355 GA | iter = 422 | Mean = 4.053979 | Best = 4.883355 GA | iter = 423 | Mean = 4.091214 | Best = 4.883355 GA | iter = 424 | Mean = 4.110293 | Best = 4.883355 GA | iter = 425 | Mean = 4.468707 | Best = 4.883355 GA | iter = 426 | Mean = 4.674913 | Best = 4.883355 GA | iter = 427 | Mean = 4.224172 | Best = 4.883355 GA | iter = 428 | Mean = 3.924926 | Best = 4.883355 GA | iter = 429 | Mean = 3.960011 | Best = 4.883355 GA | iter = 430 | Mean = 4.210621 | Best = 4.883355 GA | iter = 431 | Mean = 4.404310 | Best = 4.883355 GA | iter = 432 | Mean = 4.448848 | Best = 4.883355 GA | iter = 433 | Mean = 4.104248 | Best = 4.883355 GA | iter = 434 | Mean = 4.115596 | Best = 4.883355 GA | iter = 435 | Mean = 4.155122 | Best = 4.883355 GA | iter = 436 | Mean = 3.905107 | Best = 4.883355 GA | iter = 437 | Mean = 4.345823 | Best = 4.883355 GA | iter = 438 | Mean = 4.254621 | Best = 4.883355 GA | iter = 439 | Mean = 4.434105 | Best = 4.883355 GA | iter = 440 | Mean = 4.031097 | Best = 4.883355 GA | iter = 441 | Mean = 4.384131 | Best = 4.883355 GA | iter = 442 | Mean = 4.499428 | Best = 4.883355 GA | iter = 443 | Mean = 4.489460 | Best = 4.883355 GA | iter = 444 | Mean = 4.309377 | Best = 4.883355 GA | iter = 445 | Mean = 4.332383 | Best = 4.883355 GA | iter = 446 | Mean = 3.945244 | Best = 4.883355 GA | iter = 447 | Mean = 4.077560 | Best = 4.883355 GA | iter = 448 | Mean = 4.328452 | Best = 4.883355 GA | iter = 449 | Mean = 3.963803 | Best = 4.883355 GA | iter = 450 | Mean = 3.994722 | Best = 4.883355 GA | iter = 451 | Mean = 4.147056 | Best = 4.883355 GA | iter = 452 | Mean = 4.078689 | Best = 4.883355 GA | iter = 453 | Mean = 4.277216 | Best = 4.883355 GA | iter = 454 | Mean = 4.111645 | Best = 4.883355 GA | iter = 455 | Mean = 4.399775 | Best = 4.883355 GA | iter = 456 | Mean = 4.046580 | Best = 4.883355 GA | iter = 457 | Mean = 3.804686 | Best = 4.883355 GA | iter = 458 | Mean = 4.005363 | Best = 4.883355 GA | iter = 459 | Mean = 4.515786 | Best = 4.883355 GA | iter = 460 | Mean = 4.484678 | Best = 4.883355 GA | iter = 461 | Mean = 4.559523 | Best = 4.883355 GA | iter = 462 | Mean = 4.445385 | Best = 4.883355 GA | iter = 463 | Mean = 4.622212 | Best = 4.883355 GA | iter = 464 | Mean = 4.175103 | Best = 4.883355 GA | iter = 465 | Mean = 3.971097 | Best = 4.883355 GA | iter = 466 | Mean = 4.314815 | Best = 4.883355 GA | iter = 467 | Mean = 4.670678 | Best = 4.883355 GA | iter = 468 | Mean = 4.110520 | Best = 4.883355 GA | iter = 469 | Mean = 4.366137 | Best = 4.883355 GA | iter = 470 | Mean = 4.212352 | Best = 4.883355 GA | iter = 471 | Mean = 4.443323 | Best = 4.883355 GA | iter = 472 | Mean = 4.229185 | Best = 4.883355 GA | iter = 473 | Mean = 4.488857 | Best = 4.883355 GA | iter = 474 | Mean = 4.097661 | Best = 4.883355 GA | iter = 475 | Mean = 4.499515 | Best = 4.883355 GA | iter = 476 | Mean = 4.714629 | Best = 4.883355 GA | iter = 477 | Mean = 4.350501 | Best = 4.883355 GA | iter = 478 | Mean = 4.500414 | Best = 4.883355 GA | iter = 479 | Mean = 3.976027 | Best = 4.883355 GA | iter = 480 | Mean = 4.225450 | Best = 4.883355 GA | iter = 481 | Mean = 3.915670 | Best = 4.883355 GA | iter = 482 | Mean = 3.937670 | Best = 4.883355 GA | iter = 483 | Mean = 4.270843 | Best = 4.883355 GA | iter = 484 | Mean = 4.452442 | Best = 4.883355 GA | iter = 485 | Mean = 4.202377 | Best = 4.883355 GA | iter = 486 | Mean = 4.597868 | Best = 4.883355 GA | iter = 487 | Mean = 4.088671 | Best = 4.883355 GA | iter = 488 | Mean = 4.371724 | Best = 4.883355 GA | iter = 489 | Mean = 3.893352 | Best = 4.883355 GA | iter = 490 | Mean = 4.469933 | Best = 4.883355 GA | iter = 491 | Mean = 4.137920 | Best = 4.883355 GA | iter = 492 | Mean = 4.130518 | Best = 4.883355 GA | iter = 493 | Mean = 4.095231 | Best = 4.883355 GA | iter = 494 | Mean = 4.285774 | Best = 4.883355 GA | iter = 495 | Mean = 4.418687 | Best = 4.883355 GA | iter = 496 | Mean = 4.463213 | Best = 4.883355 GA | iter = 497 | Mean = 4.175073 | Best = 4.883355 GA | iter = 498 | Mean = 4.222641 | Best = 4.883355 GA | iter = 499 | Mean = 3.836858 | Best = 4.883355 GA | iter = 500 | Mean = 4.278270 | Best = 4.883355 GA | iter = 501 | Mean = 4.439642 | Best = 4.883355 GA | iter = 502 | Mean = 4.265244 | Best = 4.883355 GA | iter = 503 | Mean = 4.229397 | Best = 4.883355 GA | iter = 504 | Mean = 4.054677 | Best = 4.883355 GA | iter = 505 | Mean = 3.956933 | Best = 4.883355 GA | iter = 506 | Mean = 4.339441 | Best = 4.883355 GA | iter = 507 | Mean = 4.395777 | Best = 4.883355 GA | iter = 508 | Mean = 4.336906 | Best = 4.883355 GA | iter = 509 | Mean = 4.332604 | Best = 4.883355 GA | iter = 510 | Mean = 3.998476 | Best = 4.883355 GA | iter = 511 | Mean = 4.001734 | Best = 4.883355 GA | iter = 512 | Mean = 3.550158 | Best = 4.883355 GA | iter = 513 | Mean = 4.018130 | Best = 4.889578 GA | iter = 514 | Mean = 4.148540 | Best = 4.889578 GA | iter = 515 | Mean = 4.507165 | Best = 4.889578 GA | iter = 516 | Mean = 4.432054 | Best = 4.889578 GA | iter = 517 | Mean = 4.502811 | Best = 4.889578 GA | iter = 518 | Mean = 4.167615 | Best = 4.889578 GA | iter = 519 | Mean = 4.402446 | Best = 4.889578 GA | iter = 520 | Mean = 4.528674 | Best = 4.889578 GA | iter = 521 | Mean = 4.532371 | Best = 4.889578 GA | iter = 522 | Mean = 4.340939 | Best = 4.889578 GA | iter = 523 | Mean = 4.133559 | Best = 4.889578 GA | iter = 524 | Mean = 4.621039 | Best = 4.889578 GA | iter = 525 | Mean = 4.348275 | Best = 4.889578 GA | iter = 526 | Mean = 4.316525 | Best = 4.889578 GA | iter = 527 | Mean = 4.368881 | Best = 4.889578 GA | iter = 528 | Mean = 4.302691 | Best = 4.889578 GA | iter = 529 | Mean = 4.205289 | Best = 4.889578 GA | iter = 530 | Mean = 4.252829 | Best = 4.889578 GA | iter = 531 | Mean = 4.474161 | Best = 4.889578 GA | iter = 532 | Mean = 4.204126 | Best = 4.889578 GA | iter = 533 | Mean = 4.507958 | Best = 4.889578 GA | iter = 534 | Mean = 4.508443 | Best = 4.889578 GA | iter = 535 | Mean = 3.877551 | Best = 4.889578 GA | iter = 536 | Mean = 3.784491 | Best = 4.889578 GA | iter = 537 | Mean = 4.699100 | Best = 4.889578 GA | iter = 538 | Mean = 4.603841 | Best = 4.889578 GA | iter = 539 | Mean = 3.814097 | Best = 4.889578 GA | iter = 540 | Mean = 4.415370 | Best = 4.889578 GA | iter = 541 | Mean = 4.378044 | Best = 4.889578 GA | iter = 542 | Mean = 4.160236 | Best = 4.889578 GA | iter = 543 | Mean = 4.498960 | Best = 4.889578 GA | iter = 544 | Mean = 4.517655 | Best = 4.889578 GA | iter = 545 | Mean = 4.671725 | Best = 4.889578 GA | iter = 546 | Mean = 4.542864 | Best = 4.889578 GA | iter = 547 | Mean = 4.457632 | Best = 4.889578 GA | iter = 548 | Mean = 4.183443 | Best = 4.889578 GA | iter = 549 | Mean = 4.040762 | Best = 4.889578 GA | iter = 550 | Mean = 3.968213 | Best = 4.889578 GA | iter = 551 | Mean = 4.249740 | Best = 4.889578 GA | iter = 552 | Mean = 3.922989 | Best = 4.889578 GA | iter = 553 | Mean = 4.491176 | Best = 4.889578 GA | iter = 554 | Mean = 4.176229 | Best = 4.889578 GA | iter = 555 | Mean = 4.409044 | Best = 4.889578 GA | iter = 556 | Mean = 4.297296 | Best = 4.889969 GA | iter = 557 | Mean = 4.280098 | Best = 4.889969 GA | iter = 558 | Mean = 4.250875 | Best = 4.889969 GA | iter = 559 | Mean = 4.269303 | Best = 4.889969 GA | iter = 560 | Mean = 3.884629 | Best = 4.889969 GA | iter = 561 | Mean = 4.323722 | Best = 4.889969 GA | iter = 562 | Mean = 4.615786 | Best = 4.889969 GA | iter = 563 | Mean = 4.690216 | Best = 4.889969 GA | iter = 564 | Mean = 4.392801 | Best = 4.889969 GA | iter = 565 | Mean = 4.079511 | Best = 4.889969 GA | iter = 566 | Mean = 4.486223 | Best = 4.889969 GA | iter = 567 | Mean = 4.479194 | Best = 4.889969 GA | iter = 568 | Mean = 4.469671 | Best = 4.889969 GA | iter = 569 | Mean = 4.676203 | Best = 4.889969 GA | iter = 570 | Mean = 4.716225 | Best = 4.889969 GA | iter = 571 | Mean = 4.545163 | Best = 4.889969 GA | iter = 572 | Mean = 4.673026 | Best = 4.889969 GA | iter = 573 | Mean = 4.644509 | Best = 4.889969 GA | iter = 574 | Mean = 4.383560 | Best = 4.889969 GA | iter = 575 | Mean = 4.362251 | Best = 4.889969 GA | iter = 576 | Mean = 3.599289 | Best = 4.889969 GA | iter = 577 | Mean = 4.506290 | Best = 4.889969 GA | iter = 578 | Mean = 4.150550 | Best = 4.889969 GA | iter = 579 | Mean = 4.064197 | Best = 4.889969 GA | iter = 580 | Mean = 4.411492 | Best = 4.889969 GA | iter = 581 | Mean = 4.568761 | Best = 4.889969 GA | iter = 582 | Mean = 4.382522 | Best = 4.889969 GA | iter = 583 | Mean = 4.218400 | Best = 4.889969 GA | iter = 584 | Mean = 4.201553 | Best = 4.889969 GA | iter = 585 | Mean = 4.565149 | Best = 4.889969 GA | iter = 586 | Mean = 4.694499 | Best = 4.889969 GA | iter = 587 | Mean = 4.013382 | Best = 4.889969 GA | iter = 588 | Mean = 4.714146 | Best = 4.889969 GA | iter = 589 | Mean = 4.368997 | Best = 4.889969 GA | iter = 590 | Mean = 4.203503 | Best = 4.889969 GA | iter = 591 | Mean = 4.252754 | Best = 4.889969 GA | iter = 592 | Mean = 4.393538 | Best = 4.889969 GA | iter = 593 | Mean = 4.557040 | Best = 4.889969 GA | iter = 594 | Mean = 4.570770 | Best = 4.889969 GA | iter = 595 | Mean = 4.361661 | Best = 4.889969 GA | iter = 596 | Mean = 4.585649 | Best = 4.889969 GA | iter = 597 | Mean = 4.292677 | Best = 4.889969 GA | iter = 598 | Mean = 4.370131 | Best = 4.889969 GA | iter = 599 | Mean = 4.196951 | Best = 4.889969 GA | iter = 600 | Mean = 4.188282 | Best = 4.889969 GA | iter = 601 | Mean = 3.811269 | Best = 4.889969 GA | iter = 602 | Mean = 3.901314 | Best = 4.889969 GA | iter = 603 | Mean = 4.308579 | Best = 4.889969 GA | iter = 604 | Mean = 4.383070 | Best = 4.889969 GA | iter = 605 | Mean = 3.971472 | Best = 4.889969 GA | iter = 606 | Mean = 4.050133 | Best = 4.889969 GA | iter = 607 | Mean = 4.369323 | Best = 4.889969 GA | iter = 608 | Mean = 4.463987 | Best = 4.889969 GA | iter = 609 | Mean = 4.203611 | Best = 4.889969 GA | iter = 610 | Mean = 4.627048 | Best = 4.889969 GA | iter = 611 | Mean = 3.975969 | Best = 4.889969 GA | iter = 612 | Mean = 4.437199 | Best = 4.889969 GA | iter = 613 | Mean = 3.611519 | Best = 4.889969 GA | iter = 614 | Mean = 4.184231 | Best = 4.889969 GA | iter = 615 | Mean = 3.173989 | Best = 4.889969 GA | iter = 616 | Mean = 3.550661 | Best = 4.889969 GA | iter = 617 | Mean = 3.543082 | Best = 4.889969 GA | iter = 618 | Mean = 3.434455 | Best = 4.889969 GA | iter = 619 | Mean = 3.973806 | Best = 4.889969 GA | iter = 620 | Mean = 4.007380 | Best = 4.889969 GA | iter = 621 | Mean = 3.942854 | Best = 4.889969 GA | iter = 622 | Mean = 4.141496 | Best = 4.889969 GA | iter = 623 | Mean = 4.332559 | Best = 4.889969 GA | iter = 624 | Mean = 4.195981 | Best = 4.889974 GA | iter = 625 | Mean = 4.507663 | Best = 4.889974 GA | iter = 626 | Mean = 3.763100 | Best = 4.889974 GA | iter = 627 | Mean = 4.244794 | Best = 4.889974 GA | iter = 628 | Mean = 4.018732 | Best = 4.889974 GA | iter = 629 | Mean = 4.578424 | Best = 4.889974 GA | iter = 630 | Mean = 4.024348 | Best = 4.889974 GA | iter = 631 | Mean = 4.331849 | Best = 4.889974 GA | iter = 632 | Mean = 3.615219 | Best = 4.889974 GA | iter = 633 | Mean = 4.015887 | Best = 4.889974 GA | iter = 634 | Mean = 4.004201 | Best = 4.889974 GA | iter = 635 | Mean = 4.313105 | Best = 4.889974 GA | iter = 636 | Mean = 4.304233 | Best = 4.889974 GA | iter = 637 | Mean = 3.969674 | Best = 4.889974 GA | iter = 638 | Mean = 4.467436 | Best = 4.889974 GA | iter = 639 | Mean = 4.370329 | Best = 4.889974 GA | iter = 640 | Mean = 3.872309 | Best = 4.889974 GA | iter = 641 | Mean = 4.573808 | Best = 4.889974 GA | iter = 642 | Mean = 4.317107 | Best = 4.889974 GA | iter = 643 | Mean = 4.364788 | Best = 4.889974 GA | iter = 644 | Mean = 3.944036 | Best = 4.889974 GA | iter = 645 | Mean = 3.652219 | Best = 4.889974 GA | iter = 646 | Mean = 3.640016 | Best = 4.889974 GA | iter = 647 | Mean = 4.631310 | Best = 4.890983 GA | iter = 648 | Mean = 4.081466 | Best = 4.890983 GA | iter = 649 | Mean = 4.460272 | Best = 4.890983 GA | iter = 650 | Mean = 3.944346 | Best = 4.890983 GA | iter = 651 | Mean = 3.865559 | Best = 4.890983 GA | iter = 652 | Mean = 4.195322 | Best = 4.890983 GA | iter = 653 | Mean = 4.225436 | Best = 4.890983 GA | iter = 654 | Mean = 3.953585 | Best = 4.890983 GA | iter = 655 | Mean = 4.151671 | Best = 4.890983 GA | iter = 656 | Mean = 4.144446 | Best = 4.890983 GA | iter = 657 | Mean = 3.764120 | Best = 4.890983 GA | iter = 658 | Mean = 4.440983 | Best = 4.890983 GA | iter = 659 | Mean = 4.013086 | Best = 4.890983 GA | iter = 660 | Mean = 3.942642 | Best = 4.890983 GA | iter = 661 | Mean = 4.026649 | Best = 4.890983 GA | iter = 662 | Mean = 3.943309 | Best = 4.890983 GA | iter = 663 | Mean = 3.977088 | Best = 4.890983 GA | iter = 664 | Mean = 3.526867 | Best = 4.890983 GA | iter = 665 | Mean = 3.793933 | Best = 4.890983 GA | iter = 666 | Mean = 4.247764 | Best = 4.890983 GA | iter = 667 | Mean = 4.001245 | Best = 4.890983 GA | iter = 668 | Mean = 3.691638 | Best = 4.891295 GA | iter = 669 | Mean = 4.171854 | Best = 4.891295 GA | iter = 670 | Mean = 4.023650 | Best = 4.891295 GA | iter = 671 | Mean = 4.066450 | Best = 4.891295 GA | iter = 672 | Mean = 3.663941 | Best = 4.891295 GA | iter = 673 | Mean = 3.933585 | Best = 4.891295 GA | iter = 674 | Mean = 4.312114 | Best = 4.891295 GA | iter = 675 | Mean = 4.231249 | Best = 4.891295 GA | iter = 676 | Mean = 3.882729 | Best = 4.891295 GA | iter = 677 | Mean = 4.177460 | Best = 4.891295 GA | iter = 678 | Mean = 4.334581 | Best = 4.891295 GA | iter = 679 | Mean = 4.291269 | Best = 4.891295 GA | iter = 680 | Mean = 4.361331 | Best = 4.891295 GA | iter = 681 | Mean = 4.299192 | Best = 4.891295 GA | iter = 682 | Mean = 4.412390 | Best = 4.891295 GA | iter = 683 | Mean = 4.516276 | Best = 4.891295 GA | iter = 684 | Mean = 4.474157 | Best = 4.891295 GA | iter = 685 | Mean = 4.361627 | Best = 4.891295 GA | iter = 686 | Mean = 4.481783 | Best = 4.891295 GA | iter = 687 | Mean = 4.312769 | Best = 4.891295 GA | iter = 688 | Mean = 4.400923 | Best = 4.891295 GA | iter = 689 | Mean = 3.931567 | Best = 4.891295 GA | iter = 690 | Mean = 4.454490 | Best = 4.891295 GA | iter = 691 | Mean = 4.391582 | Best = 4.891295 GA | iter = 692 | Mean = 4.116595 | Best = 4.891295 GA | iter = 693 | Mean = 4.226732 | Best = 4.891295 GA | iter = 694 | Mean = 4.537002 | Best = 4.891295 GA | iter = 695 | Mean = 4.291099 | Best = 4.891295 GA | iter = 696 | Mean = 4.480645 | Best = 4.891295 GA | iter = 697 | Mean = 4.041717 | Best = 4.891295 GA | iter = 698 | Mean = 3.597059 | Best = 4.891295 GA | iter = 699 | Mean = 4.265831 | Best = 4.891295 GA | iter = 700 | Mean = 4.239458 | Best = 4.891295 GA | iter = 701 | Mean = 4.416677 | Best = 4.891295 GA | iter = 702 | Mean = 4.466098 | Best = 4.891295 GA | iter = 703 | Mean = 4.523667 | Best = 4.891295 GA | iter = 704 | Mean = 4.315742 | Best = 4.891295 GA | iter = 705 | Mean = 4.503978 | Best = 4.891295 GA | iter = 706 | Mean = 4.338166 | Best = 4.891295 GA | iter = 707 | Mean = 3.778871 | Best = 4.891295 GA | iter = 708 | Mean = 3.459123 | Best = 4.891295 GA | iter = 709 | Mean = 3.801011 | Best = 4.891295 GA | iter = 710 | Mean = 4.202870 | Best = 4.891295 GA | iter = 711 | Mean = 4.040040 | Best = 4.891295 GA | iter = 712 | Mean = 3.999874 | Best = 4.891295 GA | iter = 713 | Mean = 3.831480 | Best = 4.891295 GA | iter = 714 | Mean = 4.134526 | Best = 4.891295 GA | iter = 715 | Mean = 3.807908 | Best = 4.891295 GA | iter = 716 | Mean = 3.844824 | Best = 4.891295 GA | iter = 717 | Mean = 3.980245 | Best = 4.891295 GA | iter = 718 | Mean = 4.403480 | Best = 4.891295 GA | iter = 719 | Mean = 4.411402 | Best = 4.891295 GA | iter = 720 | Mean = 3.748239 | Best = 4.891295 GA | iter = 721 | Mean = 4.489033 | Best = 4.891295 GA | iter = 722 | Mean = 4.283748 | Best = 4.891295 GA | iter = 723 | Mean = 3.319682 | Best = 4.891295 GA | iter = 724 | Mean = 4.311505 | Best = 4.891295 GA | iter = 725 | Mean = 4.358178 | Best = 4.891295 GA | iter = 726 | Mean = 3.091328 | Best = 4.891295 GA | iter = 727 | Mean = 3.548081 | Best = 4.891295 GA | iter = 728 | Mean = 3.486175 | Best = 4.891354 GA | iter = 729 | Mean = 3.611917 | Best = 4.891354 GA | iter = 730 | Mean = 4.159400 | Best = 4.891354 GA | iter = 731 | Mean = 4.178951 | Best = 4.891354 GA | iter = 732 | Mean = 3.792076 | Best = 4.891354 GA | iter = 733 | Mean = 3.511740 | Best = 4.891354 GA | iter = 734 | Mean = 3.794444 | Best = 4.891354 GA | iter = 735 | Mean = 4.121983 | Best = 4.891354 GA | iter = 736 | Mean = 4.464283 | Best = 4.891354 GA | iter = 737 | Mean = 4.093296 | Best = 4.891354 GA | iter = 738 | Mean = 4.520708 | Best = 4.891354 GA | iter = 739 | Mean = 4.322904 | Best = 4.891354 GA | iter = 740 | Mean = 4.260274 | Best = 4.891354 GA | iter = 741 | Mean = 4.148284 | Best = 4.891354 GA | iter = 742 | Mean = 3.949722 | Best = 4.891354 GA | iter = 743 | Mean = 4.116659 | Best = 4.891354 GA | iter = 744 | Mean = 4.141898 | Best = 4.891354 GA | iter = 745 | Mean = 3.996989 | Best = 4.891354 GA | iter = 746 | Mean = 4.030550 | Best = 4.891354 GA | iter = 747 | Mean = 4.520406 | Best = 4.891354 GA | iter = 748 | Mean = 4.147247 | Best = 4.891354 GA | iter = 749 | Mean = 4.391345 | Best = 4.891354 GA | iter = 750 | Mean = 4.280504 | Best = 4.891354 GA | iter = 751 | Mean = 4.627797 | Best = 4.891354 GA | iter = 752 | Mean = 4.096205 | Best = 4.891354 GA | iter = 753 | Mean = 4.341248 | Best = 4.891354 GA | iter = 754 | Mean = 4.096428 | Best = 4.891354 GA | iter = 755 | Mean = 3.834043 | Best = 4.891354 GA | iter = 756 | Mean = 4.290821 | Best = 4.891354 GA | iter = 757 | Mean = 4.506710 | Best = 4.891354 GA | iter = 758 | Mean = 4.281924 | Best = 4.891354 GA | iter = 759 | Mean = 4.553513 | Best = 4.891354 GA | iter = 760 | Mean = 4.733473 | Best = 4.891354 GA | iter = 761 | Mean = 4.588822 | Best = 4.891354 GA | iter = 762 | Mean = 4.365278 | Best = 4.891354 GA | iter = 763 | Mean = 4.241981 | Best = 4.891354 GA | iter = 764 | Mean = 4.595728 | Best = 4.891354 GA | iter = 765 | Mean = 4.392150 | Best = 4.891354 GA | iter = 766 | Mean = 4.279187 | Best = 4.891354 GA | iter = 767 | Mean = 4.511463 | Best = 4.891354 GA | iter = 768 | Mean = 4.472757 | Best = 4.891354 GA | iter = 769 | Mean = 4.448512 | Best = 4.891354 GA | iter = 770 | Mean = 4.549416 | Best = 4.891354 GA | iter = 771 | Mean = 4.318931 | Best = 4.891354 GA | iter = 772 | Mean = 4.108808 | Best = 4.891354 GA | iter = 773 | Mean = 4.248098 | Best = 4.891354 GA | iter = 774 | Mean = 4.390188 | Best = 4.891354 GA | iter = 775 | Mean = 4.097575 | Best = 4.891354 GA | iter = 776 | Mean = 4.355141 | Best = 4.891354 GA | iter = 777 | Mean = 3.999242 | Best = 4.891354 GA | iter = 778 | Mean = 3.407046 | Best = 4.891354 GA | iter = 779 | Mean = 4.052421 | Best = 4.891354 GA | iter = 780 | Mean = 4.517464 | Best = 4.891354 GA | iter = 781 | Mean = 4.001772 | Best = 4.891354 GA | iter = 782 | Mean = 4.521394 | Best = 4.891354 GA | iter = 783 | Mean = 3.778392 | Best = 4.891354 GA | iter = 784 | Mean = 4.353900 | Best = 4.891354 GA | iter = 785 | Mean = 4.751666 | Best = 4.891354 GA | iter = 786 | Mean = 4.575996 | Best = 4.891354 GA | iter = 787 | Mean = 4.201527 | Best = 4.891354 GA | iter = 788 | Mean = 3.825618 | Best = 4.891354 GA | iter = 789 | Mean = 4.250323 | Best = 4.891354 GA | iter = 790 | Mean = 3.538625 | Best = 4.891354 GA | iter = 791 | Mean = 4.129862 | Best = 4.891354 GA | iter = 792 | Mean = 3.776160 | Best = 4.891354 GA | iter = 793 | Mean = 3.471835 | Best = 4.891354 GA | iter = 794 | Mean = 4.319581 | Best = 4.891354 GA | iter = 795 | Mean = 4.298166 | Best = 4.891354 GA | iter = 796 | Mean = 3.959711 | Best = 4.891354 GA | iter = 797 | Mean = 4.248826 | Best = 4.891354 GA | iter = 798 | Mean = 4.245272 | Best = 4.891354 GA | iter = 799 | Mean = 4.681772 | Best = 4.891354 GA | iter = 800 | Mean = 4.521501 | Best = 4.891354 GA | iter = 801 | Mean = 4.254263 | Best = 4.891354 GA | iter = 802 | Mean = 4.437762 | Best = 4.891354 GA | iter = 803 | Mean = 4.428499 | Best = 4.891354 GA | iter = 804 | Mean = 4.582227 | Best = 4.891354 GA | iter = 805 | Mean = 4.465021 | Best = 4.891354 GA | iter = 806 | Mean = 4.423905 | Best = 4.891354 GA | iter = 807 | Mean = 4.255678 | Best = 4.891354 GA | iter = 808 | Mean = 4.184022 | Best = 4.891354 GA | iter = 809 | Mean = 4.357921 | Best = 4.891354 GA | iter = 810 | Mean = 4.314963 | Best = 4.891354 GA | iter = 811 | Mean = 4.433286 | Best = 4.891354 GA | iter = 812 | Mean = 4.572295 | Best = 4.891354 GA | iter = 813 | Mean = 4.385871 | Best = 4.891354 GA | iter = 814 | Mean = 4.602550 | Best = 4.891354 GA | iter = 815 | Mean = 4.420763 | Best = 4.891354 GA | iter = 816 | Mean = 4.391753 | Best = 4.891354 GA | iter = 817 | Mean = 4.613765 | Best = 4.891354 GA | iter = 818 | Mean = 3.911706 | Best = 4.891354 GA | iter = 819 | Mean = 4.535118 | Best = 4.891354 GA | iter = 820 | Mean = 4.273370 | Best = 4.891354 GA | iter = 821 | Mean = 3.975034 | Best = 4.891354 GA | iter = 822 | Mean = 4.199422 | Best = 4.891354 GA | iter = 823 | Mean = 4.558005 | Best = 4.891354 GA | iter = 824 | Mean = 4.477241 | Best = 4.891354 GA | iter = 825 | Mean = 4.512151 | Best = 4.891354 GA | iter = 826 | Mean = 4.453711 | Best = 4.891354 GA | iter = 827 | Mean = 3.973431 | Best = 4.891354 GA | iter = 828 | Mean = 4.685070 | Best = 4.891354 GA | iter = 829 | Mean = 4.567645 | Best = 4.891354 GA | iter = 830 | Mean = 3.940738 | Best = 4.891354 GA | iter = 831 | Mean = 4.172215 | Best = 4.891354 GA | iter = 832 | Mean = 4.198122 | Best = 4.891354 GA | iter = 833 | Mean = 4.311533 | Best = 4.891354 GA | iter = 834 | Mean = 4.367633 | Best = 4.891354 GA | iter = 835 | Mean = 4.450616 | Best = 4.891354 GA | iter = 836 | Mean = 4.475484 | Best = 4.891354 GA | iter = 837 | Mean = 4.637406 | Best = 4.891354 GA | iter = 838 | Mean = 4.598522 | Best = 4.891354 GA | iter = 839 | Mean = 4.674534 | Best = 4.891354 GA | iter = 840 | Mean = 4.348074 | Best = 4.891354 GA | iter = 841 | Mean = 4.039556 | Best = 4.891354 GA | iter = 842 | Mean = 4.039684 | Best = 4.891354 GA | iter = 843 | Mean = 4.465713 | Best = 4.891354 GA | iter = 844 | Mean = 4.572227 | Best = 4.891354 GA | iter = 845 | Mean = 4.496945 | Best = 4.891354 GA | iter = 846 | Mean = 4.473167 | Best = 4.891354 GA | iter = 847 | Mean = 4.228278 | Best = 4.891354 GA | iter = 848 | Mean = 4.171504 | Best = 4.891419 GA | iter = 849 | Mean = 3.716709 | Best = 4.891419 GA | iter = 850 | Mean = 4.330506 | Best = 4.891683 GA | iter = 851 | Mean = 4.477509 | Best = 4.891683 GA | iter = 852 | Mean = 4.638521 | Best = 4.891683 GA | iter = 853 | Mean = 4.480701 | Best = 4.891683 GA | iter = 854 | Mean = 3.803425 | Best = 4.891683 GA | iter = 855 | Mean = 4.469419 | Best = 4.891683 GA | iter = 856 | Mean = 4.029176 | Best = 4.891683 GA | iter = 857 | Mean = 4.057816 | Best = 4.891683 GA | iter = 858 | Mean = 4.102836 | Best = 4.891683 GA | iter = 859 | Mean = 4.202795 | Best = 4.891683 GA | iter = 860 | Mean = 3.891435 | Best = 4.891683 GA | iter = 861 | Mean = 4.375439 | Best = 4.891683 GA | iter = 862 | Mean = 3.706233 | Best = 4.891683 GA | iter = 863 | Mean = 4.500609 | Best = 4.891683 GA | iter = 864 | Mean = 4.727572 | Best = 4.891683 GA | iter = 865 | Mean = 4.799027 | Best = 4.891683 GA | iter = 866 | Mean = 4.102450 | Best = 4.891683 GA | iter = 867 | Mean = 4.355983 | Best = 4.891683 GA | iter = 868 | Mean = 4.652105 | Best = 4.891683 GA | iter = 869 | Mean = 4.399807 | Best = 4.891683 GA | iter = 870 | Mean = 4.301906 | Best = 4.891683 GA | iter = 871 | Mean = 4.297960 | Best = 4.891683 GA | iter = 872 | Mean = 4.331246 | Best = 4.891683 GA | iter = 873 | Mean = 4.232386 | Best = 4.891683 GA | iter = 874 | Mean = 4.470982 | Best = 4.891683 GA | iter = 875 | Mean = 4.226870 | Best = 4.891683 GA | iter = 876 | Mean = 4.204756 | Best = 4.891683 GA | iter = 877 | Mean = 4.205877 | Best = 4.891683 GA | iter = 878 | Mean = 4.272645 | Best = 4.891683 GA | iter = 879 | Mean = 4.831995 | Best = 4.891683 GA | iter = 880 | Mean = 4.547000 | Best = 4.891683 GA | iter = 881 | Mean = 4.534801 | Best = 4.891683 GA | iter = 882 | Mean = 4.731760 | Best = 4.891683 GA | iter = 883 | Mean = 4.569786 | Best = 4.891683 GA | iter = 884 | Mean = 4.328754 | Best = 4.891683 GA | iter = 885 | Mean = 3.759604 | Best = 4.891683 GA | iter = 886 | Mean = 3.501234 | Best = 4.891683 GA | iter = 887 | Mean = 3.781513 | Best = 4.891683 GA | iter = 888 | Mean = 3.903459 | Best = 4.891683 GA | iter = 889 | Mean = 4.401594 | Best = 4.891683 GA | iter = 890 | Mean = 4.288311 | Best = 4.891683 GA | iter = 891 | Mean = 4.479640 | Best = 4.891683 GA | iter = 892 | Mean = 4.417242 | Best = 4.891683 GA | iter = 893 | Mean = 4.065397 | Best = 4.891683 GA | iter = 894 | Mean = 4.494432 | Best = 4.891683 GA | iter = 895 | Mean = 4.317346 | Best = 4.891683 GA | iter = 896 | Mean = 4.716428 | Best = 4.891683 GA | iter = 897 | Mean = 4.313904 | Best = 4.891683 GA | iter = 898 | Mean = 4.085366 | Best = 4.891683 GA | iter = 899 | Mean = 4.308476 | Best = 4.891683 GA | iter = 900 | Mean = 4.462815 | Best = 4.891683 GA | iter = 901 | Mean = 4.338963 | Best = 4.891683 GA | iter = 902 | Mean = 4.608540 | Best = 4.891683 GA | iter = 903 | Mean = 4.485745 | Best = 4.891683 GA | iter = 904 | Mean = 4.384903 | Best = 4.891683 GA | iter = 905 | Mean = 4.731199 | Best = 4.891683 GA | iter = 906 | Mean = 4.740184 | Best = 4.891683 GA | iter = 907 | Mean = 4.040111 | Best = 4.891683 GA | iter = 908 | Mean = 3.913370 | Best = 4.891683 GA | iter = 909 | Mean = 4.380159 | Best = 4.891683 GA | iter = 910 | Mean = 4.212011 | Best = 4.891683 GA | iter = 911 | Mean = 4.321770 | Best = 4.891683 GA | iter = 912 | Mean = 3.488939 | Best = 4.891683 GA | iter = 913 | Mean = 4.076443 | Best = 4.891683 GA | iter = 914 | Mean = 4.028122 | Best = 4.891683 GA | iter = 915 | Mean = 3.966628 | Best = 4.891683 GA | iter = 916 | Mean = 3.474349 | Best = 4.891683 GA | iter = 917 | Mean = 4.109855 | Best = 4.891683 GA | iter = 918 | Mean = 4.125282 | Best = 4.891683 GA | iter = 919 | Mean = 4.638337 | Best = 4.891683 GA | iter = 920 | Mean = 4.421546 | Best = 4.891683 GA | iter = 921 | Mean = 4.366371 | Best = 4.891683 GA | iter = 922 | Mean = 4.539371 | Best = 4.891683 GA | iter = 923 | Mean = 4.321587 | Best = 4.891683 GA | iter = 924 | Mean = 4.281741 | Best = 4.891683 GA | iter = 925 | Mean = 4.306605 | Best = 4.891683 GA | iter = 926 | Mean = 4.263729 | Best = 4.891683 GA | iter = 927 | Mean = 4.265904 | Best = 4.891683 GA | iter = 928 | Mean = 4.464132 | Best = 4.891683 GA | iter = 929 | Mean = 4.156992 | Best = 4.891683 GA | iter = 930 | Mean = 4.391833 | Best = 4.891683 GA | iter = 931 | Mean = 4.430930 | Best = 4.891683 GA | iter = 932 | Mean = 4.551835 | Best = 4.891683 GA | iter = 933 | Mean = 4.474055 | Best = 4.891683 GA | iter = 934 | Mean = 4.042610 | Best = 4.891683 GA | iter = 935 | Mean = 4.475665 | Best = 4.891683 GA | iter = 936 | Mean = 4.134842 | Best = 4.891683 GA | iter = 937 | Mean = 3.705534 | Best = 4.891683 GA | iter = 938 | Mean = 3.632644 | Best = 4.891683 GA | iter = 939 | Mean = 4.157072 | Best = 4.891683 GA | iter = 940 | Mean = 4.167081 | Best = 4.891683 GA | iter = 941 | Mean = 4.226574 | Best = 4.891683 GA | iter = 942 | Mean = 4.362973 | Best = 4.891683 GA | iter = 943 | Mean = 4.180567 | Best = 4.891683 GA | iter = 944 | Mean = 4.362737 | Best = 4.891683 GA | iter = 945 | Mean = 4.591340 | Best = 4.891683 GA | iter = 946 | Mean = 4.248274 | Best = 4.891683 GA | iter = 947 | Mean = 4.323111 | Best = 4.891683 GA | iter = 948 | Mean = 4.409285 | Best = 4.891683 GA | iter = 949 | Mean = 4.361802 | Best = 4.891683 GA | iter = 950 | Mean = 4.261083 | Best = 4.891683 GA | iter = 951 | Mean = 4.084574 | Best = 4.891683 GA | iter = 952 | Mean = 4.630070 | Best = 4.891683 GA | iter = 953 | Mean = 4.691950 | Best = 4.891683 GA | iter = 954 | Mean = 4.543431 | Best = 4.891683 GA | iter = 955 | Mean = 3.522676 | Best = 4.891683 GA | iter = 956 | Mean = 4.358089 | Best = 4.891683 GA | iter = 957 | Mean = 4.352018 | Best = 4.891683 GA | iter = 958 | Mean = 4.435947 | Best = 4.891683 GA | iter = 959 | Mean = 4.330254 | Best = 4.891683 GA | iter = 960 | Mean = 4.582293 | Best = 4.891683 GA | iter = 961 | Mean = 4.377944 | Best = 4.891683 GA | iter = 962 | Mean = 4.423445 | Best = 4.891683 GA | iter = 963 | Mean = 3.966622 | Best = 4.891683 GA | iter = 964 | Mean = 3.972944 | Best = 4.891683 GA | iter = 965 | Mean = 4.109172 | Best = 4.891683 GA | iter = 966 | Mean = 4.385474 | Best = 4.891683 GA | iter = 967 | Mean = 4.416958 | Best = 4.891683 GA | iter = 968 | Mean = 4.417517 | Best = 4.891683 GA | iter = 969 | Mean = 4.716837 | Best = 4.891683 GA | iter = 970 | Mean = 4.481226 | Best = 4.891683 GA | iter = 971 | Mean = 4.027813 | Best = 4.891683 GA | iter = 972 | Mean = 3.268436 | Best = 4.891683 GA | iter = 973 | Mean = 4.306872 | Best = 4.891683 GA | iter = 974 | Mean = 4.218410 | Best = 4.891683 GA | iter = 975 | Mean = 3.977115 | Best = 4.891683 GA | iter = 976 | Mean = 3.915487 | Best = 4.891683 GA | iter = 977 | Mean = 4.374700 | Best = 4.891683 GA | iter = 978 | Mean = 4.066726 | Best = 4.891683 GA | iter = 979 | Mean = 4.017029 | Best = 4.891683 GA | iter = 980 | Mean = 4.299574 | Best = 4.891683 GA | iter = 981 | Mean = 4.388186 | Best = 4.891683 GA | iter = 982 | Mean = 4.690316 | Best = 4.891683 GA | iter = 983 | Mean = 4.468259 | Best = 4.891683 GA | iter = 984 | Mean = 4.661903 | Best = 4.891683 GA | iter = 985 | Mean = 4.511774 | Best = 4.891683 GA | iter = 986 | Mean = 4.250920 | Best = 4.891683 GA | iter = 987 | Mean = 4.331490 | Best = 4.891683 GA | iter = 988 | Mean = 3.522562 | Best = 4.891683 GA | iter = 989 | Mean = 4.040510 | Best = 4.891683 GA | iter = 990 | Mean = 4.532578 | Best = 4.891683 GA | iter = 991 | Mean = 4.420244 | Best = 4.891683 GA | iter = 992 | Mean = 4.331787 | Best = 4.891683 GA | iter = 993 | Mean = 4.547650 | Best = 4.891683 GA | iter = 994 | Mean = 4.642100 | Best = 4.891683 GA | iter = 995 | Mean = 4.528524 | Best = 4.891683 GA | iter = 996 | Mean = 4.473163 | Best = 4.891683 GA | iter = 997 | Mean = 4.463228 | Best = 4.891683 GA | iter = 998 | Mean = 4.136081 | Best = 4.891683 GA | iter = 999 | Mean = 4.118565 | Best = 4.891683 GA | iter = 1000 | Mean = 4.373566 | Best = 4.891683 GA | iter = 1001 | Mean = 4.580010 | Best = 4.891683 GA | iter = 1002 | Mean = 3.906502 | Best = 4.891683 GA | iter = 1003 | Mean = 3.569318 | Best = 4.891683 GA | iter = 1004 | Mean = 3.618429 | Best = 4.891683 GA | iter = 1005 | Mean = 3.756276 | Best = 4.891683 GA | iter = 1006 | Mean = 3.872708 | Best = 4.891683 GA | iter = 1007 | Mean = 4.115259 | Best = 4.891683 GA | iter = 1008 | Mean = 4.396301 | Best = 4.891683 GA | iter = 1009 | Mean = 3.989186 | Best = 4.891683 GA | iter = 1010 | Mean = 3.890528 | Best = 4.891683 GA | iter = 1011 | Mean = 4.153682 | Best = 4.891683 GA | iter = 1012 | Mean = 4.162185 | Best = 4.891683 GA | iter = 1013 | Mean = 3.260853 | Best = 4.891683 GA | iter = 1014 | Mean = 4.052914 | Best = 4.891683 GA | iter = 1015 | Mean = 4.094734 | Best = 4.891683 GA | iter = 1016 | Mean = 3.875439 | Best = 4.891683 GA | iter = 1017 | Mean = 4.328006 | Best = 4.891683 GA | iter = 1018 | Mean = 4.000260 | Best = 4.891683 GA | iter = 1019 | Mean = 3.982106 | Best = 4.891683 GA | iter = 1020 | Mean = 4.205565 | Best = 4.891683 GA | iter = 1021 | Mean = 4.407303 | Best = 4.891683 GA | iter = 1022 | Mean = 3.874182 | Best = 4.891683 GA | iter = 1023 | Mean = 4.395579 | Best = 4.891683 GA | iter = 1024 | Mean = 4.432228 | Best = 4.891683 GA | iter = 1025 | Mean = 4.071964 | Best = 4.891683 GA | iter = 1026 | Mean = 3.741354 | Best = 4.891683 GA | iter = 1027 | Mean = 4.261116 | Best = 4.891683 GA | iter = 1028 | Mean = 4.366408 | Best = 4.891683 GA | iter = 1029 | Mean = 4.359388 | Best = 4.891683 GA | iter = 1030 | Mean = 4.511468 | Best = 4.891683 GA | iter = 1031 | Mean = 4.430675 | Best = 4.891683 GA | iter = 1032 | Mean = 4.327661 | Best = 4.891683 GA | iter = 1033 | Mean = 4.220194 | Best = 4.891683 GA | iter = 1034 | Mean = 4.470731 | Best = 4.891683 GA | iter = 1035 | Mean = 4.378574 | Best = 4.891683 GA | iter = 1036 | Mean = 4.387531 | Best = 4.891683 GA | iter = 1037 | Mean = 4.300961 | Best = 4.891683 GA | iter = 1038 | Mean = 4.145049 | Best = 4.891683 GA | iter = 1039 | Mean = 4.556015 | Best = 4.891683 GA | iter = 1040 | Mean = 4.481993 | Best = 4.891683 GA | iter = 1041 | Mean = 4.210875 | Best = 4.891683 GA | iter = 1042 | Mean = 4.029466 | Best = 4.891683 GA | iter = 1043 | Mean = 4.513402 | Best = 4.891683 GA | iter = 1044 | Mean = 4.402307 | Best = 4.891683 GA | iter = 1045 | Mean = 3.866141 | Best = 4.891683 GA | iter = 1046 | Mean = 3.457128 | Best = 4.891683 GA | iter = 1047 | Mean = 3.348601 | Best = 4.891683 GA | iter = 1048 | Mean = 3.607477 | Best = 4.891683 GA | iter = 1049 | Mean = 4.379462 | Best = 4.891683 GA | iter = 1050 | Mean = 4.103700 | Best = 4.891683 GA | iter = 1051 | Mean = 3.752645 | Best = 4.891683 GA | iter = 1052 | Mean = 3.840126 | Best = 4.891683 GA | iter = 1053 | Mean = 3.318117 | Best = 4.891683 GA | iter = 1054 | Mean = 4.076684 | Best = 4.891683 GA | iter = 1055 | Mean = 4.230970 | Best = 4.891683 GA | iter = 1056 | Mean = 4.188175 | Best = 4.891683 GA | iter = 1057 | Mean = 4.050553 | Best = 4.891683 GA | iter = 1058 | Mean = 4.101519 | Best = 4.891683 GA | iter = 1059 | Mean = 4.111449 | Best = 4.891683 GA | iter = 1060 | Mean = 4.247765 | Best = 4.891683 GA | iter = 1061 | Mean = 4.458323 | Best = 4.891683 GA | iter = 1062 | Mean = 4.218396 | Best = 4.891683 GA | iter = 1063 | Mean = 4.537320 | Best = 4.891683 GA | iter = 1064 | Mean = 4.303766 | Best = 4.891683 GA | iter = 1065 | Mean = 3.697121 | Best = 4.891683 GA | iter = 1066 | Mean = 3.795421 | Best = 4.891683 GA | iter = 1067 | Mean = 4.474893 | Best = 4.891683 GA | iter = 1068 | Mean = 4.440336 | Best = 4.891683 GA | iter = 1069 | Mean = 3.970334 | Best = 4.891683 GA | iter = 1070 | Mean = 4.436917 | Best = 4.891683 GA | iter = 1071 | Mean = 4.108579 | Best = 4.891683 GA | iter = 1072 | Mean = 4.313456 | Best = 4.891683 GA | iter = 1073 | Mean = 4.027632 | Best = 4.891683 GA | iter = 1074 | Mean = 3.851595 | Best = 4.891683 GA | iter = 1075 | Mean = 4.478419 | Best = 4.891683 GA | iter = 1076 | Mean = 4.107986 | Best = 4.891683 GA | iter = 1077 | Mean = 4.182986 | Best = 4.891683 GA | iter = 1078 | Mean = 4.519229 | Best = 4.891683 GA | iter = 1079 | Mean = 4.444425 | Best = 4.891683 GA | iter = 1080 | Mean = 4.617857 | Best = 4.891683 GA | iter = 1081 | Mean = 4.578483 | Best = 4.891683 GA | iter = 1082 | Mean = 4.501993 | Best = 4.891683 GA | iter = 1083 | Mean = 4.135278 | Best = 4.891683 GA | iter = 1084 | Mean = 4.170373 | Best = 4.891683 GA | iter = 1085 | Mean = 4.198790 | Best = 4.891683 GA | iter = 1086 | Mean = 3.655608 | Best = 4.891683 GA | iter = 1087 | Mean = 4.519199 | Best = 4.891683 GA | iter = 1088 | Mean = 4.579441 | Best = 4.891683 GA | iter = 1089 | Mean = 4.030477 | Best = 4.891683 GA | iter = 1090 | Mean = 3.943339 | Best = 4.891683 GA | iter = 1091 | Mean = 3.905693 | Best = 4.891683 GA | iter = 1092 | Mean = 4.480203 | Best = 4.891683 GA | iter = 1093 | Mean = 4.242172 | Best = 4.891683 GA | iter = 1094 | Mean = 4.281414 | Best = 4.891683 GA | iter = 1095 | Mean = 4.032607 | Best = 4.891683 GA | iter = 1096 | Mean = 4.133139 | Best = 4.891683 GA | iter = 1097 | Mean = 3.715492 | Best = 4.891683 GA | iter = 1098 | Mean = 4.615768 | Best = 4.891683 GA | iter = 1099 | Mean = 4.371857 | Best = 4.891683 GA | iter = 1100 | Mean = 4.335481 | Best = 4.891683 GA | iter = 1101 | Mean = 3.938259 | Best = 4.891683 GA | iter = 1102 | Mean = 3.984642 | Best = 4.891683 GA | iter = 1103 | Mean = 4.330298 | Best = 4.891683 GA | iter = 1104 | Mean = 4.382428 | Best = 4.891683 GA | iter = 1105 | Mean = 4.666774 | Best = 4.891683 GA | iter = 1106 | Mean = 4.240406 | Best = 4.891683 GA | iter = 1107 | Mean = 4.133838 | Best = 4.891683 GA | iter = 1108 | Mean = 4.242246 | Best = 4.891683 GA | iter = 1109 | Mean = 3.706746 | Best = 4.891683 GA | iter = 1110 | Mean = 3.781902 | Best = 4.891683 GA | iter = 1111 | Mean = 4.011648 | Best = 4.891683 GA | iter = 1112 | Mean = 4.448430 | Best = 4.891683 GA | iter = 1113 | Mean = 4.601025 | Best = 4.891683 GA | iter = 1114 | Mean = 4.210031 | Best = 4.891683 GA | iter = 1115 | Mean = 4.284329 | Best = 4.891683 GA | iter = 1116 | Mean = 3.996659 | Best = 4.891683 GA | iter = 1117 | Mean = 4.008239 | Best = 4.891683 GA | iter = 1118 | Mean = 4.002625 | Best = 4.891683 GA | iter = 1119 | Mean = 4.525006 | Best = 4.891683 GA | iter = 1120 | Mean = 4.397590 | Best = 4.891683 GA | iter = 1121 | Mean = 3.901968 | Best = 4.891683 GA | iter = 1122 | Mean = 4.378062 | Best = 4.891683 GA | iter = 1123 | Mean = 4.147586 | Best = 4.891683 GA | iter = 1124 | Mean = 3.929051 | Best = 4.891683 GA | iter = 1125 | Mean = 4.503645 | Best = 4.891683 GA | iter = 1126 | Mean = 4.567028 | Best = 4.891683 GA | iter = 1127 | Mean = 4.280258 | Best = 4.891683 GA | iter = 1128 | Mean = 3.586553 | Best = 4.891683 GA | iter = 1129 | Mean = 3.467060 | Best = 4.891683 GA | iter = 1130 | Mean = 4.204803 | Best = 4.891683 GA | iter = 1131 | Mean = 4.313996 | Best = 4.891683 GA | iter = 1132 | Mean = 4.359855 | Best = 4.891683 GA | iter = 1133 | Mean = 4.196622 | Best = 4.891683 GA | iter = 1134 | Mean = 4.160208 | Best = 4.891683 GA | iter = 1135 | Mean = 4.717765 | Best = 4.891683 GA | iter = 1136 | Mean = 4.698686 | Best = 4.891683 GA | iter = 1137 | Mean = 4.492926 | Best = 4.891683 GA | iter = 1138 | Mean = 4.513082 | Best = 4.891683 GA | iter = 1139 | Mean = 4.481582 | Best = 4.891683 GA | iter = 1140 | Mean = 4.589912 | Best = 4.891683 GA | iter = 1141 | Mean = 4.406880 | Best = 4.891683 GA | iter = 1142 | Mean = 4.201443 | Best = 4.891683 GA | iter = 1143 | Mean = 3.646122 | Best = 4.891683 GA | iter = 1144 | Mean = 3.904941 | Best = 4.891683 GA | iter = 1145 | Mean = 4.016208 | Best = 4.891683 GA | iter = 1146 | Mean = 3.266123 | Best = 4.891683 GA | iter = 1147 | Mean = 4.088960 | Best = 4.891683 GA | iter = 1148 | Mean = 3.989217 | Best = 4.891683 GA | iter = 1149 | Mean = 3.944338 | Best = 4.891683 GA | iter = 1150 | Mean = 4.139665 | Best = 4.891683 GA | iter = 1151 | Mean = 3.925745 | Best = 4.891683 GA | iter = 1152 | Mean = 4.393932 | Best = 4.891683 GA | iter = 1153 | Mean = 4.376911 | Best = 4.891683 GA | iter = 1154 | Mean = 3.863481 | Best = 4.891683 GA | iter = 1155 | Mean = 4.361920 | Best = 4.891683 GA | iter = 1156 | Mean = 4.035927 | Best = 4.891683 GA | iter = 1157 | Mean = 4.571284 | Best = 4.891683 GA | iter = 1158 | Mean = 4.386887 | Best = 4.891683 GA | iter = 1159 | Mean = 4.400467 | Best = 4.891683 GA | iter = 1160 | Mean = 3.893560 | Best = 4.891683 GA | iter = 1161 | Mean = 4.100818 | Best = 4.891683 GA | iter = 1162 | Mean = 4.344197 | Best = 4.891683 GA | iter = 1163 | Mean = 4.566801 | Best = 4.891683 GA | iter = 1164 | Mean = 4.211174 | Best = 4.891683 GA | iter = 1165 | Mean = 4.298737 | Best = 4.891683 GA | iter = 1166 | Mean = 4.077458 | Best = 4.891683 GA | iter = 1167 | Mean = 4.145145 | Best = 4.891683 GA | iter = 1168 | Mean = 3.648612 | Best = 4.891683 GA | iter = 1169 | Mean = 4.070171 | Best = 4.891683 GA | iter = 1170 | Mean = 3.992847 | Best = 4.891683 GA | iter = 1171 | Mean = 4.451762 | Best = 4.891683 GA | iter = 1172 | Mean = 4.006245 | Best = 4.891683 GA | iter = 1173 | Mean = 4.105219 | Best = 4.891683 GA | iter = 1174 | Mean = 4.242610 | Best = 4.891683 GA | iter = 1175 | Mean = 4.082385 | Best = 4.891683 GA | iter = 1176 | Mean = 4.428024 | Best = 4.891683 GA | iter = 1177 | Mean = 4.416281 | Best = 4.891683 GA | iter = 1178 | Mean = 4.375120 | Best = 4.891683 GA | iter = 1179 | Mean = 4.183640 | Best = 4.891683 GA | iter = 1180 | Mean = 4.462410 | Best = 4.891683 GA | iter = 1181 | Mean = 4.046158 | Best = 4.891683 GA | iter = 1182 | Mean = 4.731530 | Best = 4.891683 GA | iter = 1183 | Mean = 4.498737 | Best = 4.891683 GA | iter = 1184 | Mean = 4.484653 | Best = 4.891683 GA | iter = 1185 | Mean = 4.452803 | Best = 4.891683 GA | iter = 1186 | Mean = 4.340326 | Best = 4.891683 GA | iter = 1187 | Mean = 3.802974 | Best = 4.891683 GA | iter = 1188 | Mean = 4.333751 | Best = 4.891683 GA | iter = 1189 | Mean = 4.348365 | Best = 4.891683 GA | iter = 1190 | Mean = 4.394776 | Best = 4.891683 GA | iter = 1191 | Mean = 4.385235 | Best = 4.891683 GA | iter = 1192 | Mean = 4.383796 | Best = 4.891683 GA | iter = 1193 | Mean = 4.635035 | Best = 4.891683 GA | iter = 1194 | Mean = 3.858313 | Best = 4.891683 GA | iter = 1195 | Mean = 3.283589 | Best = 4.891683 GA | iter = 1196 | Mean = 3.596958 | Best = 4.891683 GA | iter = 1197 | Mean = 4.078766 | Best = 4.891683 GA | iter = 1198 | Mean = 4.386770 | Best = 4.891683 GA | iter = 1199 | Mean = 4.470755 | Best = 4.891683 GA | iter = 1200 | Mean = 4.166019 | Best = 4.891683 GA | iter = 1201 | Mean = 4.158643 | Best = 4.891683 GA | iter = 1202 | Mean = 4.208282 | Best = 4.891683 GA | iter = 1203 | Mean = 4.259279 | Best = 4.891683 GA | iter = 1204 | Mean = 4.561931 | Best = 4.891683 GA | iter = 1205 | Mean = 4.315585 | Best = 4.891683 GA | iter = 1206 | Mean = 3.673272 | Best = 4.891683 GA | iter = 1207 | Mean = 4.131135 | Best = 4.891683 GA | iter = 1208 | Mean = 4.055538 | Best = 4.891683 GA | iter = 1209 | Mean = 4.326850 | Best = 4.891683 GA | iter = 1210 | Mean = 4.194301 | Best = 4.891683 GA | iter = 1211 | Mean = 4.692919 | Best = 4.891683 GA | iter = 1212 | Mean = 4.528299 | Best = 4.891683 GA | iter = 1213 | Mean = 4.132555 | Best = 4.891683 GA | iter = 1214 | Mean = 4.169741 | Best = 4.891683 GA | iter = 1215 | Mean = 3.957805 | Best = 4.891683 GA | iter = 1216 | Mean = 4.089932 | Best = 4.891683 GA | iter = 1217 | Mean = 3.935211 | Best = 4.891683 GA | iter = 1218 | Mean = 4.381812 | Best = 4.891683 GA | iter = 1219 | Mean = 4.314113 | Best = 4.891683 GA | iter = 1220 | Mean = 4.042886 | Best = 4.891683 GA | iter = 1221 | Mean = 4.011308 | Best = 4.891683 GA | iter = 1222 | Mean = 4.436874 | Best = 4.891683 GA | iter = 1223 | Mean = 4.529241 | Best = 4.891683 GA | iter = 1224 | Mean = 4.266150 | Best = 4.891683 GA | iter = 1225 | Mean = 4.106523 | Best = 4.891683 GA | iter = 1226 | Mean = 4.162118 | Best = 4.891683 GA | iter = 1227 | Mean = 3.521590 | Best = 4.891683 GA | iter = 1228 | Mean = 4.139275 | Best = 4.891683 GA | iter = 1229 | Mean = 4.438330 | Best = 4.891683 GA | iter = 1230 | Mean = 4.380489 | Best = 4.891683 GA | iter = 1231 | Mean = 4.385313 | Best = 4.891683 GA | iter = 1232 | Mean = 4.355151 | Best = 4.891683 GA | iter = 1233 | Mean = 4.096228 | Best = 4.891683 GA | iter = 1234 | Mean = 4.196593 | Best = 4.891683 GA | iter = 1235 | Mean = 4.040963 | Best = 4.891683 GA | iter = 1236 | Mean = 4.279011 | Best = 4.891683 GA | iter = 1237 | Mean = 4.110824 | Best = 4.891683 GA | iter = 1238 | Mean = 4.308765 | Best = 4.891683 GA | iter = 1239 | Mean = 4.022648 | Best = 4.891683 GA | iter = 1240 | Mean = 4.206923 | Best = 4.891683 GA | iter = 1241 | Mean = 4.166013 | Best = 4.891683 GA | iter = 1242 | Mean = 3.947427 | Best = 4.891683 GA | iter = 1243 | Mean = 3.954326 | Best = 4.891683 GA | iter = 1244 | Mean = 3.480535 | Best = 4.891683 GA | iter = 1245 | Mean = 4.334071 | Best = 4.891683 GA | iter = 1246 | Mean = 4.507290 | Best = 4.891683 GA | iter = 1247 | Mean = 4.375452 | Best = 4.891683 GA | iter = 1248 | Mean = 4.669537 | Best = 4.891683 GA | iter = 1249 | Mean = 4.304762 | Best = 4.891683 GA | iter = 1250 | Mean = 3.907749 | Best = 4.891683 GA | iter = 1251 | Mean = 4.355401 | Best = 4.891683 GA | iter = 1252 | Mean = 4.032004 | Best = 4.891683 GA | iter = 1253 | Mean = 4.450080 | Best = 4.891683 GA | iter = 1254 | Mean = 3.966371 | Best = 4.891683 GA | iter = 1255 | Mean = 4.708044 | Best = 4.891683 GA | iter = 1256 | Mean = 4.172017 | Best = 4.891683 GA | iter = 1257 | Mean = 4.404754 | Best = 4.891683 GA | iter = 1258 | Mean = 4.494797 | Best = 4.891683 GA | iter = 1259 | Mean = 4.556139 | Best = 4.891683 GA | iter = 1260 | Mean = 4.295274 | Best = 4.891683 GA | iter = 1261 | Mean = 4.612236 | Best = 4.891683 GA | iter = 1262 | Mean = 4.657106 | Best = 4.891683 GA | iter = 1263 | Mean = 3.958456 | Best = 4.891683 GA | iter = 1264 | Mean = 4.580533 | Best = 4.891683 GA | iter = 1265 | Mean = 4.685486 | Best = 4.891683 GA | iter = 1266 | Mean = 4.519342 | Best = 4.891683 GA | iter = 1267 | Mean = 4.588547 | Best = 4.891683 GA | iter = 1268 | Mean = 3.997827 | Best = 4.891683 GA | iter = 1269 | Mean = 4.122793 | Best = 4.891683 GA | iter = 1270 | Mean = 3.976286 | Best = 4.891683 GA | iter = 1271 | Mean = 4.244786 | Best = 4.891683 GA | iter = 1272 | Mean = 4.182915 | Best = 4.891683 GA | iter = 1273 | Mean = 4.582926 | Best = 4.891683 GA | iter = 1274 | Mean = 4.513615 | Best = 4.891683 GA | iter = 1275 | Mean = 4.245837 | Best = 4.891683 GA | iter = 1276 | Mean = 4.539610 | Best = 4.891683 GA | iter = 1277 | Mean = 4.467999 | Best = 4.891683 GA | iter = 1278 | Mean = 4.317914 | Best = 4.891683 GA | iter = 1279 | Mean = 3.882074 | Best = 4.891683 GA | iter = 1280 | Mean = 4.204486 | Best = 4.891683 GA | iter = 1281 | Mean = 3.705170 | Best = 4.891683 GA | iter = 1282 | Mean = 4.072814 | Best = 4.891683 GA | iter = 1283 | Mean = 4.430283 | Best = 4.891683 GA | iter = 1284 | Mean = 4.078672 | Best = 4.891683 GA | iter = 1285 | Mean = 4.244014 | Best = 4.891683 GA | iter = 1286 | Mean = 4.092525 | Best = 4.891683 GA | iter = 1287 | Mean = 4.491747 | Best = 4.891683 GA | iter = 1288 | Mean = 4.354767 | Best = 4.891683 GA | iter = 1289 | Mean = 4.555025 | Best = 4.891683 GA | iter = 1290 | Mean = 3.923748 | Best = 4.891683 GA | iter = 1291 | Mean = 3.854761 | Best = 4.891683 GA | iter = 1292 | Mean = 3.757618 | Best = 4.891683 GA | iter = 1293 | Mean = 4.079745 | Best = 4.891683 GA | iter = 1294 | Mean = 4.083010 | Best = 4.891683 GA | iter = 1295 | Mean = 3.625666 | Best = 4.891683 GA | iter = 1296 | Mean = 4.170249 | Best = 4.891683 GA | iter = 1297 | Mean = 4.172542 | Best = 4.891683 GA | iter = 1298 | Mean = 4.503445 | Best = 4.891683 GA | iter = 1299 | Mean = 4.500082 | Best = 4.891683 GA | iter = 1300 | Mean = 4.556582 | Best = 4.891683 GA | iter = 1301 | Mean = 4.214361 | Best = 4.891683 GA | iter = 1302 | Mean = 4.362739 | Best = 4.891683 GA | iter = 1303 | Mean = 4.453482 | Best = 4.891683 GA | iter = 1304 | Mean = 4.292062 | Best = 4.891683 GA | iter = 1305 | Mean = 3.897812 | Best = 4.891683 GA | iter = 1306 | Mean = 4.225580 | Best = 4.891683 GA | iter = 1307 | Mean = 4.285826 | Best = 4.891683 GA | iter = 1308 | Mean = 4.360144 | Best = 4.891683 GA | iter = 1309 | Mean = 4.598198 | Best = 4.891683 GA | iter = 1310 | Mean = 4.442408 | Best = 4.891683 GA | iter = 1311 | Mean = 3.638263 | Best = 4.891683 GA | iter = 1312 | Mean = 4.173207 | Best = 4.891683 GA | iter = 1313 | Mean = 3.996086 | Best = 4.891683 GA | iter = 1314 | Mean = 3.861576 | Best = 4.891683 GA | iter = 1315 | Mean = 3.701101 | Best = 4.891683 GA | iter = 1316 | Mean = 4.327011 | Best = 4.891683 GA | iter = 1317 | Mean = 4.392250 | Best = 4.891683 GA | iter = 1318 | Mean = 3.887543 | Best = 4.891683 GA | iter = 1319 | Mean = 4.009571 | Best = 4.891683 GA | iter = 1320 | Mean = 4.093564 | Best = 4.891683 GA | iter = 1321 | Mean = 3.958828 | Best = 4.891683 GA | iter = 1322 | Mean = 4.089840 | Best = 4.891683 GA | iter = 1323 | Mean = 3.976804 | Best = 4.891683 GA | iter = 1324 | Mean = 4.268614 | Best = 4.891683 GA | iter = 1325 | Mean = 4.488663 | Best = 4.891683 GA | iter = 1326 | Mean = 4.218092 | Best = 4.891683 GA | iter = 1327 | Mean = 3.985426 | Best = 4.891683 GA | iter = 1328 | Mean = 4.208417 | Best = 4.891683 GA | iter = 1329 | Mean = 4.071059 | Best = 4.891683 GA | iter = 1330 | Mean = 4.143379 | Best = 4.891683 GA | iter = 1331 | Mean = 3.885245 | Best = 4.891683 GA | iter = 1332 | Mean = 4.222823 | Best = 4.891683 GA | iter = 1333 | Mean = 3.691512 | Best = 4.891683 GA | iter = 1334 | Mean = 3.889535 | Best = 4.891683 GA | iter = 1335 | Mean = 4.175774 | Best = 4.891683 GA | iter = 1336 | Mean = 4.168379 | Best = 4.891683 GA | iter = 1337 | Mean = 4.057140 | Best = 4.891683 GA | iter = 1338 | Mean = 3.853045 | Best = 4.891683 GA | iter = 1339 | Mean = 4.202340 | Best = 4.891683 GA | iter = 1340 | Mean = 3.891280 | Best = 4.891683 GA | iter = 1341 | Mean = 3.852002 | Best = 4.891683 GA | iter = 1342 | Mean = 4.395964 | Best = 4.891683 GA | iter = 1343 | Mean = 4.229647 | Best = 4.891683 GA | iter = 1344 | Mean = 4.382317 | Best = 4.891683 GA | iter = 1345 | Mean = 4.366763 | Best = 4.891683 GA | iter = 1346 | Mean = 4.374437 | Best = 4.891683 GA | iter = 1347 | Mean = 4.081016 | Best = 4.891683 GA | iter = 1348 | Mean = 4.063059 | Best = 4.891683 GA | iter = 1349 | Mean = 4.002146 | Best = 4.891683 GA | iter = 1350 | Mean = 3.692933 | Best = 4.891683 GA | iter = 1351 | Mean = 4.171380 | Best = 4.891683 GA | iter = 1352 | Mean = 3.989610 | Best = 4.891683 GA | iter = 1353 | Mean = 3.784962 | Best = 4.891683 GA | iter = 1354 | Mean = 4.264269 | Best = 4.891683 GA | iter = 1355 | Mean = 4.404347 | Best = 4.891683 GA | iter = 1356 | Mean = 4.542594 | Best = 4.891683 GA | iter = 1357 | Mean = 4.548267 | Best = 4.891683 GA | iter = 1358 | Mean = 4.210439 | Best = 4.891683 GA | iter = 1359 | Mean = 4.088392 | Best = 4.891683 GA | iter = 1360 | Mean = 4.251495 | Best = 4.891683 GA | iter = 1361 | Mean = 4.321445 | Best = 4.891683 GA | iter = 1362 | Mean = 4.584403 | Best = 4.891683 GA | iter = 1363 | Mean = 4.564774 | Best = 4.891683 GA | iter = 1364 | Mean = 4.458369 | Best = 4.891683 GA | iter = 1365 | Mean = 4.417567 | Best = 4.891683 GA | iter = 1366 | Mean = 4.169427 | Best = 4.891683 GA | iter = 1367 | Mean = 4.422087 | Best = 4.891683 GA | iter = 1368 | Mean = 4.157835 | Best = 4.891683 GA | iter = 1369 | Mean = 4.002181 | Best = 4.891683 GA | iter = 1370 | Mean = 3.928940 | Best = 4.891683 GA | iter = 1371 | Mean = 3.525351 | Best = 4.891683 GA | iter = 1372 | Mean = 3.694511 | Best = 4.891683 GA | iter = 1373 | Mean = 4.015186 | Best = 4.891683 GA | iter = 1374 | Mean = 4.488672 | Best = 4.891683 GA | iter = 1375 | Mean = 4.171282 | Best = 4.891683 GA | iter = 1376 | Mean = 4.531375 | Best = 4.891683 GA | iter = 1377 | Mean = 4.006005 | Best = 4.891683 GA | iter = 1378 | Mean = 4.424860 | Best = 4.891683 GA | iter = 1379 | Mean = 4.477059 | Best = 4.891683 GA | iter = 1380 | Mean = 4.016259 | Best = 4.891683 GA | iter = 1381 | Mean = 4.514081 | Best = 4.891683 GA | iter = 1382 | Mean = 4.476393 | Best = 4.891683 GA | iter = 1383 | Mean = 4.547902 | Best = 4.891683 GA | iter = 1384 | Mean = 4.047469 | Best = 4.891683 GA | iter = 1385 | Mean = 4.429634 | Best = 4.891683 GA | iter = 1386 | Mean = 4.558462 | Best = 4.891683 GA | iter = 1387 | Mean = 4.418968 | Best = 4.891683 GA | iter = 1388 | Mean = 4.459229 | Best = 4.891683 GA | iter = 1389 | Mean = 4.656021 | Best = 4.891683 GA | iter = 1390 | Mean = 4.462618 | Best = 4.891683 GA | iter = 1391 | Mean = 4.602401 | Best = 4.891683 GA | iter = 1392 | Mean = 4.470416 | Best = 4.891683 GA | iter = 1393 | Mean = 4.485792 | Best = 4.891683 GA | iter = 1394 | Mean = 4.218093 | Best = 4.891683 GA | iter = 1395 | Mean = 4.211984 | Best = 4.891683 GA | iter = 1396 | Mean = 3.566605 | Best = 4.891683 GA | iter = 1397 | Mean = 4.186773 | Best = 4.891683 GA | iter = 1398 | Mean = 4.314786 | Best = 4.891683 GA | iter = 1399 | Mean = 4.044117 | Best = 4.891683 GA | iter = 1400 | Mean = 3.849445 | Best = 4.891683 GA | iter = 1401 | Mean = 3.002480 | Best = 4.891683 GA | iter = 1402 | Mean = 3.980713 | Best = 4.891683 GA | iter = 1403 | Mean = 3.980782 | Best = 4.891683 GA | iter = 1404 | Mean = 4.349621 | Best = 4.891683 GA | iter = 1405 | Mean = 4.139746 | Best = 4.891683 GA | iter = 1406 | Mean = 4.115769 | Best = 4.891683 GA | iter = 1407 | Mean = 3.701274 | Best = 4.891683 GA | iter = 1408 | Mean = 4.030499 | Best = 4.891683 GA | iter = 1409 | Mean = 4.369470 | Best = 4.891683 GA | iter = 1410 | Mean = 4.574670 | Best = 4.891683 GA | iter = 1411 | Mean = 4.155002 | Best = 4.891683 GA | iter = 1412 | Mean = 4.053728 | Best = 4.891683 GA | iter = 1413 | Mean = 3.795074 | Best = 4.891683 GA | iter = 1414 | Mean = 3.742485 | Best = 4.891683 GA | iter = 1415 | Mean = 4.407380 | Best = 4.891683 GA | iter = 1416 | Mean = 4.457514 | Best = 4.891683 GA | iter = 1417 | Mean = 3.750224 | Best = 4.891683 GA | iter = 1418 | Mean = 3.759218 | Best = 4.891683 GA | iter = 1419 | Mean = 3.962802 | Best = 4.891683 GA | iter = 1420 | Mean = 4.142114 | Best = 4.891683 GA | iter = 1421 | Mean = 3.905271 | Best = 4.891683 GA | iter = 1422 | Mean = 3.758481 | Best = 4.891683 GA | iter = 1423 | Mean = 4.700555 | Best = 4.891683 GA | iter = 1424 | Mean = 3.963895 | Best = 4.891683 GA | iter = 1425 | Mean = 4.234311 | Best = 4.891683 GA | iter = 1426 | Mean = 3.738568 | Best = 4.891683 GA | iter = 1427 | Mean = 4.145671 | Best = 4.891683 GA | iter = 1428 | Mean = 4.362553 | Best = 4.891683 GA | iter = 1429 | Mean = 3.384683 | Best = 4.891683 GA | iter = 1430 | Mean = 4.225161 | Best = 4.891683 GA | iter = 1431 | Mean = 3.646078 | Best = 4.891683 GA | iter = 1432 | Mean = 3.853739 | Best = 4.891683 GA | iter = 1433 | Mean = 4.286118 | Best = 4.891683 GA | iter = 1434 | Mean = 4.312533 | Best = 4.891683 GA | iter = 1435 | Mean = 4.545674 | Best = 4.891683 GA | iter = 1436 | Mean = 4.440547 | Best = 4.891683 GA | iter = 1437 | Mean = 4.403303 | Best = 4.891683 GA | iter = 1438 | Mean = 4.457585 | Best = 4.891683 GA | iter = 1439 | Mean = 4.438111 | Best = 4.891683 GA | iter = 1440 | Mean = 3.828150 | Best = 4.891683 GA | iter = 1441 | Mean = 4.231770 | Best = 4.891683 GA | iter = 1442 | Mean = 4.397778 | Best = 4.891683 GA | iter = 1443 | Mean = 4.057683 | Best = 4.891683 GA | iter = 1444 | Mean = 4.312984 | Best = 4.891683 GA | iter = 1445 | Mean = 4.383475 | Best = 4.891683 GA | iter = 1446 | Mean = 4.291019 | Best = 4.891683 GA | iter = 1447 | Mean = 4.349401 | Best = 4.891683 GA | iter = 1448 | Mean = 4.416776 | Best = 4.891683 GA | iter = 1449 | Mean = 4.144008 | Best = 4.891683 GA | iter = 1450 | Mean = 4.101719 | Best = 4.891683 GA | iter = 1451 | Mean = 4.316993 | Best = 4.891683 GA | iter = 1452 | Mean = 4.231625 | Best = 4.891683 GA | iter = 1453 | Mean = 3.755019 | Best = 4.891683 GA | iter = 1454 | Mean = 3.754973 | Best = 4.891683 GA | iter = 1455 | Mean = 4.184191 | Best = 4.891683 GA | iter = 1456 | Mean = 4.256525 | Best = 4.891683 GA | iter = 1457 | Mean = 4.090521 | Best = 4.891683 GA | iter = 1458 | Mean = 3.801864 | Best = 4.891683 GA | iter = 1459 | Mean = 3.339914 | Best = 4.891683 GA | iter = 1460 | Mean = 3.645526 | Best = 4.891683 GA | iter = 1461 | Mean = 3.879602 | Best = 4.891683 GA | iter = 1462 | Mean = 4.023425 | Best = 4.891683 GA | iter = 1463 | Mean = 4.231291 | Best = 4.891683 GA | iter = 1464 | Mean = 4.295644 | Best = 4.891683 GA | iter = 1465 | Mean = 4.260855 | Best = 4.891683 GA | iter = 1466 | Mean = 4.139480 | Best = 4.891683 GA | iter = 1467 | Mean = 4.099891 | Best = 4.891683 GA | iter = 1468 | Mean = 4.536961 | Best = 4.891683 GA | iter = 1469 | Mean = 3.836163 | Best = 4.891683 GA | iter = 1470 | Mean = 4.212335 | Best = 4.891683 GA | iter = 1471 | Mean = 3.932532 | Best = 4.891683 GA | iter = 1472 | Mean = 4.387060 | Best = 4.891683 GA | iter = 1473 | Mean = 4.433652 | Best = 4.891683 GA | iter = 1474 | Mean = 4.345747 | Best = 4.891683 GA | iter = 1475 | Mean = 3.881425 | Best = 4.891683 GA | iter = 1476 | Mean = 3.768292 | Best = 4.891683 GA | iter = 1477 | Mean = 4.393859 | Best = 4.891683 GA | iter = 1478 | Mean = 4.762754 | Best = 4.891683 GA | iter = 1479 | Mean = 4.257348 | Best = 4.891683 GA | iter = 1480 | Mean = 4.348202 | Best = 4.891683 GA | iter = 1481 | Mean = 4.039344 | Best = 4.891683 GA | iter = 1482 | Mean = 4.058502 | Best = 4.891683 GA | iter = 1483 | Mean = 4.469129 | Best = 4.891683 GA | iter = 1484 | Mean = 4.189116 | Best = 4.891683 GA | iter = 1485 | Mean = 4.075004 | Best = 4.891683 GA | iter = 1486 | Mean = 4.139162 | Best = 4.891683 GA | iter = 1487 | Mean = 4.008326 | Best = 4.891683 GA | iter = 1488 | Mean = 3.619841 | Best = 4.891683 GA | iter = 1489 | Mean = 3.813747 | Best = 4.891683 GA | iter = 1490 | Mean = 3.274882 | Best = 4.891683 GA | iter = 1491 | Mean = 4.251763 | Best = 4.891683 GA | iter = 1492 | Mean = 3.930485 | Best = 4.891683 GA | iter = 1493 | Mean = 4.117379 | Best = 4.891683 GA | iter = 1494 | Mean = 4.056141 | Best = 4.891683 GA | iter = 1495 | Mean = 4.335540 | Best = 4.891683 GA | iter = 1496 | Mean = 4.215896 | Best = 4.891683 GA | iter = 1497 | Mean = 3.920413 | Best = 4.891683 GA | iter = 1498 | Mean = 4.251250 | Best = 4.891683 GA | iter = 1499 | Mean = 4.064473 | Best = 4.891683 GA | iter = 1500 | Mean = 4.528263 | Best = 4.891683 GA | iter = 1501 | Mean = 4.279336 | Best = 4.891683 GA | iter = 1502 | Mean = 4.358436 | Best = 4.891683 GA | iter = 1503 | Mean = 4.417571 | Best = 4.891683 GA | iter = 1504 | Mean = 4.489498 | Best = 4.891683 GA | iter = 1505 | Mean = 4.183842 | Best = 4.891683 GA | iter = 1506 | Mean = 4.142557 | Best = 4.891683 GA | iter = 1507 | Mean = 4.334178 | Best = 4.891683 GA | iter = 1508 | Mean = 4.521068 | Best = 4.891683 GA | iter = 1509 | Mean = 4.586978 | Best = 4.891683 GA | iter = 1510 | Mean = 4.424066 | Best = 4.891683 GA | iter = 1511 | Mean = 4.374447 | Best = 4.891683 GA | iter = 1512 | Mean = 4.467536 | Best = 4.891683 GA | iter = 1513 | Mean = 4.264218 | Best = 4.891683 GA | iter = 1514 | Mean = 4.516806 | Best = 4.891683 GA | iter = 1515 | Mean = 4.062901 | Best = 4.891683 GA | iter = 1516 | Mean = 3.962656 | Best = 4.891683 GA | iter = 1517 | Mean = 3.747323 | Best = 4.891683 GA | iter = 1518 | Mean = 4.226876 | Best = 4.891683 GA | iter = 1519 | Mean = 4.375494 | Best = 4.891683 GA | iter = 1520 | Mean = 4.358241 | Best = 4.891683 GA | iter = 1521 | Mean = 4.513907 | Best = 4.891683 GA | iter = 1522 | Mean = 4.076256 | Best = 4.891683 GA | iter = 1523 | Mean = 4.208406 | Best = 4.891683 GA | iter = 1524 | Mean = 3.926363 | Best = 4.891683 GA | iter = 1525 | Mean = 4.297999 | Best = 4.891683 GA | iter = 1526 | Mean = 4.485035 | Best = 4.891683 GA | iter = 1527 | Mean = 4.215381 | Best = 4.891683 GA | iter = 1528 | Mean = 4.557915 | Best = 4.891683 GA | iter = 1529 | Mean = 4.282777 | Best = 4.891683 GA | iter = 1530 | Mean = 4.496313 | Best = 4.891683 GA | iter = 1531 | Mean = 4.455389 | Best = 4.891683 GA | iter = 1532 | Mean = 4.660660 | Best = 4.891683 GA | iter = 1533 | Mean = 4.595340 | Best = 4.891683 GA | iter = 1534 | Mean = 4.535482 | Best = 4.891683 GA | iter = 1535 | Mean = 4.157970 | Best = 4.891683 GA | iter = 1536 | Mean = 4.567970 | Best = 4.891683 GA | iter = 1537 | Mean = 4.335399 | Best = 4.891683 GA | iter = 1538 | Mean = 4.220227 | Best = 4.891683 GA | iter = 1539 | Mean = 4.466357 | Best = 4.891683 GA | iter = 1540 | Mean = 4.039214 | Best = 4.891683 GA | iter = 1541 | Mean = 4.233860 | Best = 4.891683 GA | iter = 1542 | Mean = 4.528542 | Best = 4.891683 GA | iter = 1543 | Mean = 4.534204 | Best = 4.891683 GA | iter = 1544 | Mean = 4.604066 | Best = 4.891683 GA | iter = 1545 | Mean = 4.182686 | Best = 4.891683 GA | iter = 1546 | Mean = 4.241404 | Best = 4.891683 GA | iter = 1547 | Mean = 4.345283 | Best = 4.891683 GA | iter = 1548 | Mean = 4.239395 | Best = 4.891683 GA | iter = 1549 | Mean = 4.026515 | Best = 4.891683 GA | iter = 1550 | Mean = 3.817380 | Best = 4.891683 GA | iter = 1551 | Mean = 4.243753 | Best = 4.891683 GA | iter = 1552 | Mean = 4.035901 | Best = 4.891683 GA | iter = 1553 | Mean = 4.342748 | Best = 4.891683 GA | iter = 1554 | Mean = 4.075389 | Best = 4.891683 GA | iter = 1555 | Mean = 4.346221 | Best = 4.891683 GA | iter = 1556 | Mean = 4.384786 | Best = 4.891683 GA | iter = 1557 | Mean = 4.088377 | Best = 4.891683 GA | iter = 1558 | Mean = 4.321743 | Best = 4.891683 GA | iter = 1559 | Mean = 4.494326 | Best = 4.891683 GA | iter = 1560 | Mean = 4.288560 | Best = 4.891683 GA | iter = 1561 | Mean = 3.884854 | Best = 4.891683 GA | iter = 1562 | Mean = 4.464617 | Best = 4.891683 GA | iter = 1563 | Mean = 4.539774 | Best = 4.891683 GA | iter = 1564 | Mean = 4.366209 | Best = 4.891683 GA | iter = 1565 | Mean = 4.315974 | Best = 4.891683 GA | iter = 1566 | Mean = 4.749668 | Best = 4.891683 GA | iter = 1567 | Mean = 4.419675 | Best = 4.891683 GA | iter = 1568 | Mean = 4.516112 | Best = 4.891683 GA | iter = 1569 | Mean = 4.195652 | Best = 4.891683 GA | iter = 1570 | Mean = 4.077138 | Best = 4.891683 GA | iter = 1571 | Mean = 3.853058 | Best = 4.891683 GA | iter = 1572 | Mean = 4.190242 | Best = 4.891683 GA | iter = 1573 | Mean = 4.465978 | Best = 4.891683 GA | iter = 1574 | Mean = 4.283978 | Best = 4.891683 GA | iter = 1575 | Mean = 3.541054 | Best = 4.891683 GA | iter = 1576 | Mean = 4.005085 | Best = 4.891683 GA | iter = 1577 | Mean = 4.252641 | Best = 4.891683 GA | iter = 1578 | Mean = 4.372607 | Best = 4.891683 GA | iter = 1579 | Mean = 4.027988 | Best = 4.891683 GA | iter = 1580 | Mean = 3.843510 | Best = 4.891683 GA | iter = 1581 | Mean = 4.625983 | Best = 4.891683 GA | iter = 1582 | Mean = 4.338970 | Best = 4.891683 GA | iter = 1583 | Mean = 4.622075 | Best = 4.891683 GA | iter = 1584 | Mean = 4.311298 | Best = 4.891683 GA | iter = 1585 | Mean = 4.362710 | Best = 4.891683 GA | iter = 1586 | Mean = 4.046923 | Best = 4.891683 GA | iter = 1587 | Mean = 4.139238 | Best = 4.891683 GA | iter = 1588 | Mean = 3.594933 | Best = 4.891683 GA | iter = 1589 | Mean = 3.479930 | Best = 4.891683 GA | iter = 1590 | Mean = 4.000755 | Best = 4.891683 GA | iter = 1591 | Mean = 4.059657 | Best = 4.891683 GA | iter = 1592 | Mean = 4.263446 | Best = 4.891683 GA | iter = 1593 | Mean = 4.415883 | Best = 4.891683 GA | iter = 1594 | Mean = 4.231574 | Best = 4.891683 GA | iter = 1595 | Mean = 3.737230 | Best = 4.891683 GA | iter = 1596 | Mean = 3.877014 | Best = 4.891683 GA | iter = 1597 | Mean = 4.229874 | Best = 4.891683 GA | iter = 1598 | Mean = 4.584193 | Best = 4.891683 GA | iter = 1599 | Mean = 4.675102 | Best = 4.891683 GA | iter = 1600 | Mean = 4.361916 | Best = 4.891683 GA | iter = 1601 | Mean = 4.251015 | Best = 4.891683 GA | iter = 1602 | Mean = 4.110241 | Best = 4.891683 GA | iter = 1603 | Mean = 4.602350 | Best = 4.891683 GA | iter = 1604 | Mean = 4.428263 | Best = 4.891683 GA | iter = 1605 | Mean = 3.783526 | Best = 4.891683 GA | iter = 1606 | Mean = 3.831136 | Best = 4.891683 GA | iter = 1607 | Mean = 4.205602 | Best = 4.891683 GA | iter = 1608 | Mean = 4.273288 | Best = 4.891683 GA | iter = 1609 | Mean = 4.553156 | Best = 4.891683 GA | iter = 1610 | Mean = 4.372140 | Best = 4.891683 GA | iter = 1611 | Mean = 4.351357 | Best = 4.891683 GA | iter = 1612 | Mean = 3.354622 | Best = 4.891683 GA | iter = 1613 | Mean = 3.608534 | Best = 4.891683 GA | iter = 1614 | Mean = 3.771271 | Best = 4.891683 GA | iter = 1615 | Mean = 4.208901 | Best = 4.891683 GA | iter = 1616 | Mean = 3.408314 | Best = 4.891683 GA | iter = 1617 | Mean = 4.243673 | Best = 4.891683 GA | iter = 1618 | Mean = 3.805100 | Best = 4.891683 GA | iter = 1619 | Mean = 3.692885 | Best = 4.891683 GA | iter = 1620 | Mean = 3.489564 | Best = 4.891683 GA | iter = 1621 | Mean = 3.449288 | Best = 4.891683 GA | iter = 1622 | Mean = 3.847235 | Best = 4.891683 GA | iter = 1623 | Mean = 3.686626 | Best = 4.891683 GA | iter = 1624 | Mean = 4.475514 | Best = 4.891683 GA | iter = 1625 | Mean = 4.592552 | Best = 4.891683 GA | iter = 1626 | Mean = 4.675353 | Best = 4.891683 GA | iter = 1627 | Mean = 4.305952 | Best = 4.891683 GA | iter = 1628 | Mean = 4.320050 | Best = 4.891683 GA | iter = 1629 | Mean = 3.643684 | Best = 4.891683 GA | iter = 1630 | Mean = 4.133736 | Best = 4.891683 GA | iter = 1631 | Mean = 4.460489 | Best = 4.891683 GA | iter = 1632 | Mean = 4.377636 | Best = 4.891683 GA | iter = 1633 | Mean = 4.222499 | Best = 4.891683 GA | iter = 1634 | Mean = 4.633266 | Best = 4.891683 GA | iter = 1635 | Mean = 4.516036 | Best = 4.891683 GA | iter = 1636 | Mean = 3.786922 | Best = 4.891683 GA | iter = 1637 | Mean = 4.395885 | Best = 4.891683 GA | iter = 1638 | Mean = 3.931922 | Best = 4.891683 GA | iter = 1639 | Mean = 4.104381 | Best = 4.891683 GA | iter = 1640 | Mean = 4.233741 | Best = 4.891683 GA | iter = 1641 | Mean = 4.512597 | Best = 4.891683 GA | iter = 1642 | Mean = 3.981267 | Best = 4.891683 GA | iter = 1643 | Mean = 4.095434 | Best = 4.891683 GA | iter = 1644 | Mean = 4.106254 | Best = 4.891683 GA | iter = 1645 | Mean = 3.989586 | Best = 4.891683 GA | iter = 1646 | Mean = 4.318257 | Best = 4.891683 GA | iter = 1647 | Mean = 4.472289 | Best = 4.891683 GA | iter = 1648 | Mean = 3.730155 | Best = 4.891683 GA | iter = 1649 | Mean = 4.202096 | Best = 4.891683 GA | iter = 1650 | Mean = 4.299669 | Best = 4.891683 GA | iter = 1651 | Mean = 4.360572 | Best = 4.891683 GA | iter = 1652 | Mean = 4.554317 | Best = 4.891683 GA | iter = 1653 | Mean = 4.515912 | Best = 4.891683 GA | iter = 1654 | Mean = 4.234964 | Best = 4.891683 GA | iter = 1655 | Mean = 3.854753 | Best = 4.891683 GA | iter = 1656 | Mean = 4.135076 | Best = 4.891683 GA | iter = 1657 | Mean = 4.249335 | Best = 4.891683 GA | iter = 1658 | Mean = 4.530027 | Best = 4.891683 GA | iter = 1659 | Mean = 4.101750 | Best = 4.891683 GA | iter = 1660 | Mean = 4.278354 | Best = 4.891683 GA | iter = 1661 | Mean = 4.558980 | Best = 4.891683 GA | iter = 1662 | Mean = 4.313346 | Best = 4.891683 GA | iter = 1663 | Mean = 4.690783 | Best = 4.891683 GA | iter = 1664 | Mean = 4.358762 | Best = 4.891683 GA | iter = 1665 | Mean = 4.537392 | Best = 4.891683 GA | iter = 1666 | Mean = 4.456238 | Best = 4.891683 GA | iter = 1667 | Mean = 4.274824 | Best = 4.891683 GA | iter = 1668 | Mean = 4.378025 | Best = 4.891683 GA | iter = 1669 | Mean = 4.438496 | Best = 4.891683 GA | iter = 1670 | Mean = 4.278105 | Best = 4.891683 GA | iter = 1671 | Mean = 3.947189 | Best = 4.891683 GA | iter = 1672 | Mean = 4.355009 | Best = 4.891683 GA | iter = 1673 | Mean = 4.211186 | Best = 4.891683 GA | iter = 1674 | Mean = 4.520926 | Best = 4.891683 GA | iter = 1675 | Mean = 4.322341 | Best = 4.891683 GA | iter = 1676 | Mean = 4.081662 | Best = 4.891683 GA | iter = 1677 | Mean = 3.876425 | Best = 4.891683 GA | iter = 1678 | Mean = 4.216195 | Best = 4.891683 GA | iter = 1679 | Mean = 4.403601 | Best = 4.891683 GA | iter = 1680 | Mean = 4.288048 | Best = 4.891683 GA | iter = 1681 | Mean = 4.309799 | Best = 4.891683 GA | iter = 1682 | Mean = 4.413476 | Best = 4.891683 GA | iter = 1683 | Mean = 4.278461 | Best = 4.891683 GA | iter = 1684 | Mean = 4.265423 | Best = 4.891683 GA | iter = 1685 | Mean = 3.722627 | Best = 4.891683 GA | iter = 1686 | Mean = 4.067948 | Best = 4.891683 GA | iter = 1687 | Mean = 3.450126 | Best = 4.891683 GA | iter = 1688 | Mean = 3.723592 | Best = 4.891683 GA | iter = 1689 | Mean = 4.180881 | Best = 4.891683 GA | iter = 1690 | Mean = 4.463732 | Best = 4.891683 GA | iter = 1691 | Mean = 4.390042 | Best = 4.891683 GA | iter = 1692 | Mean = 4.474539 | Best = 4.891683 GA | iter = 1693 | Mean = 4.093774 | Best = 4.891683 GA | iter = 1694 | Mean = 4.379941 | Best = 4.891683 GA | iter = 1695 | Mean = 4.554161 | Best = 4.891683 GA | iter = 1696 | Mean = 4.354900 | Best = 4.891683 GA | iter = 1697 | Mean = 4.273190 | Best = 4.891683 GA | iter = 1698 | Mean = 4.199433 | Best = 4.891683 GA | iter = 1699 | Mean = 3.808950 | Best = 4.891683 GA | iter = 1700 | Mean = 3.598307 | Best = 4.891683 GA | iter = 1701 | Mean = 4.166428 | Best = 4.891683 GA | iter = 1702 | Mean = 4.651080 | Best = 4.891683 GA | iter = 1703 | Mean = 4.546241 | Best = 4.891683 GA | iter = 1704 | Mean = 4.609350 | Best = 4.891683 GA | iter = 1705 | Mean = 4.582923 | Best = 4.891683 GA | iter = 1706 | Mean = 4.248950 | Best = 4.891683 GA | iter = 1707 | Mean = 4.533750 | Best = 4.891683 GA | iter = 1708 | Mean = 4.050347 | Best = 4.891683 GA | iter = 1709 | Mean = 4.465752 | Best = 4.891683 GA | iter = 1710 | Mean = 4.427176 | Best = 4.891683 GA | iter = 1711 | Mean = 4.250982 | Best = 4.891683 GA | iter = 1712 | Mean = 4.444362 | Best = 4.891683 GA | iter = 1713 | Mean = 4.416931 | Best = 4.891683 GA | iter = 1714 | Mean = 4.562362 | Best = 4.891683 GA | iter = 1715 | Mean = 3.719304 | Best = 4.891683 GA | iter = 1716 | Mean = 4.346433 | Best = 4.891683 GA | iter = 1717 | Mean = 4.234451 | Best = 4.891683 GA | iter = 1718 | Mean = 4.372963 | Best = 4.891683 GA | iter = 1719 | Mean = 4.200312 | Best = 4.891683 GA | iter = 1720 | Mean = 4.300360 | Best = 4.891683 GA | iter = 1721 | Mean = 3.919607 | Best = 4.891683 GA | iter = 1722 | Mean = 3.672919 | Best = 4.891683 GA | iter = 1723 | Mean = 3.995666 | Best = 4.891683 GA | iter = 1724 | Mean = 4.246915 | Best = 4.891683 GA | iter = 1725 | Mean = 3.938446 | Best = 4.891683 GA | iter = 1726 | Mean = 4.411908 | Best = 4.891683 GA | iter = 1727 | Mean = 4.505544 | Best = 4.891683 GA | iter = 1728 | Mean = 4.520454 | Best = 4.891683 GA | iter = 1729 | Mean = 3.993903 | Best = 4.891683 GA | iter = 1730 | Mean = 3.848119 | Best = 4.891683 GA | iter = 1731 | Mean = 3.991560 | Best = 4.891683 GA | iter = 1732 | Mean = 4.266617 | Best = 4.891683 GA | iter = 1733 | Mean = 4.560139 | Best = 4.891683 GA | iter = 1734 | Mean = 4.366475 | Best = 4.891683 GA | iter = 1735 | Mean = 4.399313 | Best = 4.891683 GA | iter = 1736 | Mean = 4.452085 | Best = 4.891683 GA | iter = 1737 | Mean = 4.642980 | Best = 4.891683 GA | iter = 1738 | Mean = 4.154882 | Best = 4.891683 GA | iter = 1739 | Mean = 4.450583 | Best = 4.891683 GA | iter = 1740 | Mean = 4.407576 | Best = 4.891683 GA | iter = 1741 | Mean = 4.208429 | Best = 4.891683 GA | iter = 1742 | Mean = 4.054785 | Best = 4.891683 GA | iter = 1743 | Mean = 4.458016 | Best = 4.891683 GA | iter = 1744 | Mean = 4.493037 | Best = 4.891683 GA | iter = 1745 | Mean = 4.663103 | Best = 4.891683 GA | iter = 1746 | Mean = 3.739345 | Best = 4.891683 GA | iter = 1747 | Mean = 4.425198 | Best = 4.891683 GA | iter = 1748 | Mean = 4.438803 | Best = 4.891683 GA | iter = 1749 | Mean = 4.287183 | Best = 4.891683 GA | iter = 1750 | Mean = 4.360233 | Best = 4.891683 GA | iter = 1751 | Mean = 4.112163 | Best = 4.891683 GA | iter = 1752 | Mean = 4.408124 | Best = 4.891683 GA | iter = 1753 | Mean = 4.143776 | Best = 4.891683 GA | iter = 1754 | Mean = 4.641441 | Best = 4.891683 GA | iter = 1755 | Mean = 4.404269 | Best = 4.891683 GA | iter = 1756 | Mean = 4.371114 | Best = 4.891683 GA | iter = 1757 | Mean = 4.517705 | Best = 4.891683 GA | iter = 1758 | Mean = 4.203115 | Best = 4.891683 GA | iter = 1759 | Mean = 4.111542 | Best = 4.891683 GA | iter = 1760 | Mean = 3.105985 | Best = 4.891683 GA | iter = 1761 | Mean = 3.840082 | Best = 4.891683 GA | iter = 1762 | Mean = 3.927434 | Best = 4.891683 GA | iter = 1763 | Mean = 4.249485 | Best = 4.891683 GA | iter = 1764 | Mean = 4.789531 | Best = 4.891683 GA | iter = 1765 | Mean = 4.605244 | Best = 4.891683 GA | iter = 1766 | Mean = 4.095195 | Best = 4.891683 GA | iter = 1767 | Mean = 4.267154 | Best = 4.891683 GA | iter = 1768 | Mean = 4.084812 | Best = 4.891683 GA | iter = 1769 | Mean = 4.507286 | Best = 4.891683 GA | iter = 1770 | Mean = 4.365333 | Best = 4.891683 GA | iter = 1771 | Mean = 4.242970 | Best = 4.891683 GA | iter = 1772 | Mean = 4.145870 | Best = 4.891683 GA | iter = 1773 | Mean = 3.998808 | Best = 4.891683 GA | iter = 1774 | Mean = 4.225398 | Best = 4.891683 GA | iter = 1775 | Mean = 4.205399 | Best = 4.891683 GA | iter = 1776 | Mean = 4.119799 | Best = 4.891683 GA | iter = 1777 | Mean = 4.098155 | Best = 4.891683 GA | iter = 1778 | Mean = 4.089550 | Best = 4.891683 GA | iter = 1779 | Mean = 3.645435 | Best = 4.891683 GA | iter = 1780 | Mean = 3.757558 | Best = 4.891683 GA | iter = 1781 | Mean = 3.971645 | Best = 4.891683 GA | iter = 1782 | Mean = 4.366549 | Best = 4.891683 GA | iter = 1783 | Mean = 4.660522 | Best = 4.891683 GA | iter = 1784 | Mean = 4.371145 | Best = 4.891683 GA | iter = 1785 | Mean = 4.291319 | Best = 4.891683 GA | iter = 1786 | Mean = 4.401885 | Best = 4.891683 GA | iter = 1787 | Mean = 4.378333 | Best = 4.891683 GA | iter = 1788 | Mean = 4.288557 | Best = 4.891683 GA | iter = 1789 | Mean = 4.540529 | Best = 4.891683 GA | iter = 1790 | Mean = 4.333213 | Best = 4.891683 GA | iter = 1791 | Mean = 4.507092 | Best = 4.891683 GA | iter = 1792 | Mean = 4.397983 | Best = 4.891683 GA | iter = 1793 | Mean = 4.493346 | Best = 4.891683 GA | iter = 1794 | Mean = 4.618210 | Best = 4.891683 GA | iter = 1795 | Mean = 4.463118 | Best = 4.891683 GA | iter = 1796 | Mean = 4.498455 | Best = 4.891683 GA | iter = 1797 | Mean = 4.398883 | Best = 4.891683 GA | iter = 1798 | Mean = 4.501669 | Best = 4.891683 GA | iter = 1799 | Mean = 3.964260 | Best = 4.891683 GA | iter = 1800 | Mean = 4.066335 | Best = 4.891683 GA | iter = 1801 | Mean = 4.325807 | Best = 4.891683 GA | iter = 1802 | Mean = 4.750694 | Best = 4.891683 GA | iter = 1803 | Mean = 4.469488 | Best = 4.891683 GA | iter = 1804 | Mean = 4.417571 | Best = 4.891683 GA | iter = 1805 | Mean = 4.343686 | Best = 4.891683 GA | iter = 1806 | Mean = 3.833558 | Best = 4.891683 GA | iter = 1807 | Mean = 3.615270 | Best = 4.891683 GA | iter = 1808 | Mean = 4.220719 | Best = 4.891683 GA | iter = 1809 | Mean = 3.523624 | Best = 4.891683 GA | iter = 1810 | Mean = 4.057758 | Best = 4.891683 GA | iter = 1811 | Mean = 3.641906 | Best = 4.891683 GA | iter = 1812 | Mean = 3.766817 | Best = 4.891683 GA | iter = 1813 | Mean = 4.412250 | Best = 4.891683 GA | iter = 1814 | Mean = 4.057439 | Best = 4.891683 GA | iter = 1815 | Mean = 4.368483 | Best = 4.891683 GA | iter = 1816 | Mean = 4.424160 | Best = 4.891683 GA | iter = 1817 | Mean = 4.506559 | Best = 4.891683 GA | iter = 1818 | Mean = 4.458848 | Best = 4.891683 GA | iter = 1819 | Mean = 4.439533 | Best = 4.891683 GA | iter = 1820 | Mean = 4.400199 | Best = 4.891683 GA | iter = 1821 | Mean = 4.382083 | Best = 4.891683 GA | iter = 1822 | Mean = 4.363229 | Best = 4.891683 GA | iter = 1823 | Mean = 3.839127 | Best = 4.891683 GA | iter = 1824 | Mean = 3.657375 | Best = 4.891683 GA | iter = 1825 | Mean = 4.005251 | Best = 4.891683 GA | iter = 1826 | Mean = 3.931250 | Best = 4.891683 GA | iter = 1827 | Mean = 4.386035 | Best = 4.891683 GA | iter = 1828 | Mean = 4.030779 | Best = 4.891683 GA | iter = 1829 | Mean = 4.301322 | Best = 4.891683 GA | iter = 1830 | Mean = 4.312060 | Best = 4.891683 GA | iter = 1831 | Mean = 4.037680 | Best = 4.891683 GA | iter = 1832 | Mean = 3.874595 | Best = 4.891683 GA | iter = 1833 | Mean = 4.361818 | Best = 4.891683 GA | iter = 1834 | Mean = 4.279329 | Best = 4.891683 GA | iter = 1835 | Mean = 3.982535 | Best = 4.891683 GA | iter = 1836 | Mean = 4.180469 | Best = 4.891683 GA | iter = 1837 | Mean = 4.423076 | Best = 4.891683 GA | iter = 1838 | Mean = 4.132723 | Best = 4.891683 GA | iter = 1839 | Mean = 4.129448 | Best = 4.891683 GA | iter = 1840 | Mean = 3.757822 | Best = 4.891683 GA | iter = 1841 | Mean = 3.737794 | Best = 4.891683 GA | iter = 1842 | Mean = 4.246506 | Best = 4.891683 GA | iter = 1843 | Mean = 4.415546 | Best = 4.891683 GA | iter = 1844 | Mean = 4.355576 | Best = 4.891683 GA | iter = 1845 | Mean = 4.322941 | Best = 4.891683 GA | iter = 1846 | Mean = 4.307727 | Best = 4.891683 GA | iter = 1847 | Mean = 4.133655 | Best = 4.891683 GA | iter = 1848 | Mean = 4.044563 | Best = 4.891683 GA | iter = 1849 | Mean = 4.685045 | Best = 4.891683 GA | iter = 1850 | Mean = 3.815014 | Best = 4.891683 GA | iter = 1851 | Mean = 4.193002 | Best = 4.891683 GA | iter = 1852 | Mean = 4.018524 | Best = 4.891683 GA | iter = 1853 | Mean = 3.823061 | Best = 4.891683 GA | iter = 1854 | Mean = 4.126608 | Best = 4.891683 GA | iter = 1855 | Mean = 3.467039 | Best = 4.891683 GA | iter = 1856 | Mean = 3.943493 | Best = 4.891683 GA | iter = 1857 | Mean = 3.872702 | Best = 4.891683 GA | iter = 1858 | Mean = 4.505451 | Best = 4.891683 GA | iter = 1859 | Mean = 4.345212 | Best = 4.891683 GA | iter = 1860 | Mean = 3.888063 | Best = 4.891683 GA | iter = 1861 | Mean = 4.318175 | Best = 4.891683 GA | iter = 1862 | Mean = 4.219116 | Best = 4.891683 GA | iter = 1863 | Mean = 4.029813 | Best = 4.891683 GA | iter = 1864 | Mean = 4.393161 | Best = 4.891683 GA | iter = 1865 | Mean = 4.391784 | Best = 4.891683 GA | iter = 1866 | Mean = 3.875722 | Best = 4.891683 GA | iter = 1867 | Mean = 4.073108 | Best = 4.891683 GA | iter = 1868 | Mean = 3.682346 | Best = 4.891683 GA | iter = 1869 | Mean = 4.066711 | Best = 4.891683 GA | iter = 1870 | Mean = 4.217949 | Best = 4.891683 GA | iter = 1871 | Mean = 3.825173 | Best = 4.891683 GA | iter = 1872 | Mean = 4.184773 | Best = 4.891683 GA | iter = 1873 | Mean = 4.649526 | Best = 4.891683 GA | iter = 1874 | Mean = 4.464584 | Best = 4.891683 GA | iter = 1875 | Mean = 4.664902 | Best = 4.891683 GA | iter = 1876 | Mean = 4.214142 | Best = 4.891683 GA | iter = 1877 | Mean = 3.808563 | Best = 4.899157 GA | iter = 1878 | Mean = 4.251876 | Best = 4.899157 GA | iter = 1879 | Mean = 4.457152 | Best = 4.899157 GA | iter = 1880 | Mean = 4.325082 | Best = 4.899157 GA | iter = 1881 | Mean = 4.414213 | Best = 4.899157 GA | iter = 1882 | Mean = 4.160461 | Best = 4.899157 GA | iter = 1883 | Mean = 4.446525 | Best = 4.899157 GA | iter = 1884 | Mean = 4.363030 | Best = 4.899157 GA | iter = 1885 | Mean = 4.327261 | Best = 4.899157 GA | iter = 1886 | Mean = 4.671060 | Best = 4.899157 GA | iter = 1887 | Mean = 4.248944 | Best = 4.899157 GA | iter = 1888 | Mean = 4.474201 | Best = 4.899157 GA | iter = 1889 | Mean = 4.498140 | Best = 4.899157 GA | iter = 1890 | Mean = 4.601444 | Best = 4.899157 GA | iter = 1891 | Mean = 4.720438 | Best = 4.899157 GA | iter = 1892 | Mean = 4.533286 | Best = 4.899157 GA | iter = 1893 | Mean = 3.853589 | Best = 4.899157 GA | iter = 1894 | Mean = 4.109983 | Best = 4.899157 GA | iter = 1895 | Mean = 4.144288 | Best = 4.899157 GA | iter = 1896 | Mean = 3.497988 | Best = 4.899157 GA | iter = 1897 | Mean = 3.970174 | Best = 4.899157 GA | iter = 1898 | Mean = 4.269277 | Best = 4.899157 GA | iter = 1899 | Mean = 3.992426 | Best = 4.899157 GA | iter = 1900 | Mean = 3.999124 | Best = 4.899157 GA | iter = 1901 | Mean = 3.735171 | Best = 4.899157 GA | iter = 1902 | Mean = 3.820583 | Best = 4.899157 GA | iter = 1903 | Mean = 4.059084 | Best = 4.899157 GA | iter = 1904 | Mean = 3.793214 | Best = 4.899157 GA | iter = 1905 | Mean = 4.020877 | Best = 4.899157 GA | iter = 1906 | Mean = 3.859180 | Best = 4.899157 GA | iter = 1907 | Mean = 3.913259 | Best = 4.899157 GA | iter = 1908 | Mean = 4.015528 | Best = 4.899157 GA | iter = 1909 | Mean = 4.425010 | Best = 4.899157 GA | iter = 1910 | Mean = 4.264744 | Best = 4.899157 GA | iter = 1911 | Mean = 4.342299 | Best = 4.899157 GA | iter = 1912 | Mean = 4.485232 | Best = 4.899157 GA | iter = 1913 | Mean = 4.616918 | Best = 4.899157 GA | iter = 1914 | Mean = 4.407667 | Best = 4.899157 GA | iter = 1915 | Mean = 4.590702 | Best = 4.899157 GA | iter = 1916 | Mean = 4.554569 | Best = 4.899157 GA | iter = 1917 | Mean = 4.353185 | Best = 4.899157 GA | iter = 1918 | Mean = 4.432197 | Best = 4.899157 GA | iter = 1919 | Mean = 3.889699 | Best = 4.899157 GA | iter = 1920 | Mean = 4.417971 | Best = 4.899157 GA | iter = 1921 | Mean = 4.504010 | Best = 4.899157 GA | iter = 1922 | Mean = 4.400023 | Best = 4.899157 GA | iter = 1923 | Mean = 4.296105 | Best = 4.899157 GA | iter = 1924 | Mean = 4.463058 | Best = 4.899157 GA | iter = 1925 | Mean = 4.536321 | Best = 4.899157 GA | iter = 1926 | Mean = 4.282469 | Best = 4.899157 GA | iter = 1927 | Mean = 4.664934 | Best = 4.899157 GA | iter = 1928 | Mean = 4.647813 | Best = 4.899157 GA | iter = 1929 | Mean = 4.086558 | Best = 4.899157 GA | iter = 1930 | Mean = 4.088802 | Best = 4.899157 GA | iter = 1931 | Mean = 4.178008 | Best = 4.899157 GA | iter = 1932 | Mean = 4.375251 | Best = 4.899157 GA | iter = 1933 | Mean = 4.019744 | Best = 4.899157 GA | iter = 1934 | Mean = 4.020192 | Best = 4.899157 GA | iter = 1935 | Mean = 4.477093 | Best = 4.899157 GA | iter = 1936 | Mean = 4.396182 | Best = 4.899157 GA | iter = 1937 | Mean = 4.570660 | Best = 4.899157 GA | iter = 1938 | Mean = 3.951894 | Best = 4.899157 GA | iter = 1939 | Mean = 4.221742 | Best = 4.899157 GA | iter = 1940 | Mean = 4.324173 | Best = 4.899157 GA | iter = 1941 | Mean = 3.984078 | Best = 4.899157 GA | iter = 1942 | Mean = 4.269914 | Best = 4.899157 GA | iter = 1943 | Mean = 4.500495 | Best = 4.899157 GA | iter = 1944 | Mean = 4.367130 | Best = 4.899157 GA | iter = 1945 | Mean = 4.168964 | Best = 4.899157 GA | iter = 1946 | Mean = 4.411838 | Best = 4.899157 GA | iter = 1947 | Mean = 4.613917 | Best = 4.899157 GA | iter = 1948 | Mean = 4.700868 | Best = 4.899157 GA | iter = 1949 | Mean = 4.547405 | Best = 4.899157 GA | iter = 1950 | Mean = 4.629456 | Best = 4.899785 GA | iter = 1951 | Mean = 4.604376 | Best = 4.899785 GA | iter = 1952 | Mean = 4.377715 | Best = 4.899785 GA | iter = 1953 | Mean = 3.997920 | Best = 4.899785 GA | iter = 1954 | Mean = 4.099572 | Best = 4.899785 GA | iter = 1955 | Mean = 4.129719 | Best = 4.899785 GA | iter = 1956 | Mean = 4.407544 | Best = 4.899785 GA | iter = 1957 | Mean = 4.531521 | Best = 4.899785 GA | iter = 1958 | Mean = 4.174894 | Best = 4.899785 GA | iter = 1959 | Mean = 4.351915 | Best = 4.899785 GA | iter = 1960 | Mean = 4.665849 | Best = 4.899785 GA | iter = 1961 | Mean = 4.432475 | Best = 4.899785 GA | iter = 1962 | Mean = 4.319925 | Best = 4.899785 GA | iter = 1963 | Mean = 4.105210 | Best = 4.899785 GA | iter = 1964 | Mean = 4.435307 | Best = 4.899785 GA | iter = 1965 | Mean = 4.644696 | Best = 4.899785 GA | iter = 1966 | Mean = 4.535038 | Best = 4.899785 GA | iter = 1967 | Mean = 4.249177 | Best = 4.899785 GA | iter = 1968 | Mean = 4.373105 | Best = 4.899785 GA | iter = 1969 | Mean = 4.590451 | Best = 4.899785 GA | iter = 1970 | Mean = 4.145693 | Best = 4.899785 GA | iter = 1971 | Mean = 4.310079 | Best = 4.899785 GA | iter = 1972 | Mean = 3.937988 | Best = 4.899785 GA | iter = 1973 | Mean = 3.812564 | Best = 4.899785 GA | iter = 1974 | Mean = 3.988407 | Best = 4.899785 GA | iter = 1975 | Mean = 4.118600 | Best = 4.899785 GA | iter = 1976 | Mean = 4.696994 | Best = 4.899785 GA | iter = 1977 | Mean = 4.523162 | Best = 4.899785 GA | iter = 1978 | Mean = 4.612535 | Best = 4.899785 GA | iter = 1979 | Mean = 4.308086 | Best = 4.899785 GA | iter = 1980 | Mean = 4.319721 | Best = 4.899785 GA | iter = 1981 | Mean = 4.427040 | Best = 4.899785 GA | iter = 1982 | Mean = 4.318679 | Best = 4.899785 GA | iter = 1983 | Mean = 4.252416 | Best = 4.899785 GA | iter = 1984 | Mean = 4.480643 | Best = 4.899785 GA | iter = 1985 | Mean = 4.314660 | Best = 4.899785 GA | iter = 1986 | Mean = 4.265967 | Best = 4.899785 GA | iter = 1987 | Mean = 4.250885 | Best = 4.899785 GA | iter = 1988 | Mean = 3.974269 | Best = 4.899785 GA | iter = 1989 | Mean = 4.456091 | Best = 4.899785 GA | iter = 1990 | Mean = 4.130354 | Best = 4.899785 GA | iter = 1991 | Mean = 4.736016 | Best = 4.899785 GA | iter = 1992 | Mean = 3.999655 | Best = 4.899785 GA | iter = 1993 | Mean = 4.096085 | Best = 4.899785 GA | iter = 1994 | Mean = 4.073285 | Best = 4.899785 GA | iter = 1995 | Mean = 4.705629 | Best = 4.899785 GA | iter = 1996 | Mean = 4.242253 | Best = 4.899785 GA | iter = 1997 | Mean = 4.650596 | Best = 4.899785 GA | iter = 1998 | Mean = 4.218067 | Best = 4.899785 GA | iter = 1999 | Mean = 3.885801 | Best = 4.899785 GA | iter = 2000 | Mean = 4.047088 | Best = 4.899785 GA | iter = 2001 | Mean = 4.172059 | Best = 4.899785 GA | iter = 2002 | Mean = 4.010842 | Best = 4.899785 GA | iter = 2003 | Mean = 4.371351 | Best = 4.899785 GA | iter = 2004 | Mean = 4.316170 | Best = 4.899785 GA | iter = 2005 | Mean = 4.018502 | Best = 4.899785 GA | iter = 2006 | Mean = 3.978372 | Best = 4.899785 GA | iter = 2007 | Mean = 3.779349 | Best = 4.899785 GA | iter = 2008 | Mean = 4.322717 | Best = 4.899785 GA | iter = 2009 | Mean = 3.930231 | Best = 4.899785 GA | iter = 2010 | Mean = 4.120686 | Best = 4.899785 GA | iter = 2011 | Mean = 4.160452 | Best = 4.899785 GA | iter = 2012 | Mean = 4.193860 | Best = 4.899785 GA | iter = 2013 | Mean = 4.244926 | Best = 4.899785 GA | iter = 2014 | Mean = 4.243942 | Best = 4.899785 GA | iter = 2015 | Mean = 4.330444 | Best = 4.899785 GA | iter = 2016 | Mean = 4.319705 | Best = 4.899785 GA | iter = 2017 | Mean = 4.293784 | Best = 4.899785 GA | iter = 2018 | Mean = 4.459053 | Best = 4.899785 GA | iter = 2019 | Mean = 4.168501 | Best = 4.899785 GA | iter = 2020 | Mean = 4.297490 | Best = 4.899785 GA | iter = 2021 | Mean = 3.717538 | Best = 4.899785 GA | iter = 2022 | Mean = 4.338537 | Best = 4.899785 GA | iter = 2023 | Mean = 4.462700 | Best = 4.899785 GA | iter = 2024 | Mean = 4.042468 | Best = 4.899785 GA | iter = 2025 | Mean = 4.015883 | Best = 4.899785 GA | iter = 2026 | Mean = 3.896298 | Best = 4.899785 GA | iter = 2027 | Mean = 4.152422 | Best = 4.899785 GA | iter = 2028 | Mean = 4.261563 | Best = 4.899785 GA | iter = 2029 | Mean = 3.848495 | Best = 4.899785 GA | iter = 2030 | Mean = 3.846392 | Best = 4.899785 GA | iter = 2031 | Mean = 4.236304 | Best = 4.899785 GA | iter = 2032 | Mean = 3.639956 | Best = 4.899785 GA | iter = 2033 | Mean = 4.309862 | Best = 4.899785 GA | iter = 2034 | Mean = 3.461650 | Best = 4.899785 GA | iter = 2035 | Mean = 4.230669 | Best = 4.899785 GA | iter = 2036 | Mean = 4.278182 | Best = 4.899785 GA | iter = 2037 | Mean = 4.384192 | Best = 4.899785 GA | iter = 2038 | Mean = 4.364673 | Best = 4.899785 GA | iter = 2039 | Mean = 4.699067 | Best = 4.899785 GA | iter = 2040 | Mean = 4.560484 | Best = 4.899785 GA | iter = 2041 | Mean = 4.450761 | Best = 4.899785 GA | iter = 2042 | Mean = 4.597679 | Best = 4.899785 GA | iter = 2043 | Mean = 4.332742 | Best = 4.899785 GA | iter = 2044 | Mean = 4.174141 | Best = 4.899785 GA | iter = 2045 | Mean = 3.453507 | Best = 4.899785 GA | iter = 2046 | Mean = 3.476204 | Best = 4.899785 GA | iter = 2047 | Mean = 3.065930 | Best = 4.899785 GA | iter = 2048 | Mean = 3.549156 | Best = 4.899785 GA | iter = 2049 | Mean = 3.723066 | Best = 4.899785 GA | iter = 2050 | Mean = 4.118295 | Best = 4.899785 GA | iter = 2051 | Mean = 4.204913 | Best = 4.899785 GA | iter = 2052 | Mean = 4.222231 | Best = 4.899785 GA | iter = 2053 | Mean = 4.044308 | Best = 4.899785 GA | iter = 2054 | Mean = 4.268117 | Best = 4.899785 GA | iter = 2055 | Mean = 4.102214 | Best = 4.899785 GA | iter = 2056 | Mean = 4.212134 | Best = 4.899785 GA | iter = 2057 | Mean = 4.483327 | Best = 4.899785 GA | iter = 2058 | Mean = 4.571205 | Best = 4.899785 GA | iter = 2059 | Mean = 4.029146 | Best = 4.899785 GA | iter = 2060 | Mean = 4.314928 | Best = 4.899785 GA | iter = 2061 | Mean = 4.194782 | Best = 4.899785 GA | iter = 2062 | Mean = 4.140101 | Best = 4.899785 GA | iter = 2063 | Mean = 3.978164 | Best = 4.899785 GA | iter = 2064 | Mean = 4.215087 | Best = 4.899785 GA | iter = 2065 | Mean = 4.374522 | Best = 4.899785 GA | iter = 2066 | Mean = 4.527654 | Best = 4.899785 GA | iter = 2067 | Mean = 4.171555 | Best = 4.899785 GA | iter = 2068 | Mean = 4.313435 | Best = 4.899785 GA | iter = 2069 | Mean = 4.366261 | Best = 4.899785 GA | iter = 2070 | Mean = 4.140417 | Best = 4.899785 GA | iter = 2071 | Mean = 4.378533 | Best = 4.899785 GA | iter = 2072 | Mean = 4.338793 | Best = 4.899785 GA | iter = 2073 | Mean = 4.489599 | Best = 4.899785 GA | iter = 2074 | Mean = 4.204937 | Best = 4.899785 GA | iter = 2075 | Mean = 3.849515 | Best = 4.899785 GA | iter = 2076 | Mean = 4.148378 | Best = 4.899785 GA | iter = 2077 | Mean = 3.986420 | Best = 4.899785 GA | iter = 2078 | Mean = 4.292511 | Best = 4.899785 GA | iter = 2079 | Mean = 4.452964 | Best = 4.899785 GA | iter = 2080 | Mean = 3.821602 | Best = 4.899785 GA | iter = 2081 | Mean = 3.709401 | Best = 4.899785 GA | iter = 2082 | Mean = 3.392051 | Best = 4.899785 GA | iter = 2083 | Mean = 3.882458 | Best = 4.899785 GA | iter = 2084 | Mean = 3.991130 | Best = 4.899785 GA | iter = 2085 | Mean = 3.897085 | Best = 4.899785 GA | iter = 2086 | Mean = 3.915675 | Best = 4.899785 GA | iter = 2087 | Mean = 4.390655 | Best = 4.899785 GA | iter = 2088 | Mean = 4.374713 | Best = 4.899785 GA | iter = 2089 | Mean = 4.199214 | Best = 4.899785 GA | iter = 2090 | Mean = 4.387316 | Best = 4.899785 GA | iter = 2091 | Mean = 4.181498 | Best = 4.899785 GA | iter = 2092 | Mean = 4.261013 | Best = 4.899785 GA | iter = 2093 | Mean = 4.486173 | Best = 4.899785 GA | iter = 2094 | Mean = 4.475392 | Best = 4.899785 GA | iter = 2095 | Mean = 4.342255 | Best = 4.899785 GA | iter = 2096 | Mean = 3.797423 | Best = 4.899785 GA | iter = 2097 | Mean = 3.706131 | Best = 4.899785 GA | iter = 2098 | Mean = 3.741049 | Best = 4.899785 GA | iter = 2099 | Mean = 4.018442 | Best = 4.899785 GA | iter = 2100 | Mean = 4.214323 | Best = 4.899785 GA | iter = 2101 | Mean = 4.203646 | Best = 4.899785 GA | iter = 2102 | Mean = 4.422814 | Best = 4.899785 GA | iter = 2103 | Mean = 4.306724 | Best = 4.899785 GA | iter = 2104 | Mean = 4.179829 | Best = 4.899785 GA | iter = 2105 | Mean = 3.577443 | Best = 4.899785 GA | iter = 2106 | Mean = 3.815659 | Best = 4.899785 GA | iter = 2107 | Mean = 4.380294 | Best = 4.899785 GA | iter = 2108 | Mean = 4.167017 | Best = 4.899785 GA | iter = 2109 | Mean = 4.144249 | Best = 4.899785 GA | iter = 2110 | Mean = 4.396978 | Best = 4.899785 GA | iter = 2111 | Mean = 3.859958 | Best = 4.899785 GA | iter = 2112 | Mean = 4.539659 | Best = 4.899785 GA | iter = 2113 | Mean = 4.534810 | Best = 4.899785 GA | iter = 2114 | Mean = 4.423992 | Best = 4.899785 GA | iter = 2115 | Mean = 3.926599 | Best = 4.899785 GA | iter = 2116 | Mean = 3.667404 | Best = 4.899785 GA | iter = 2117 | Mean = 3.741821 | Best = 4.899785 GA | iter = 2118 | Mean = 3.610242 | Best = 4.899785 GA | iter = 2119 | Mean = 4.010187 | Best = 4.899785 GA | iter = 2120 | Mean = 3.621298 | Best = 4.899785 GA | iter = 2121 | Mean = 3.822030 | Best = 4.899785 GA | iter = 2122 | Mean = 4.594229 | Best = 4.899785 GA | iter = 2123 | Mean = 4.311736 | Best = 4.899785 GA | iter = 2124 | Mean = 4.179766 | Best = 4.899785 GA | iter = 2125 | Mean = 3.921654 | Best = 4.899785 GA | iter = 2126 | Mean = 3.630511 | Best = 4.899785 GA | iter = 2127 | Mean = 3.761680 | Best = 4.899785 GA | iter = 2128 | Mean = 4.244612 | Best = 4.899785 GA | iter = 2129 | Mean = 4.222874 | Best = 4.899785 GA | iter = 2130 | Mean = 4.078904 | Best = 4.899785 GA | iter = 2131 | Mean = 4.441848 | Best = 4.899785 GA | iter = 2132 | Mean = 3.974752 | Best = 4.899785 GA | iter = 2133 | Mean = 4.256139 | Best = 4.899785 GA | iter = 2134 | Mean = 4.539076 | Best = 4.899785 GA | iter = 2135 | Mean = 4.457243 | Best = 4.899785 GA | iter = 2136 | Mean = 4.611075 | Best = 4.899785 GA | iter = 2137 | Mean = 4.511197 | Best = 4.899785 GA | iter = 2138 | Mean = 4.268100 | Best = 4.899785 GA | iter = 2139 | Mean = 4.303392 | Best = 4.899785 GA | iter = 2140 | Mean = 4.397970 | Best = 4.899785 GA | iter = 2141 | Mean = 4.012093 | Best = 4.899785 GA | iter = 2142 | Mean = 4.525000 | Best = 4.899785 GA | iter = 2143 | Mean = 4.109697 | Best = 4.899785 GA | iter = 2144 | Mean = 4.300560 | Best = 4.899785 GA | iter = 2145 | Mean = 3.837061 | Best = 4.899785 GA | iter = 2146 | Mean = 4.317819 | Best = 4.899785 GA | iter = 2147 | Mean = 4.427795 | Best = 4.899785 GA | iter = 2148 | Mean = 3.945723 | Best = 4.899785 GA | iter = 2149 | Mean = 4.058183 | Best = 4.899785 GA | iter = 2150 | Mean = 3.920045 | Best = 4.899785 GA | iter = 2151 | Mean = 3.831992 | Best = 4.899785 GA | iter = 2152 | Mean = 3.002713 | Best = 4.899785 GA | iter = 2153 | Mean = 3.393276 | Best = 4.899785 GA | iter = 2154 | Mean = 3.064087 | Best = 4.899785 GA | iter = 2155 | Mean = 3.743811 | Best = 4.899785 GA | iter = 2156 | Mean = 3.974632 | Best = 4.899785 GA | iter = 2157 | Mean = 4.195450 | Best = 4.899785 GA | iter = 2158 | Mean = 3.923512 | Best = 4.899785 GA | iter = 2159 | Mean = 4.297151 | Best = 4.899785 GA | iter = 2160 | Mean = 3.855855 | Best = 4.899785 GA | iter = 2161 | Mean = 3.694471 | Best = 4.899785 GA | iter = 2162 | Mean = 3.841759 | Best = 4.899785 GA | iter = 2163 | Mean = 4.177303 | Best = 4.899785 GA | iter = 2164 | Mean = 3.862864 | Best = 4.899785 GA | iter = 2165 | Mean = 3.476405 | Best = 4.899785 GA | iter = 2166 | Mean = 3.503804 | Best = 4.899785 GA | iter = 2167 | Mean = 4.041942 | Best = 4.899785 GA | iter = 2168 | Mean = 4.254804 | Best = 4.899785 GA | iter = 2169 | Mean = 4.364298 | Best = 4.899785 GA | iter = 2170 | Mean = 4.330228 | Best = 4.899785 GA | iter = 2171 | Mean = 3.643688 | Best = 4.899785 GA | iter = 2172 | Mean = 3.947279 | Best = 4.899785 GA | iter = 2173 | Mean = 4.286224 | Best = 4.899785 GA | iter = 2174 | Mean = 4.279032 | Best = 4.899785 GA | iter = 2175 | Mean = 4.338684 | Best = 4.899785 GA | iter = 2176 | Mean = 4.328682 | Best = 4.899785 GA | iter = 2177 | Mean = 4.435818 | Best = 4.899785 GA | iter = 2178 | Mean = 4.044491 | Best = 4.899785 GA | iter = 2179 | Mean = 4.301259 | Best = 4.899785 GA | iter = 2180 | Mean = 4.057513 | Best = 4.899785 GA | iter = 2181 | Mean = 4.020962 | Best = 4.899785 GA | iter = 2182 | Mean = 4.379506 | Best = 4.899785 GA | iter = 2183 | Mean = 4.116809 | Best = 4.899785 GA | iter = 2184 | Mean = 3.780642 | Best = 4.899785 GA | iter = 2185 | Mean = 4.289408 | Best = 4.899785 GA | iter = 2186 | Mean = 4.188241 | Best = 4.899785 GA | iter = 2187 | Mean = 4.116734 | Best = 4.899785 GA | iter = 2188 | Mean = 4.251522 | Best = 4.899785 GA | iter = 2189 | Mean = 4.392890 | Best = 4.899785 GA | iter = 2190 | Mean = 4.083278 | Best = 4.899785 GA | iter = 2191 | Mean = 3.927737 | Best = 4.899785 GA | iter = 2192 | Mean = 4.353678 | Best = 4.899785 GA | iter = 2193 | Mean = 3.724538 | Best = 4.899785 GA | iter = 2194 | Mean = 4.403740 | Best = 4.899785 GA | iter = 2195 | Mean = 4.524635 | Best = 4.899785 GA | iter = 2196 | Mean = 4.166056 | Best = 4.899785 GA | iter = 2197 | Mean = 4.065372 | Best = 4.899785 GA | iter = 2198 | Mean = 4.677970 | Best = 4.899785 GA | iter = 2199 | Mean = 4.156171 | Best = 4.899785 GA | iter = 2200 | Mean = 4.631534 | Best = 4.899785 GA | iter = 2201 | Mean = 4.450996 | Best = 4.899785 GA | iter = 2202 | Mean = 4.211201 | Best = 4.899785 GA | iter = 2203 | Mean = 4.316834 | Best = 4.899785 GA | iter = 2204 | Mean = 4.665721 | Best = 4.899785 GA | iter = 2205 | Mean = 4.491513 | Best = 4.899785 GA | iter = 2206 | Mean = 4.139059 | Best = 4.899785 GA | iter = 2207 | Mean = 4.175611 | Best = 4.899785 GA | iter = 2208 | Mean = 3.996069 | Best = 4.899785 GA | iter = 2209 | Mean = 4.095595 | Best = 4.899785 GA | iter = 2210 | Mean = 4.071785 | Best = 4.899785 GA | iter = 2211 | Mean = 4.388830 | Best = 4.899785 GA | iter = 2212 | Mean = 4.062264 | Best = 4.900285 GA | iter = 2213 | Mean = 4.021331 | Best = 4.900285 GA | iter = 2214 | Mean = 3.487878 | Best = 4.900285 GA | iter = 2215 | Mean = 4.259607 | Best = 4.900285 GA | iter = 2216 | Mean = 4.179487 | Best = 4.900285 GA | iter = 2217 | Mean = 3.548370 | Best = 4.900285 GA | iter = 2218 | Mean = 3.553957 | Best = 4.900285 GA | iter = 2219 | Mean = 4.250902 | Best = 4.900285 GA | iter = 2220 | Mean = 4.212763 | Best = 4.900285 GA | iter = 2221 | Mean = 4.140503 | Best = 4.900285 GA | iter = 2222 | Mean = 4.085498 | Best = 4.900285 GA | iter = 2223 | Mean = 4.038429 | Best = 4.900285 GA | iter = 2224 | Mean = 4.347534 | Best = 4.900285 GA | iter = 2225 | Mean = 4.096172 | Best = 4.900285 GA | iter = 2226 | Mean = 3.730557 | Best = 4.900285 GA | iter = 2227 | Mean = 4.502930 | Best = 4.900285 GA | iter = 2228 | Mean = 4.622299 | Best = 4.900285 GA | iter = 2229 | Mean = 4.178477 | Best = 4.900285 GA | iter = 2230 | Mean = 4.202402 | Best = 4.900285 GA | iter = 2231 | Mean = 4.392356 | Best = 4.900285 GA | iter = 2232 | Mean = 3.849754 | Best = 4.900285 GA | iter = 2233 | Mean = 3.764765 | Best = 4.900285 GA | iter = 2234 | Mean = 3.392706 | Best = 4.900285 GA | iter = 2235 | Mean = 4.335142 | Best = 4.900285 GA | iter = 2236 | Mean = 3.907251 | Best = 4.900285 GA | iter = 2237 | Mean = 4.190354 | Best = 4.900285 GA | iter = 2238 | Mean = 4.238050 | Best = 4.900285 GA | iter = 2239 | Mean = 4.111978 | Best = 4.900285 GA | iter = 2240 | Mean = 3.656191 | Best = 4.900285 GA | iter = 2241 | Mean = 4.354843 | Best = 4.900285 GA | iter = 2242 | Mean = 4.521196 | Best = 4.900285 GA | iter = 2243 | Mean = 4.282760 | Best = 4.900285 GA | iter = 2244 | Mean = 4.290075 | Best = 4.900285 GA | iter = 2245 | Mean = 3.606316 | Best = 4.900285 GA | iter = 2246 | Mean = 4.439214 | Best = 4.900285 GA | iter = 2247 | Mean = 4.384426 | Best = 4.900285 GA | iter = 2248 | Mean = 4.281339 | Best = 4.900285 GA | iter = 2249 | Mean = 4.281059 | Best = 4.900285 GA | iter = 2250 | Mean = 4.181807 | Best = 4.900285 GA | iter = 2251 | Mean = 3.706303 | Best = 4.900285 GA | iter = 2252 | Mean = 3.590091 | Best = 4.900285 GA | iter = 2253 | Mean = 3.373322 | Best = 4.900285 GA | iter = 2254 | Mean = 3.522151 | Best = 4.900285 GA | iter = 2255 | Mean = 3.517413 | Best = 4.900285 GA | iter = 2256 | Mean = 3.743960 | Best = 4.900285 GA | iter = 2257 | Mean = 4.005474 | Best = 4.900285 GA | iter = 2258 | Mean = 4.194053 | Best = 4.900285 GA | iter = 2259 | Mean = 3.833262 | Best = 4.900285 GA | iter = 2260 | Mean = 4.198364 | Best = 4.900285 GA | iter = 2261 | Mean = 4.007264 | Best = 4.900285 GA | iter = 2262 | Mean = 4.350842 | Best = 4.900285 GA | iter = 2263 | Mean = 4.225658 | Best = 4.900285 GA | iter = 2264 | Mean = 4.069063 | Best = 4.900285 GA | iter = 2265 | Mean = 4.098214 | Best = 4.900285 GA | iter = 2266 | Mean = 4.100946 | Best = 4.900285 GA | iter = 2267 | Mean = 4.504952 | Best = 4.900285 GA | iter = 2268 | Mean = 4.172481 | Best = 4.900285 GA | iter = 2269 | Mean = 4.172009 | Best = 4.900285 GA | iter = 2270 | Mean = 3.693405 | Best = 4.900285 GA | iter = 2271 | Mean = 4.196117 | Best = 4.900285 GA | iter = 2272 | Mean = 4.485659 | Best = 4.900285 GA | iter = 2273 | Mean = 4.766077 | Best = 4.900285 GA | iter = 2274 | Mean = 4.436267 | Best = 4.900285 GA | iter = 2275 | Mean = 4.273966 | Best = 4.900285 GA | iter = 2276 | Mean = 4.111841 | Best = 4.900285 GA | iter = 2277 | Mean = 4.074473 | Best = 4.900285 GA | iter = 2278 | Mean = 4.123720 | Best = 4.900285 GA | iter = 2279 | Mean = 4.207855 | Best = 4.900285 GA | iter = 2280 | Mean = 4.283264 | Best = 4.900285 GA | iter = 2281 | Mean = 4.151322 | Best = 4.900285 GA | iter = 2282 | Mean = 4.437990 | Best = 4.900285 GA | iter = 2283 | Mean = 4.187397 | Best = 4.900285 GA | iter = 2284 | Mean = 4.627035 | Best = 4.900285 GA | iter = 2285 | Mean = 4.676684 | Best = 4.900285 GA | iter = 2286 | Mean = 4.214550 | Best = 4.900285 GA | iter = 2287 | Mean = 3.948346 | Best = 4.900285 GA | iter = 2288 | Mean = 4.496203 | Best = 4.900285 GA | iter = 2289 | Mean = 4.247626 | Best = 4.900285 GA | iter = 2290 | Mean = 4.225862 | Best = 4.900285 GA | iter = 2291 | Mean = 4.260015 | Best = 4.900285 GA | iter = 2292 | Mean = 4.497914 | Best = 4.900285 GA | iter = 2293 | Mean = 4.230517 | Best = 4.900285 GA | iter = 2294 | Mean = 4.215866 | Best = 4.900285 GA | iter = 2295 | Mean = 4.436006 | Best = 4.900285 GA | iter = 2296 | Mean = 4.411194 | Best = 4.900285 GA | iter = 2297 | Mean = 4.332217 | Best = 4.900285 GA | iter = 2298 | Mean = 4.196786 | Best = 4.900285 GA | iter = 2299 | Mean = 4.014546 | Best = 4.900285 GA | iter = 2300 | Mean = 3.529903 | Best = 4.900285 GA | iter = 2301 | Mean = 3.882168 | Best = 4.900285 GA | iter = 2302 | Mean = 4.417870 | Best = 4.900285 GA | iter = 2303 | Mean = 3.781738 | Best = 4.900285 GA | iter = 2304 | Mean = 4.220172 | Best = 4.900313 GA | iter = 2305 | Mean = 4.634228 | Best = 4.900313 GA | iter = 2306 | Mean = 4.439950 | Best = 4.900313 GA | iter = 2307 | Mean = 4.456546 | Best = 4.900313 GA | iter = 2308 | Mean = 3.712779 | Best = 4.900313 GA | iter = 2309 | Mean = 3.434708 | Best = 4.900313 GA | iter = 2310 | Mean = 4.356027 | Best = 4.900313 GA | iter = 2311 | Mean = 4.300523 | Best = 4.900313 GA | iter = 2312 | Mean = 4.530955 | Best = 4.900313 GA | iter = 2313 | Mean = 4.011821 | Best = 4.900517 GA | iter = 2314 | Mean = 4.323985 | Best = 4.900517 GA | iter = 2315 | Mean = 4.322944 | Best = 4.900517 GA | iter = 2316 | Mean = 4.488000 | Best = 4.900517 GA | iter = 2317 | Mean = 4.395728 | Best = 4.900517 GA | iter = 2318 | Mean = 4.087048 | Best = 4.900517 GA | iter = 2319 | Mean = 3.989709 | Best = 4.900517 GA | iter = 2320 | Mean = 4.115404 | Best = 4.900517 GA | iter = 2321 | Mean = 4.275701 | Best = 4.900517 GA | iter = 2322 | Mean = 4.338843 | Best = 4.900517 GA | iter = 2323 | Mean = 4.251760 | Best = 4.900517 GA | iter = 2324 | Mean = 4.099171 | Best = 4.900517 GA | iter = 2325 | Mean = 4.407424 | Best = 4.900517 GA | iter = 2326 | Mean = 4.392778 | Best = 4.900517 GA | iter = 2327 | Mean = 4.408086 | Best = 4.900517 GA | iter = 2328 | Mean = 4.001562 | Best = 4.900531 GA | iter = 2329 | Mean = 4.611078 | Best = 4.900531 GA | iter = 2330 | Mean = 4.292730 | Best = 4.900531 GA | iter = 2331 | Mean = 4.289429 | Best = 4.900531 GA | iter = 2332 | Mean = 4.123832 | Best = 4.900531 GA | iter = 2333 | Mean = 4.332035 | Best = 4.900531 GA | iter = 2334 | Mean = 4.447286 | Best = 4.900531 GA | iter = 2335 | Mean = 4.864828 | Best = 4.900531 GA | iter = 2336 | Mean = 4.480284 | Best = 4.900531 GA | iter = 2337 | Mean = 4.200575 | Best = 4.900531 GA | iter = 2338 | Mean = 4.292809 | Best = 4.900531 GA | iter = 2339 | Mean = 4.313316 | Best = 4.900531 GA | iter = 2340 | Mean = 4.318794 | Best = 4.900531 GA | iter = 2341 | Mean = 4.562362 | Best = 4.900531 GA | iter = 2342 | Mean = 4.360665 | Best = 4.900531 GA | iter = 2343 | Mean = 4.624764 | Best = 4.900531 GA | iter = 2344 | Mean = 4.618671 | Best = 4.900531 GA | iter = 2345 | Mean = 4.458670 | Best = 4.900531 GA | iter = 2346 | Mean = 4.083175 | Best = 4.900531 GA | iter = 2347 | Mean = 4.257137 | Best = 4.900531 GA | iter = 2348 | Mean = 4.127979 | Best = 4.900531 GA | iter = 2349 | Mean = 4.215755 | Best = 4.900531 GA | iter = 2350 | Mean = 4.535847 | Best = 4.900531 GA | iter = 2351 | Mean = 4.222279 | Best = 4.900531 GA | iter = 2352 | Mean = 4.497657 | Best = 4.900531 GA | iter = 2353 | Mean = 4.005263 | Best = 4.900531 GA | iter = 2354 | Mean = 4.420520 | Best = 4.900531 GA | iter = 2355 | Mean = 4.174109 | Best = 4.900531 GA | iter = 2356 | Mean = 4.415750 | Best = 4.900531 GA | iter = 2357 | Mean = 4.123591 | Best = 4.900531 GA | iter = 2358 | Mean = 3.903349 | Best = 4.900531 GA | iter = 2359 | Mean = 3.929796 | Best = 4.900531 GA | iter = 2360 | Mean = 3.892841 | Best = 4.900531 GA | iter = 2361 | Mean = 4.213157 | Best = 4.900531 GA | iter = 2362 | Mean = 4.152886 | Best = 4.900531 GA | iter = 2363 | Mean = 4.209269 | Best = 4.900531 GA | iter = 2364 | Mean = 4.333411 | Best = 4.900531 GA | iter = 2365 | Mean = 4.262474 | Best = 4.900531 GA | iter = 2366 | Mean = 4.240246 | Best = 4.900531 GA | iter = 2367 | Mean = 4.504070 | Best = 4.900531 GA | iter = 2368 | Mean = 4.386401 | Best = 4.900531 GA | iter = 2369 | Mean = 4.226039 | Best = 4.900531 GA | iter = 2370 | Mean = 4.342555 | Best = 4.900531 GA | iter = 2371 | Mean = 4.522717 | Best = 4.900531 GA | iter = 2372 | Mean = 4.319419 | Best = 4.900531 GA | iter = 2373 | Mean = 4.448801 | Best = 4.900531 GA | iter = 2374 | Mean = 4.768391 | Best = 4.900531 GA | iter = 2375 | Mean = 4.443759 | Best = 4.900531 GA | iter = 2376 | Mean = 4.191980 | Best = 4.900531 GA | iter = 2377 | Mean = 4.291624 | Best = 4.900531 GA | iter = 2378 | Mean = 4.600626 | Best = 4.900531 GA | iter = 2379 | Mean = 4.442316 | Best = 4.900531 GA | iter = 2380 | Mean = 4.790824 | Best = 4.900531 GA | iter = 2381 | Mean = 4.635620 | Best = 4.900531 GA | iter = 2382 | Mean = 4.057600 | Best = 4.900531 GA | iter = 2383 | Mean = 4.421243 | Best = 4.900531 GA | iter = 2384 | Mean = 4.514265 | Best = 4.900531 GA | iter = 2385 | Mean = 4.450205 | Best = 4.900531 GA | iter = 2386 | Mean = 4.144913 | Best = 4.900531 GA | iter = 2387 | Mean = 4.435235 | Best = 4.900531 GA | iter = 2388 | Mean = 4.263145 | Best = 4.900531 GA | iter = 2389 | Mean = 4.495296 | Best = 4.900531 GA | iter = 2390 | Mean = 4.341322 | Best = 4.900531 GA | iter = 2391 | Mean = 4.381288 | Best = 4.900531 GA | iter = 2392 | Mean = 3.854569 | Best = 4.900531 GA | iter = 2393 | Mean = 4.144873 | Best = 4.900531 GA | iter = 2394 | Mean = 4.339851 | Best = 4.900531 GA | iter = 2395 | Mean = 4.085365 | Best = 4.900531 GA | iter = 2396 | Mean = 4.255151 | Best = 4.900531 GA | iter = 2397 | Mean = 4.455032 | Best = 4.900531 GA | iter = 2398 | Mean = 4.458467 | Best = 4.900531 GA | iter = 2399 | Mean = 4.601368 | Best = 4.900531 GA | iter = 2400 | Mean = 4.537456 | Best = 4.900531 GA | iter = 2401 | Mean = 4.523272 | Best = 4.900531 GA | iter = 2402 | Mean = 3.972531 | Best = 4.900531 GA | iter = 2403 | Mean = 4.501268 | Best = 4.900531 GA | iter = 2404 | Mean = 4.051366 | Best = 4.900531 GA | iter = 2405 | Mean = 4.528340 | Best = 4.900531 GA | iter = 2406 | Mean = 4.025909 | Best = 4.900531 GA | iter = 2407 | Mean = 3.412560 | Best = 4.900531 GA | iter = 2408 | Mean = 4.257246 | Best = 4.900531 GA | iter = 2409 | Mean = 4.603382 | Best = 4.900531 GA | iter = 2410 | Mean = 4.293225 | Best = 4.900531 GA | iter = 2411 | Mean = 3.808976 | Best = 4.900531 GA | iter = 2412 | Mean = 4.367508 | Best = 4.900531 GA | iter = 2413 | Mean = 4.374276 | Best = 4.900531 GA | iter = 2414 | Mean = 4.186581 | Best = 4.900531 GA | iter = 2415 | Mean = 4.581984 | Best = 4.900531 GA | iter = 2416 | Mean = 3.853556 | Best = 4.900531 GA | iter = 2417 | Mean = 4.015714 | Best = 4.900531 GA | iter = 2418 | Mean = 3.488579 | Best = 4.900531 GA | iter = 2419 | Mean = 4.430666 | Best = 4.900531 GA | iter = 2420 | Mean = 4.053998 | Best = 4.900531 GA | iter = 2421 | Mean = 4.124676 | Best = 4.900531 GA | iter = 2422 | Mean = 4.460625 | Best = 4.900531 GA | iter = 2423 | Mean = 4.262056 | Best = 4.900531 GA | iter = 2424 | Mean = 4.178295 | Best = 4.900531 GA | iter = 2425 | Mean = 4.183475 | Best = 4.900531 GA | iter = 2426 | Mean = 4.285416 | Best = 4.900531 GA | iter = 2427 | Mean = 3.932174 | Best = 4.900531 GA | iter = 2428 | Mean = 4.311870 | Best = 4.900531 GA | iter = 2429 | Mean = 4.198743 | Best = 4.900531 GA | iter = 2430 | Mean = 4.572928 | Best = 4.900531 GA | iter = 2431 | Mean = 4.384075 | Best = 4.900531 GA | iter = 2432 | Mean = 4.439631 | Best = 4.900531 GA | iter = 2433 | Mean = 4.175282 | Best = 4.900531 GA | iter = 2434 | Mean = 4.407220 | Best = 4.900531 GA | iter = 2435 | Mean = 4.249125 | Best = 4.900531 GA | iter = 2436 | Mean = 4.505584 | Best = 4.900531 GA | iter = 2437 | Mean = 4.482044 | Best = 4.900531 GA | iter = 2438 | Mean = 4.206680 | Best = 4.900531 GA | iter = 2439 | Mean = 4.081324 | Best = 4.900531 GA | iter = 2440 | Mean = 4.101518 | Best = 4.900531 GA | iter = 2441 | Mean = 4.096051 | Best = 4.900531 GA | iter = 2442 | Mean = 3.784550 | Best = 4.900531 GA | iter = 2443 | Mean = 3.922683 | Best = 4.900531 GA | iter = 2444 | Mean = 4.486391 | Best = 4.900531 GA | iter = 2445 | Mean = 4.327040 | Best = 4.900531 GA | iter = 2446 | Mean = 4.314619 | Best = 4.900531 GA | iter = 2447 | Mean = 4.127863 | Best = 4.900531 GA | iter = 2448 | Mean = 4.684417 | Best = 4.900531 GA | iter = 2449 | Mean = 3.889549 | Best = 4.900531 GA | iter = 2450 | Mean = 3.825421 | Best = 4.900531 GA | iter = 2451 | Mean = 4.133031 | Best = 4.900531 GA | iter = 2452 | Mean = 4.104174 | Best = 4.900531 GA | iter = 2453 | Mean = 3.817923 | Best = 4.900531 GA | iter = 2454 | Mean = 4.304516 | Best = 4.900531 GA | iter = 2455 | Mean = 4.196329 | Best = 4.900531 GA | iter = 2456 | Mean = 4.310089 | Best = 4.900531 GA | iter = 2457 | Mean = 4.367228 | Best = 4.900531 GA | iter = 2458 | Mean = 4.433873 | Best = 4.900531 GA | iter = 2459 | Mean = 4.180296 | Best = 4.900531 GA | iter = 2460 | Mean = 4.051976 | Best = 4.900531 GA | iter = 2461 | Mean = 3.481214 | Best = 4.900531 GA | iter = 2462 | Mean = 4.251008 | Best = 4.900531 GA | iter = 2463 | Mean = 4.601937 | Best = 4.900531 GA | iter = 2464 | Mean = 4.590111 | Best = 4.900531 GA | iter = 2465 | Mean = 4.177843 | Best = 4.900531 GA | iter = 2466 | Mean = 4.520401 | Best = 4.900531 GA | iter = 2467 | Mean = 4.523989 | Best = 4.900531 GA | iter = 2468 | Mean = 4.672805 | Best = 4.900531 GA | iter = 2469 | Mean = 4.044161 | Best = 4.900531 GA | iter = 2470 | Mean = 4.332299 | Best = 4.900531 GA | iter = 2471 | Mean = 3.760950 | Best = 4.900531 GA | iter = 2472 | Mean = 3.732136 | Best = 4.900531 GA | iter = 2473 | Mean = 4.030246 | Best = 4.900531 GA | iter = 2474 | Mean = 4.354444 | Best = 4.900531 GA | iter = 2475 | Mean = 4.177479 | Best = 4.900531 GA | iter = 2476 | Mean = 4.408591 | Best = 4.900531 GA | iter = 2477 | Mean = 4.186933 | Best = 4.900531 GA | iter = 2478 | Mean = 4.131012 | Best = 4.900531 GA | iter = 2479 | Mean = 4.292101 | Best = 4.900531 GA | iter = 2480 | Mean = 4.369053 | Best = 4.900531 GA | iter = 2481 | Mean = 4.034571 | Best = 4.900531 GA | iter = 2482 | Mean = 4.374408 | Best = 4.900531 GA | iter = 2483 | Mean = 4.339118 | Best = 4.900531 GA | iter = 2484 | Mean = 3.756747 | Best = 4.900531 GA | iter = 2485 | Mean = 4.072808 | Best = 4.900531 GA | iter = 2486 | Mean = 4.396082 | Best = 4.900531 GA | iter = 2487 | Mean = 4.281013 | Best = 4.900531 GA | iter = 2488 | Mean = 3.709332 | Best = 4.900531 GA | iter = 2489 | Mean = 3.706291 | Best = 4.900531 GA | iter = 2490 | Mean = 4.042527 | Best = 4.900531 GA | iter = 2491 | Mean = 4.114259 | Best = 4.900531 GA | iter = 2492 | Mean = 3.840227 | Best = 4.900531 GA | iter = 2493 | Mean = 4.086331 | Best = 4.900531 GA | iter = 2494 | Mean = 3.914402 | Best = 4.900531 GA | iter = 2495 | Mean = 3.277791 | Best = 4.900531 GA | iter = 2496 | Mean = 3.852665 | Best = 4.900531 GA | iter = 2497 | Mean = 4.260662 | Best = 4.900531 GA | iter = 2498 | Mean = 4.130327 | Best = 4.900531 GA | iter = 2499 | Mean = 4.107297 | Best = 4.900531 GA | iter = 2500 | Mean = 4.066608 | Best = 4.900531 GA | iter = 2501 | Mean = 4.614279 | Best = 4.900531 GA | iter = 2502 | Mean = 4.284623 | Best = 4.900531 GA | iter = 2503 | Mean = 4.108746 | Best = 4.900531 GA | iter = 2504 | Mean = 4.499356 | Best = 4.900531 GA | iter = 2505 | Mean = 4.519163 | Best = 4.900531 GA | iter = 2506 | Mean = 4.381785 | Best = 4.900531 GA | iter = 2507 | Mean = 4.331159 | Best = 4.900531 GA | iter = 2508 | Mean = 4.133553 | Best = 4.900531 GA | iter = 2509 | Mean = 3.889033 | Best = 4.900531 GA | iter = 2510 | Mean = 3.858247 | Best = 4.900531 GA | iter = 2511 | Mean = 3.401545 | Best = 4.900531 GA | iter = 2512 | Mean = 4.518278 | Best = 4.900531 GA | iter = 2513 | Mean = 4.317948 | Best = 4.900531 GA | iter = 2514 | Mean = 4.169494 | Best = 4.900531 GA | iter = 2515 | Mean = 3.861034 | Best = 4.900531 GA | iter = 2516 | Mean = 4.334403 | Best = 4.900531 GA | iter = 2517 | Mean = 4.260760 | Best = 4.900531 GA | iter = 2518 | Mean = 4.418436 | Best = 4.900531 GA | iter = 2519 | Mean = 4.427504 | Best = 4.900531 GA | iter = 2520 | Mean = 4.487930 | Best = 4.900531 GA | iter = 2521 | Mean = 4.400833 | Best = 4.900531 GA | iter = 2522 | Mean = 4.238745 | Best = 4.900531 GA | iter = 2523 | Mean = 3.892980 | Best = 4.900531 GA | iter = 2524 | Mean = 4.018843 | Best = 4.900531 GA | iter = 2525 | Mean = 4.567386 | Best = 4.900531 GA | iter = 2526 | Mean = 4.117501 | Best = 4.900531 GA | iter = 2527 | Mean = 4.542365 | Best = 4.900531 GA | iter = 2528 | Mean = 4.614504 | Best = 4.900531 GA | iter = 2529 | Mean = 4.539398 | Best = 4.900531 GA | iter = 2530 | Mean = 4.179020 | Best = 4.900531 GA | iter = 2531 | Mean = 4.438200 | Best = 4.900531 GA | iter = 2532 | Mean = 4.412248 | Best = 4.900531 GA | iter = 2533 | Mean = 4.365178 | Best = 4.900531 GA | iter = 2534 | Mean = 3.949088 | Best = 4.900531 GA | iter = 2535 | Mean = 4.199050 | Best = 4.900531 GA | iter = 2536 | Mean = 4.087591 | Best = 4.900531 GA | iter = 2537 | Mean = 4.327074 | Best = 4.900531 GA | iter = 2538 | Mean = 4.048871 | Best = 4.900531 GA | iter = 2539 | Mean = 3.806268 | Best = 4.900531 GA | iter = 2540 | Mean = 3.697024 | Best = 4.900531 GA | iter = 2541 | Mean = 3.594215 | Best = 4.900531 GA | iter = 2542 | Mean = 3.551623 | Best = 4.900531 GA | iter = 2543 | Mean = 4.045825 | Best = 4.900531 GA | iter = 2544 | Mean = 4.041763 | Best = 4.900531 GA | iter = 2545 | Mean = 4.280270 | Best = 4.900531 GA | iter = 2546 | Mean = 3.950884 | Best = 4.900531 GA | iter = 2547 | Mean = 4.151924 | Best = 4.900531 GA | iter = 2548 | Mean = 4.211023 | Best = 4.900531 GA | iter = 2549 | Mean = 4.419862 | Best = 4.900531 GA | iter = 2550 | Mean = 3.920445 | Best = 4.900531 GA | iter = 2551 | Mean = 4.591413 | Best = 4.900531 GA | iter = 2552 | Mean = 4.594340 | Best = 4.900531 GA | iter = 2553 | Mean = 4.241686 | Best = 4.900531 GA | iter = 2554 | Mean = 4.527594 | Best = 4.900531 GA | iter = 2555 | Mean = 4.293386 | Best = 4.900531 GA | iter = 2556 | Mean = 3.966714 | Best = 4.900531 GA | iter = 2557 | Mean = 4.655685 | Best = 4.900531 GA | iter = 2558 | Mean = 4.689005 | Best = 4.900531 GA | iter = 2559 | Mean = 4.377948 | Best = 4.900531 GA | iter = 2560 | Mean = 4.567316 | Best = 4.900531 GA | iter = 2561 | Mean = 4.045459 | Best = 4.900531 GA | iter = 2562 | Mean = 4.550352 | Best = 4.900531 GA | iter = 2563 | Mean = 4.370809 | Best = 4.900531 GA | iter = 2564 | Mean = 4.555354 | Best = 4.900531 GA | iter = 2565 | Mean = 4.591769 | Best = 4.900531 GA | iter = 2566 | Mean = 4.634019 | Best = 4.900531 GA | iter = 2567 | Mean = 4.151089 | Best = 4.900531 GA | iter = 2568 | Mean = 4.783516 | Best = 4.900531 GA | iter = 2569 | Mean = 4.475736 | Best = 4.900531 GA | iter = 2570 | Mean = 4.372474 | Best = 4.900531 GA | iter = 2571 | Mean = 4.235867 | Best = 4.900531 GA | iter = 2572 | Mean = 4.498081 | Best = 4.900531 GA | iter = 2573 | Mean = 4.699512 | Best = 4.900531 GA | iter = 2574 | Mean = 4.693664 | Best = 4.900531 GA | iter = 2575 | Mean = 4.257722 | Best = 4.900531 GA | iter = 2576 | Mean = 4.725239 | Best = 4.900531 GA | iter = 2577 | Mean = 4.598424 | Best = 4.900531 GA | iter = 2578 | Mean = 4.434457 | Best = 4.900531 GA | iter = 2579 | Mean = 4.318223 | Best = 4.900531 GA | iter = 2580 | Mean = 4.334948 | Best = 4.900531 GA | iter = 2581 | Mean = 4.282336 | Best = 4.900531 GA | iter = 2582 | Mean = 4.354826 | Best = 4.900531 GA | iter = 2583 | Mean = 4.118053 | Best = 4.900531 GA | iter = 2584 | Mean = 4.061912 | Best = 4.900531 GA | iter = 2585 | Mean = 4.110276 | Best = 4.900531 GA | iter = 2586 | Mean = 4.422757 | Best = 4.900531 GA | iter = 2587 | Mean = 4.568850 | Best = 4.900531 GA | iter = 2588 | Mean = 4.444065 | Best = 4.900531 GA | iter = 2589 | Mean = 4.198974 | Best = 4.900531 GA | iter = 2590 | Mean = 4.613786 | Best = 4.900531 GA | iter = 2591 | Mean = 3.893445 | Best = 4.900531 GA | iter = 2592 | Mean = 3.814275 | Best = 4.900531 GA | iter = 2593 | Mean = 4.250787 | Best = 4.900531 GA | iter = 2594 | Mean = 4.225565 | Best = 4.900531 GA | iter = 2595 | Mean = 4.444406 | Best = 4.900531 GA | iter = 2596 | Mean = 4.510948 | Best = 4.900531 GA | iter = 2597 | Mean = 4.429011 | Best = 4.900531 GA | iter = 2598 | Mean = 3.970171 | Best = 4.900531 GA | iter = 2599 | Mean = 4.393503 | Best = 4.900531 GA | iter = 2600 | Mean = 3.767328 | Best = 4.900531 GA | iter = 2601 | Mean = 3.725837 | Best = 4.900531 GA | iter = 2602 | Mean = 3.823990 | Best = 4.900531 GA | iter = 2603 | Mean = 4.082449 | Best = 4.900531 GA | iter = 2604 | Mean = 3.784605 | Best = 4.900531 GA | iter = 2605 | Mean = 3.638455 | Best = 4.900531 GA | iter = 2606 | Mean = 4.330208 | Best = 4.900531 GA | iter = 2607 | Mean = 4.253631 | Best = 4.900531 GA | iter = 2608 | Mean = 4.275802 | Best = 4.900531 GA | iter = 2609 | Mean = 4.229840 | Best = 4.900531 GA | iter = 2610 | Mean = 4.018090 | Best = 4.900531 GA | iter = 2611 | Mean = 3.546617 | Best = 4.900531 GA | iter = 2612 | Mean = 4.085311 | Best = 4.900531 GA | iter = 2613 | Mean = 4.148770 | Best = 4.900531 GA | iter = 2614 | Mean = 4.172915 | Best = 4.900531 GA | iter = 2615 | Mean = 3.859543 | Best = 4.900531 GA | iter = 2616 | Mean = 4.146161 | Best = 4.900531 GA | iter = 2617 | Mean = 4.208330 | Best = 4.900531 GA | iter = 2618 | Mean = 3.988829 | Best = 4.900531 GA | iter = 2619 | Mean = 4.228036 | Best = 4.900531 GA | iter = 2620 | Mean = 3.911492 | Best = 4.900531 GA | iter = 2621 | Mean = 3.952311 | Best = 4.900531 GA | iter = 2622 | Mean = 4.212354 | Best = 4.900531 GA | iter = 2623 | Mean = 4.444497 | Best = 4.900531 GA | iter = 2624 | Mean = 4.450426 | Best = 4.900531 GA | iter = 2625 | Mean = 4.262349 | Best = 4.900531 GA | iter = 2626 | Mean = 4.460078 | Best = 4.900531 GA | iter = 2627 | Mean = 4.371614 | Best = 4.900531 GA | iter = 2628 | Mean = 4.383859 | Best = 4.900531 GA | iter = 2629 | Mean = 3.513299 | Best = 4.900531 GA | iter = 2630 | Mean = 4.255820 | Best = 4.900531 GA | iter = 2631 | Mean = 4.112938 | Best = 4.900531 GA | iter = 2632 | Mean = 4.219871 | Best = 4.900531 GA | iter = 2633 | Mean = 4.513283 | Best = 4.900531 GA | iter = 2634 | Mean = 4.589058 | Best = 4.900531 GA | iter = 2635 | Mean = 4.335871 | Best = 4.900531 GA | iter = 2636 | Mean = 4.594377 | Best = 4.900531 GA | iter = 2637 | Mean = 4.487684 | Best = 4.900531 GA | iter = 2638 | Mean = 4.620566 | Best = 4.900531 GA | iter = 2639 | Mean = 3.442394 | Best = 4.900531 GA | iter = 2640 | Mean = 4.279199 | Best = 4.900531 GA | iter = 2641 | Mean = 4.578888 | Best = 4.900531 GA | iter = 2642 | Mean = 4.357251 | Best = 4.900531 GA | iter = 2643 | Mean = 3.930842 | Best = 4.900531 GA | iter = 2644 | Mean = 4.057200 | Best = 4.900531 GA | iter = 2645 | Mean = 4.256308 | Best = 4.900531 GA | iter = 2646 | Mean = 4.470445 | Best = 4.900531 GA | iter = 2647 | Mean = 4.620913 | Best = 4.900531 GA | iter = 2648 | Mean = 4.592463 | Best = 4.900531 GA | iter = 2649 | Mean = 3.449058 | Best = 4.900531 GA | iter = 2650 | Mean = 4.193134 | Best = 4.900531 GA | iter = 2651 | Mean = 3.418902 | Best = 4.900531 GA | iter = 2652 | Mean = 3.956863 | Best = 4.900531 GA | iter = 2653 | Mean = 4.146476 | Best = 4.900531 GA | iter = 2654 | Mean = 4.151393 | Best = 4.900531 GA | iter = 2655 | Mean = 4.106350 | Best = 4.900531 GA | iter = 2656 | Mean = 4.075436 | Best = 4.900531 GA | iter = 2657 | Mean = 4.228836 | Best = 4.900531 GA | iter = 2658 | Mean = 3.976646 | Best = 4.900531 GA | iter = 2659 | Mean = 4.014091 | Best = 4.900531 GA | iter = 2660 | Mean = 4.324402 | Best = 4.900531 GA | iter = 2661 | Mean = 4.440766 | Best = 4.900531 GA | iter = 2662 | Mean = 4.534856 | Best = 4.900531 GA | iter = 2663 | Mean = 4.264120 | Best = 4.900531 GA | iter = 2664 | Mean = 3.894891 | Best = 4.900531 GA | iter = 2665 | Mean = 4.108026 | Best = 4.900531 GA | iter = 2666 | Mean = 3.950386 | Best = 4.900531 GA | iter = 2667 | Mean = 4.196787 | Best = 4.900531 GA | iter = 2668 | Mean = 3.923483 | Best = 4.900531 GA | iter = 2669 | Mean = 3.589253 | Best = 4.900531 GA | iter = 2670 | Mean = 3.868820 | Best = 4.900531 GA | iter = 2671 | Mean = 4.337241 | Best = 4.900531 GA | iter = 2672 | Mean = 4.082904 | Best = 4.900531 GA | iter = 2673 | Mean = 4.072525 | Best = 4.900531 GA | iter = 2674 | Mean = 4.626690 | Best = 4.900531 GA | iter = 2675 | Mean = 4.358654 | Best = 4.900531 GA | iter = 2676 | Mean = 4.035726 | Best = 4.900531 GA | iter = 2677 | Mean = 4.321070 | Best = 4.900531 GA | iter = 2678 | Mean = 3.727676 | Best = 4.900531 GA | iter = 2679 | Mean = 4.151426 | Best = 4.900531 GA | iter = 2680 | Mean = 4.201406 | Best = 4.900531 GA | iter = 2681 | Mean = 4.414091 | Best = 4.900531 GA | iter = 2682 | Mean = 3.654042 | Best = 4.900531 GA | iter = 2683 | Mean = 4.223562 | Best = 4.900531 GA | iter = 2684 | Mean = 4.044785 | Best = 4.900531 GA | iter = 2685 | Mean = 3.657081 | Best = 4.900531 GA | iter = 2686 | Mean = 4.531096 | Best = 4.900531 GA | iter = 2687 | Mean = 4.456017 | Best = 4.900531 GA | iter = 2688 | Mean = 3.905367 | Best = 4.900531 GA | iter = 2689 | Mean = 4.512628 | Best = 4.900531 GA | iter = 2690 | Mean = 3.897934 | Best = 4.900531 GA | iter = 2691 | Mean = 4.276269 | Best = 4.900531 GA | iter = 2692 | Mean = 4.500761 | Best = 4.900531 GA | iter = 2693 | Mean = 4.245500 | Best = 4.900531 GA | iter = 2694 | Mean = 4.198357 | Best = 4.900531 GA | iter = 2695 | Mean = 4.546264 | Best = 4.900531 GA | iter = 2696 | Mean = 4.221691 | Best = 4.900531 GA | iter = 2697 | Mean = 4.236515 | Best = 4.900531 GA | iter = 2698 | Mean = 4.377448 | Best = 4.900531 GA | iter = 2699 | Mean = 4.678628 | Best = 4.900531 GA | iter = 2700 | Mean = 4.224634 | Best = 4.900531 GA | iter = 2701 | Mean = 4.418305 | Best = 4.900531 GA | iter = 2702 | Mean = 4.015006 | Best = 4.900531 GA | iter = 2703 | Mean = 4.302180 | Best = 4.900531 GA | iter = 2704 | Mean = 4.550895 | Best = 4.900531 GA | iter = 2705 | Mean = 3.993958 | Best = 4.900531 GA | iter = 2706 | Mean = 4.378067 | Best = 4.900531 GA | iter = 2707 | Mean = 4.028203 | Best = 4.900531 GA | iter = 2708 | Mean = 4.191289 | Best = 4.900531 GA | iter = 2709 | Mean = 4.295145 | Best = 4.900531 GA | iter = 2710 | Mean = 4.365642 | Best = 4.900531 GA | iter = 2711 | Mean = 4.525560 | Best = 4.900531 GA | iter = 2712 | Mean = 4.106224 | Best = 4.900531 GA | iter = 2713 | Mean = 3.845901 | Best = 4.900531 GA | iter = 2714 | Mean = 4.193877 | Best = 4.900531 GA | iter = 2715 | Mean = 4.071877 | Best = 4.900531 GA | iter = 2716 | Mean = 4.561721 | Best = 4.900531 GA | iter = 2717 | Mean = 4.479509 | Best = 4.900531 GA | iter = 2718 | Mean = 4.515238 | Best = 4.900531 GA | iter = 2719 | Mean = 4.270767 | Best = 4.900531 GA | iter = 2720 | Mean = 4.048651 | Best = 4.900531 GA | iter = 2721 | Mean = 4.340802 | Best = 4.900531 GA | iter = 2722 | Mean = 4.267121 | Best = 4.900531 GA | iter = 2723 | Mean = 4.582044 | Best = 4.900531 GA | iter = 2724 | Mean = 3.989736 | Best = 4.900531 GA | iter = 2725 | Mean = 4.281164 | Best = 4.900531 GA | iter = 2726 | Mean = 4.059902 | Best = 4.900531 GA | iter = 2727 | Mean = 4.360947 | Best = 4.900531 GA | iter = 2728 | Mean = 3.936448 | Best = 4.900531 GA | iter = 2729 | Mean = 3.805035 | Best = 4.900531 GA | iter = 2730 | Mean = 3.848990 | Best = 4.900531 GA | iter = 2731 | Mean = 3.681183 | Best = 4.900531 GA | iter = 2732 | Mean = 4.173676 | Best = 4.900531 GA | iter = 2733 | Mean = 4.442475 | Best = 4.900531 GA | iter = 2734 | Mean = 4.055226 | Best = 4.900531 GA | iter = 2735 | Mean = 4.335464 | Best = 4.900531 GA | iter = 2736 | Mean = 4.272131 | Best = 4.900531 GA | iter = 2737 | Mean = 4.358950 | Best = 4.900531 GA | iter = 2738 | Mean = 4.466241 | Best = 4.900531 GA | iter = 2739 | Mean = 4.237275 | Best = 4.900531 GA | iter = 2740 | Mean = 4.555580 | Best = 4.900531 GA | iter = 2741 | Mean = 4.317982 | Best = 4.900531 GA | iter = 2742 | Mean = 4.236445 | Best = 4.900531 GA | iter = 2743 | Mean = 3.623190 | Best = 4.900531 GA | iter = 2744 | Mean = 4.063601 | Best = 4.900531 GA | iter = 2745 | Mean = 4.279753 | Best = 4.900531 GA | iter = 2746 | Mean = 4.080975 | Best = 4.900531 GA | iter = 2747 | Mean = 4.129855 | Best = 4.900531 GA | iter = 2748 | Mean = 4.528419 | Best = 4.900531 GA | iter = 2749 | Mean = 4.642575 | Best = 4.900531 GA | iter = 2750 | Mean = 4.152714 | Best = 4.900531 GA | iter = 2751 | Mean = 4.008511 | Best = 4.900531 GA | iter = 2752 | Mean = 3.992228 | Best = 4.900531 GA | iter = 2753 | Mean = 4.476499 | Best = 4.900531 GA | iter = 2754 | Mean = 4.375815 | Best = 4.900531 GA | iter = 2755 | Mean = 4.155512 | Best = 4.900531 GA | iter = 2756 | Mean = 4.279266 | Best = 4.900531 GA | iter = 2757 | Mean = 4.105004 | Best = 4.900531 GA | iter = 2758 | Mean = 4.213356 | Best = 4.900531 GA | iter = 2759 | Mean = 4.076883 | Best = 4.900531 GA | iter = 2760 | Mean = 4.250773 | Best = 4.900531 GA | iter = 2761 | Mean = 4.590756 | Best = 4.900531 GA | iter = 2762 | Mean = 4.611435 | Best = 4.900531 GA | iter = 2763 | Mean = 4.239284 | Best = 4.900531 GA | iter = 2764 | Mean = 3.975719 | Best = 4.900531 GA | iter = 2765 | Mean = 4.170322 | Best = 4.900531 GA | iter = 2766 | Mean = 4.426248 | Best = 4.900531 GA | iter = 2767 | Mean = 4.282041 | Best = 4.900531 GA | iter = 2768 | Mean = 4.142789 | Best = 4.900531 GA | iter = 2769 | Mean = 4.486328 | Best = 4.900531 GA | iter = 2770 | Mean = 4.559194 | Best = 4.900531 GA | iter = 2771 | Mean = 4.655534 | Best = 4.900531 GA | iter = 2772 | Mean = 4.373405 | Best = 4.900531 GA | iter = 2773 | Mean = 4.007483 | Best = 4.900531 GA | iter = 2774 | Mean = 4.437620 | Best = 4.900531 GA | iter = 2775 | Mean = 4.432438 | Best = 4.900531 GA | iter = 2776 | Mean = 3.996546 | Best = 4.900531 GA | iter = 2777 | Mean = 3.714745 | Best = 4.900531 GA | iter = 2778 | Mean = 3.929767 | Best = 4.900531 GA | iter = 2779 | Mean = 4.755885 | Best = 4.900531 GA | iter = 2780 | Mean = 4.380614 | Best = 4.900531 GA | iter = 2781 | Mean = 4.241554 | Best = 4.900531 GA | iter = 2782 | Mean = 4.746143 | Best = 4.900531 GA | iter = 2783 | Mean = 4.611932 | Best = 4.900531 GA | iter = 2784 | Mean = 4.545633 | Best = 4.900531 GA | iter = 2785 | Mean = 4.754406 | Best = 4.900531 GA | iter = 2786 | Mean = 4.553520 | Best = 4.900531 GA | iter = 2787 | Mean = 4.231534 | Best = 4.900531 GA | iter = 2788 | Mean = 4.244568 | Best = 4.900531 GA | iter = 2789 | Mean = 4.555198 | Best = 4.900531 GA | iter = 2790 | Mean = 4.325561 | Best = 4.900531 GA | iter = 2791 | Mean = 4.136678 | Best = 4.900531 GA | iter = 2792 | Mean = 4.516065 | Best = 4.900531 GA | iter = 2793 | Mean = 4.291420 | Best = 4.900531 GA | iter = 2794 | Mean = 4.629865 | Best = 4.900531 GA | iter = 2795 | Mean = 4.600902 | Best = 4.900531 GA | iter = 2796 | Mean = 4.265242 | Best = 4.900531 GA | iter = 2797 | Mean = 4.304567 | Best = 4.900531 GA | iter = 2798 | Mean = 4.518462 | Best = 4.900531 GA | iter = 2799 | Mean = 4.326415 | Best = 4.900531 GA | iter = 2800 | Mean = 4.376091 | Best = 4.900531 GA | iter = 2801 | Mean = 4.198596 | Best = 4.900531 GA | iter = 2802 | Mean = 4.561556 | Best = 4.900531 GA | iter = 2803 | Mean = 4.077586 | Best = 4.900531 GA | iter = 2804 | Mean = 3.865633 | Best = 4.900531 GA | iter = 2805 | Mean = 4.081774 | Best = 4.900531 GA | iter = 2806 | Mean = 3.991287 | Best = 4.900531 GA | iter = 2807 | Mean = 3.883297 | Best = 4.900531 GA | iter = 2808 | Mean = 3.992636 | Best = 4.900531 GA | iter = 2809 | Mean = 4.281091 | Best = 4.900531 GA | iter = 2810 | Mean = 4.492830 | Best = 4.900531 GA | iter = 2811 | Mean = 4.388355 | Best = 4.900531 GA | iter = 2812 | Mean = 4.121285 | Best = 4.900531 GA | iter = 2813 | Mean = 4.118560 | Best = 4.900531 GA | iter = 2814 | Mean = 3.727828 | Best = 4.900531 GA | iter = 2815 | Mean = 4.360594 | Best = 4.900531 GA | iter = 2816 | Mean = 4.411224 | Best = 4.900531 GA | iter = 2817 | Mean = 3.832850 | Best = 4.900531 GA | iter = 2818 | Mean = 4.183919 | Best = 4.900531 GA | iter = 2819 | Mean = 4.241192 | Best = 4.900531 GA | iter = 2820 | Mean = 4.451249 | Best = 4.900531 GA | iter = 2821 | Mean = 4.362073 | Best = 4.900531 GA | iter = 2822 | Mean = 4.518187 | Best = 4.900531 GA | iter = 2823 | Mean = 4.498946 | Best = 4.900531 GA | iter = 2824 | Mean = 4.246044 | Best = 4.900531 GA | iter = 2825 | Mean = 4.306885 | Best = 4.900531 GA | iter = 2826 | Mean = 4.027576 | Best = 4.900531 GA | iter = 2827 | Mean = 4.573842 | Best = 4.900531 GA | iter = 2828 | Mean = 4.260362 | Best = 4.900531 GA | iter = 2829 | Mean = 4.006574 | Best = 4.900531 GA | iter = 2830 | Mean = 4.129046 | Best = 4.900531 GA | iter = 2831 | Mean = 4.051624 | Best = 4.900531 GA | iter = 2832 | Mean = 4.172052 | Best = 4.900531 GA | iter = 2833 | Mean = 3.580697 | Best = 4.900531 GA | iter = 2834 | Mean = 4.271679 | Best = 4.900531 GA | iter = 2835 | Mean = 4.136936 | Best = 4.900531 GA | iter = 2836 | Mean = 4.352956 | Best = 4.900531 GA | iter = 2837 | Mean = 4.456738 | Best = 4.900531 GA | iter = 2838 | Mean = 4.263445 | Best = 4.900531 GA | iter = 2839 | Mean = 3.997502 | Best = 4.900531 GA | iter = 2840 | Mean = 4.203509 | Best = 4.900531 GA | iter = 2841 | Mean = 3.918349 | Best = 4.900531 GA | iter = 2842 | Mean = 3.989998 | Best = 4.900531 GA | iter = 2843 | Mean = 4.279530 | Best = 4.900531 GA | iter = 2844 | Mean = 4.176248 | Best = 4.900531 GA | iter = 2845 | Mean = 4.332453 | Best = 4.900531 GA | iter = 2846 | Mean = 4.094296 | Best = 4.900531 GA | iter = 2847 | Mean = 4.586851 | Best = 4.900531 GA | iter = 2848 | Mean = 4.194920 | Best = 4.900531 GA | iter = 2849 | Mean = 3.996744 | Best = 4.900531 GA | iter = 2850 | Mean = 3.852218 | Best = 4.900531 GA | iter = 2851 | Mean = 3.773956 | Best = 4.900531 GA | iter = 2852 | Mean = 3.613003 | Best = 4.900531 GA | iter = 2853 | Mean = 4.139652 | Best = 4.900531 GA | iter = 2854 | Mean = 4.142144 | Best = 4.900531 GA | iter = 2855 | Mean = 3.924635 | Best = 4.900531 GA | iter = 2856 | Mean = 4.234285 | Best = 4.900531 GA | iter = 2857 | Mean = 4.313552 | Best = 4.900531 GA | iter = 2858 | Mean = 4.278162 | Best = 4.900531 GA | iter = 2859 | Mean = 4.338098 | Best = 4.900531 GA | iter = 2860 | Mean = 4.126939 | Best = 4.900531 GA | iter = 2861 | Mean = 4.329032 | Best = 4.900531 GA | iter = 2862 | Mean = 4.459943 | Best = 4.900531 GA | iter = 2863 | Mean = 4.266664 | Best = 4.900531 GA | iter = 2864 | Mean = 4.263316 | Best = 4.900531 GA | iter = 2865 | Mean = 4.182887 | Best = 4.900531 GA | iter = 2866 | Mean = 4.189608 | Best = 4.900531 GA | iter = 2867 | Mean = 4.141920 | Best = 4.900531 GA | iter = 2868 | Mean = 3.808870 | Best = 4.900531 GA | iter = 2869 | Mean = 4.086283 | Best = 4.900531 GA | iter = 2870 | Mean = 3.900777 | Best = 4.900531 GA | iter = 2871 | Mean = 3.818589 | Best = 4.900531 GA | iter = 2872 | Mean = 4.258049 | Best = 4.900531 GA | iter = 2873 | Mean = 4.419359 | Best = 4.900531 GA | iter = 2874 | Mean = 3.735604 | Best = 4.900531 GA | iter = 2875 | Mean = 4.197445 | Best = 4.900531 GA | iter = 2876 | Mean = 4.545603 | Best = 4.900531 GA | iter = 2877 | Mean = 4.711036 | Best = 4.900531 GA | iter = 2878 | Mean = 4.030957 | Best = 4.900531 GA | iter = 2879 | Mean = 4.383802 | Best = 4.900531 GA | iter = 2880 | Mean = 4.057438 | Best = 4.900531 GA | iter = 2881 | Mean = 4.110226 | Best = 4.900531 GA | iter = 2882 | Mean = 4.161696 | Best = 4.900531 GA | iter = 2883 | Mean = 4.365888 | Best = 4.900531 GA | iter = 2884 | Mean = 3.622634 | Best = 4.900531 GA | iter = 2885 | Mean = 3.632788 | Best = 4.900531 GA | iter = 2886 | Mean = 3.786392 | Best = 4.900531 GA | iter = 2887 | Mean = 3.801727 | Best = 4.900531 GA | iter = 2888 | Mean = 3.782868 | Best = 4.900531 GA | iter = 2889 | Mean = 3.903932 | Best = 4.900531 GA | iter = 2890 | Mean = 3.783175 | Best = 4.900531 GA | iter = 2891 | Mean = 4.073918 | Best = 4.900531 GA | iter = 2892 | Mean = 4.259770 | Best = 4.900531 GA | iter = 2893 | Mean = 3.471477 | Best = 4.900531 GA | iter = 2894 | Mean = 4.041596 | Best = 4.900531 GA | iter = 2895 | Mean = 4.433366 | Best = 4.900531 GA | iter = 2896 | Mean = 4.043943 | Best = 4.900531 GA | iter = 2897 | Mean = 4.280595 | Best = 4.900531 GA | iter = 2898 | Mean = 4.531922 | Best = 4.900531 GA | iter = 2899 | Mean = 3.697642 | Best = 4.900531 GA | iter = 2900 | Mean = 4.242295 | Best = 4.900531 GA | iter = 2901 | Mean = 4.352613 | Best = 4.900531 GA | iter = 2902 | Mean = 4.175540 | Best = 4.900531 GA | iter = 2903 | Mean = 3.833557 | Best = 4.900531 GA | iter = 2904 | Mean = 3.243397 | Best = 4.900531 GA | iter = 2905 | Mean = 4.292062 | Best = 4.900531 GA | iter = 2906 | Mean = 4.017258 | Best = 4.900531 GA | iter = 2907 | Mean = 4.010480 | Best = 4.900531 GA | iter = 2908 | Mean = 3.919476 | Best = 4.900531 GA | iter = 2909 | Mean = 4.437953 | Best = 4.900531 GA | iter = 2910 | Mean = 4.428608 | Best = 4.900531 GA | iter = 2911 | Mean = 4.355408 | Best = 4.900531 GA | iter = 2912 | Mean = 4.196103 | Best = 4.900531 GA | iter = 2913 | Mean = 3.917359 | Best = 4.900531 GA | iter = 2914 | Mean = 4.082783 | Best = 4.900531 GA | iter = 2915 | Mean = 4.247647 | Best = 4.900531 GA | iter = 2916 | Mean = 4.594778 | Best = 4.900531 GA | iter = 2917 | Mean = 4.326514 | Best = 4.900531 GA | iter = 2918 | Mean = 4.478857 | Best = 4.900531 GA | iter = 2919 | Mean = 4.168802 | Best = 4.900531 GA | iter = 2920 | Mean = 4.381731 | Best = 4.900531 GA | iter = 2921 | Mean = 3.932926 | Best = 4.900531 GA | iter = 2922 | Mean = 4.216500 | Best = 4.900531 GA | iter = 2923 | Mean = 4.513282 | Best = 4.900531 GA | iter = 2924 | Mean = 4.314635 | Best = 4.900531 GA | iter = 2925 | Mean = 4.124217 | Best = 4.900531 GA | iter = 2926 | Mean = 4.095482 | Best = 4.900531 GA | iter = 2927 | Mean = 4.486605 | Best = 4.900531 GA | iter = 2928 | Mean = 3.969378 | Best = 4.900531 GA | iter = 2929 | Mean = 4.284362 | Best = 4.900531 GA | iter = 2930 | Mean = 4.129770 | Best = 4.900531 GA | iter = 2931 | Mean = 3.610604 | Best = 4.900531 GA | iter = 2932 | Mean = 3.930680 | Best = 4.900531 GA | iter = 2933 | Mean = 3.988115 | Best = 4.900531 GA | iter = 2934 | Mean = 4.052454 | Best = 4.900531 GA | iter = 2935 | Mean = 3.436876 | Best = 4.900531 GA | iter = 2936 | Mean = 3.950023 | Best = 4.900531 GA | iter = 2937 | Mean = 3.803424 | Best = 4.900531 GA | iter = 2938 | Mean = 3.720773 | Best = 4.900531 GA | iter = 2939 | Mean = 4.203375 | Best = 4.900531 GA | iter = 2940 | Mean = 3.631438 | Best = 4.900531 GA | iter = 2941 | Mean = 3.483212 | Best = 4.900531 GA | iter = 2942 | Mean = 4.053013 | Best = 4.900531 GA | iter = 2943 | Mean = 4.546803 | Best = 4.900531 GA | iter = 2944 | Mean = 4.504047 | Best = 4.900531 GA | iter = 2945 | Mean = 4.575873 | Best = 4.900531 GA | iter = 2946 | Mean = 4.550222 | Best = 4.900531 GA | iter = 2947 | Mean = 4.229862 | Best = 4.900531 GA | iter = 2948 | Mean = 4.407400 | Best = 4.900531 GA | iter = 2949 | Mean = 4.461553 | Best = 4.900531 GA | iter = 2950 | Mean = 3.709511 | Best = 4.900531 GA | iter = 2951 | Mean = 4.295157 | Best = 4.900531 GA | iter = 2952 | Mean = 4.078005 | Best = 4.900531 GA | iter = 2953 | Mean = 4.112134 | Best = 4.900531 GA | iter = 2954 | Mean = 4.147874 | Best = 4.900531 GA | iter = 2955 | Mean = 3.705753 | Best = 4.900531 GA | iter = 2956 | Mean = 4.134735 | Best = 4.900531 GA | iter = 2957 | Mean = 4.351665 | Best = 4.900531 GA | iter = 2958 | Mean = 4.279895 | Best = 4.900531 GA | iter = 2959 | Mean = 4.278843 | Best = 4.900531 GA | iter = 2960 | Mean = 4.275949 | Best = 4.900531 GA | iter = 2961 | Mean = 4.631422 | Best = 4.900531 GA | iter = 2962 | Mean = 4.436045 | Best = 4.900531 GA | iter = 2963 | Mean = 4.077060 | Best = 4.900531 GA | iter = 2964 | Mean = 4.150234 | Best = 4.900531 GA | iter = 2965 | Mean = 4.120024 | Best = 4.900531 GA | iter = 2966 | Mean = 3.892779 | Best = 4.900531 GA | iter = 2967 | Mean = 4.335829 | Best = 4.900531 GA | iter = 2968 | Mean = 4.561808 | Best = 4.900531 GA | iter = 2969 | Mean = 4.348755 | Best = 4.900531 GA | iter = 2970 | Mean = 3.949232 | Best = 4.900531 GA | iter = 2971 | Mean = 3.798302 | Best = 4.900531 GA | iter = 2972 | Mean = 4.178620 | Best = 4.900531 GA | iter = 2973 | Mean = 4.311869 | Best = 4.900531 GA | iter = 2974 | Mean = 4.278739 | Best = 4.900531 GA | iter = 2975 | Mean = 4.076401 | Best = 4.900531 GA | iter = 2976 | Mean = 3.784908 | Best = 4.900531 GA | iter = 2977 | Mean = 3.886805 | Best = 4.900531 GA | iter = 2978 | Mean = 4.465828 | Best = 4.900531 GA | iter = 2979 | Mean = 4.595887 | Best = 4.900531 GA | iter = 2980 | Mean = 4.123282 | Best = 4.900531 GA | iter = 2981 | Mean = 4.434521 | Best = 4.900531 GA | iter = 2982 | Mean = 4.474501 | Best = 4.900531 GA | iter = 2983 | Mean = 4.514997 | Best = 4.900531 GA | iter = 2984 | Mean = 4.525087 | Best = 4.900531 GA | iter = 2985 | Mean = 4.448185 | Best = 4.900531 GA | iter = 2986 | Mean = 4.248402 | Best = 4.900531 GA | iter = 2987 | Mean = 4.151181 | Best = 4.900531 GA | iter = 2988 | Mean = 3.807778 | Best = 4.900531 GA | iter = 2989 | Mean = 3.773564 | Best = 4.900531 GA | iter = 2990 | Mean = 4.502704 | Best = 4.900531 GA | iter = 2991 | Mean = 4.163781 | Best = 4.900531 GA | iter = 2992 | Mean = 4.554996 | Best = 4.900531 GA | iter = 2993 | Mean = 4.236599 | Best = 4.900531 GA | iter = 2994 | Mean = 4.475176 | Best = 4.900531 GA | iter = 2995 | Mean = 4.164776 | Best = 4.900531 GA | iter = 2996 | Mean = 3.670657 | Best = 4.900531 GA | iter = 2997 | Mean = 4.062327 | Best = 4.900531 GA | iter = 2998 | Mean = 3.766430 | Best = 4.900531 GA | iter = 2999 | Mean = 4.047050 | Best = 4.900531 GA | iter = 3000 | Mean = 4.644055 | Best = 4.900531 The plot shows how optimal value converges. Although it is shown as positive values, we need to manually add a negative sign. From the summary below, the fitness function value and the parameters are very close to what we got before using simulated annealing. -- Genetic Algorithm ------------------- GA settings: Type = real-valued Population size = 50 Number of generations = 3000 Elitism = 2 Crossover probability = 0.8 Mutation probability = 0.1 Search domain = x1 x2 lower -2 -2 upper 2 2 GA results: Iterations = 3000 Fitness function value = 4.900531 Solution = x1 x2 [1,] 1.99625 0.5972324 Example: maybe biexponential or not, depending on complexity of cost surface. 8.5 Calibration of Dynamic Models The principles of nonlinear regression carry over directly to calibtration of more complex models. In particular, in many domains of biology, dynamic models are used to describe the time-varying behaviour of systems (from biomolecular networks to cell-cell interactions to physiology to ecology). These models take many forms, but a commonly used formulation is a model based on ordinary differential equations (i.e. rate equations). These models are deterministic (cannot incorporate random effects) and assume that the dynamics occur in a spatially homogeneous environment (they capture spatially distributed phenomena). Despite these limitations, these models can describe a wide variety of dynamic behaviours, and so are useful starting points for investigations across biology. Ordinary differential equation models used in biology often take the form \\[\\begin{equation*} \\frac{d}{dt} {\\bf x}(t) = {\\bf f}({\\bf x}(t), {\\bf p}) \\end{equation*}\\] where components of the time-varying vector \\({\\bf x}(t)\\) are the states of the system (e.g.~population sizes, molecular concentrations), the components of vector \\({\\bf p}\\) are the model parameters: numerical values that represent fixed features of the system and its environment (e.g. interaction strengths, temperature, nutrient availability), and the vector-valued function \\({\\bf f}\\) describes the rate of change of the state variables. As a concrete example, consider the Lotka-Volterra equations, a classical model to describe interacting predator and prey populations [ref]: \\[\\begin{eqnarray*} \\frac{d}{dt} x_1(t) &amp;=&amp; p_1 x_1(t) - p_2 x_1(t) x_2(t)\\\\ \\frac{d}{dt} x_2(t) &amp;=&amp; p_3 x_1(t) x_2(t) - p_4 x_2(t)\\\\ \\end{eqnarray*}\\] Here \\(x_1\\) is the size of the prey population; \\(x_2\\) is the size of the predator population. The prey are presumed to have access to resources that allow exponential growth in the absence of predation (growth at rate \\(p_1 x_1(t)\\). Interactions between prey and predator populations (assumed to occur at rate \\(x_1(t) x_2(t)\\) lead to an decrease in the prey population and an increase in the predator population (characterized by parameters \\(p_2\\) and \\(p_3\\) respectively). Finally, the prey suffer an exponential decline in population size in the absence of prey (decay rate \\(p_4 x_2(t)\\)). A simulation of the model is shown in Figure. Note that simualtion of the model requires specification of (i) values for each of the model parameters, and (ii) initial conditions, i.e.~the size of each population at time \\(t=0\\). Figure XYZ shows a dataset the corresponds to observations of a predator-prey population system. To calibrate the model Lotka-Volterra to this dataset we seek values for the four parameters \\(p_1\\), \\(p_2\\), \\(p_3\\), and \\(p_4\\) for which simulations of the model provide the best fit. As in the regression tasks described previously, the standard measure for quality of fit is the sum of squared errors. We thus proceed with a minimization task: for each simulation of the model, we compare with the dataset and determine the sum of squared errors. We aim to minimize this fit over the space of model parameters. Theres one additional feature we must account for: to specify a simulation we need numerical values for the model parameters and the initial conditions. Calling these initial conditions \\(p_5 = x_1(0)\\) and \\(p_6=x_2(0)\\), we are then pose our optimization problem as a search over a six-dimensional parameter space. In what follows, we illustrate the use of both simulated annealing and a genetic algorithm to calibrate this model. Matt: insert example 8.6 Uncertainty Analysis and Bayesian Calibration The regression tasks discussed above resulted in estimates of parameter values based on the provided datasets. Regression is usually followed by uncertainty analysis, which provides some measure of confidence in those estimated parameter values. For instance, in the case of linear regression, 95% confidence intervals on the estimates (best fit line slope and intercept) and corresponding confidence intervals on the model predictions are supported by extensive statistical theory, and can be easily generated in R. The plot below shows 95% confidence intervals on the model predictions. The confint() function finds 95% confidence intervals on the estimates (best fit line slope and intercept). This can also be derived using the confidence interval formula: \\(\\text{estimate}\\pm \\text{critical value}\\times\\text{standard error}\\) For nonlinear regression (including calibration of dynamic models), the theory is less helpful, but estimates of uncertainty intervals can be achieved (as discussed below in the section on optimal experimental design). Waiting for profiling to be done... 2.5% 97.5% Km 0.249475144 0.881545621 Vmax 0.004390728 0.006526742 lower upper 0.004292667 0.006319616 lower upper 0.1896696 0.7647611 Bayesian methods address the regression task by combining calibration and uncertainty in a single process. The basic idea behind Bayesian analysis (founded on Bayes Theorem, which may be familiar from elementary probability), is to start with an initial parameter estimate (analogous to the initial guess needed in nonlinear regression) and then use the available data to refine that estimate. The difference is that instead of the initial guess and refined estimate being single numerical values, they are distributions. In Bayesian terminology, we being with a prior distribution, which may be informed by expert knowledge (e.g.~a normal distribution centered at a good initial estimate), or may be a wild guess (e.g.~a uniform distribution over a wide range of possible values). Application of a Bayesian calibration scheme uses the available data to generate an improved distribution of the parameter values, called the posterior distribution. A successful Bayesian calibration could take a `wild guess uniform prior and return a tightly-centered posterior. Uncertainty can then be gleaned directly from the posterior, by reporting, e.g.~ a 95% confidence interval. One commonly cited concern with Bayesian techniques is the dependence of the result on the rather subjective selection of a prior. This is analogous to concerns about any bias introduced by choice of initial guess in the non-Bayesian analysis described above. In both cases, multi-start techniques can be used to address this issue. Here well consider a simple numerical implementation of a Bayesian approach: Approximate Bayesian Computation (ABC). This approach is based on a simple idea: the rejection method, in which we sample repeatedly from the prior distribution and reject all samples that do not satisfy a pre-specified tolerance for quality of fit. (This is reminiscent of the selection step in genetic algorithms: culling unfit members of a population.) For the Michaelis-Menten example, the estimated values for \\(K\\) and \\(V_m\\) are 0.4398016 and 0.0054252 respectively. Now we simulate numbers for \\(K\\) and \\(V_m\\), and accept the value if the distances between the simulated values and the true values are under the threshold. Here we randomly sample \\(K\\) and \\(V_m\\) 200000 times, it can be seen from the histograms that the values that occur most frequently are close to the estimated values derived from the non linear regression model. [1] 2164 Typically, approximate Bayesian computation is implemented as an iterative method, in which a sequence of rejection steps is applied, withe the prior being refined at each step (generating, in essence, a sequence of posterior distributions, the last of which is considered to be the best description of the desired parameter estimates). After 20000 simulations, it can be seen from the histograms that the most frequent values are close to the estimated values derived from the non linear regression model. 8.7 References Ashyraliyev, M., FomekongNanfack, Y., Kaandorp, J. A., &amp; Blom, J. G. (2009). Systems biology: parameter estimation for biochemical models. The FEBS journal, 276(4), 886-902. Beaumont, Mark A. Approximate bayesian computation. Annual review of statistics and its application 6 (2019): 379-403. Cho, Yong-Soon, and Hyeong-Seok Lim. Comparison of various estimation methods for the parameters of Michaelis-Menten equation based on in vitro elimination kinetic simulation data. Translational and clinical pharmacology 26.1 (2018): 39-47. Fairway, Julien, (2002) Practical Regression and Anova using R. https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf "]]
